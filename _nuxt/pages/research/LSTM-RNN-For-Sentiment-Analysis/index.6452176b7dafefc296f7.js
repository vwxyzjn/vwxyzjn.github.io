webpackJsonp([1],{"/DX2":function(e,n,t){e.exports=t.p+"img/output_18_0.1a262f8.png"},"BI+w":function(e,n,t){e.exports=t.p+"img/output_11_5.827a616.png"},CrrM:function(e,n,t){e.exports=t.p+"img/output_11_2.0408346.png"},EpOa:function(e,n,t){e.exports=t.p+"img/output_11_3.eda9622.png"},FIMK:function(e,n,t){"use strict";var a="\n# LSTM-RNN For Sentiment Analysis\nAuthor: Trilby Hren and Costa Huang\n\nSource code vailable at https://github.com/costahuang/Sentiment-Analysis-LSTM\n\n**Our project has three main parts:**\n1. Reproduce and understand Keras(a python machine learning library) official demo code on LSTM-RNN for sentiment analysis \n * [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py) Main program that rains a LSTM on the IMDB sentiment classification task [1]\n * [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py) Preprocessing script of IMDB movie review dataset [2]\n* Improve the program's accuracy by exploring different techniques:\n * Preprocessing techniques\n * Activation functions\n * Optimizer choices\n* Futher application\n * Visualization of sentiment analysis\n * Applied our result to Amazon Review Datas\n\n\n\n## Baseline algorithm (Demo code from Keras)\nFirstly we execute [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py)[1] and yields following results where a test accuracy of 82.35% is acquired.\n\n\n```python\n'''Trains a LSTM on the IMDB sentiment classification task.\nThe dataset is actually too small for LSTM to be of any advantage\ncompared to simpler, much faster methods such as TF-IDF + LogReg.\nNotes:\n- RNNs are tricky. Choice of batch size is important,\nchoice of loss and optimizer is critical, etc.\nSome configurations won't converge.\n- LSTM loss decrease patterns during training can be quite different\nfrom what you see with CNNs/MLPs/etc.\n'''\nfrom __future__ import print_function\nimport numpy as np\nnp.random.seed(1337)  # for reproducibility\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import LSTM, SimpleRNN, GRU\nfrom keras.datasets import imdb\n\nmax_features = 20000\nmaxlen = 80  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\nprint('Loading data...')\n(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\nprint(len(X_train), 'train sequences')\nprint(len(X_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\n\nprint('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, dropout=0.2))\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\n          validation_data=(X_test, y_test))\nscore, acc = model.evaluate(X_test, y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n```\n\n    Using Theano backend.\n    \n\n    Loading data...\n    25000 train sequences\n    25000 test sequences\n    Pad sequences (samples x time)\n    X_train shape: (25000, 80)\n    X_test shape: (25000, 80)\n    Build model...\n    Train...\n    Train on 25000 samples, validate on 25000 samples\n    Epoch 1/10\n    25000/25000 [==============================] - 138s - loss: 0.5333 - acc: 0.7316 - val_loss: 0.4631 - val_acc: 0.7898\n    Epoch 2/10\n    25000/25000 [==============================] - 144s - loss: 0.3812 - acc: 0.8358 - val_loss: 0.3776 - val_acc: 0.8370\n    Epoch 3/10\n    25000/25000 [==============================] - 147s - loss: 0.3077 - acc: 0.8737 - val_loss: 0.3687 - val_acc: 0.8360\n    Epoch 4/10\n    25000/25000 [==============================] - 146s - loss: 0.2497 - acc: 0.8979 - val_loss: 0.4161 - val_acc: 0.8322\n    Epoch 5/10\n    25000/25000 [==============================] - 149s - loss: 0.2100 - acc: 0.9180 - val_loss: 0.4277 - val_acc: 0.8330\n    Epoch 6/10\n    25000/25000 [==============================] - 151s - loss: 0.1801 - acc: 0.9298 - val_loss: 0.4318 - val_acc: 0.8362\n    Epoch 7/10\n    25000/25000 [==============================] - 148s - loss: 0.1503 - acc: 0.9423 - val_loss: 0.4964 - val_acc: 0.8280\n    Epoch 8/10\n    25000/25000 [==============================] - 150s - loss: 0.1328 - acc: 0.9499 - val_loss: 0.5169 - val_acc: 0.8266\n    Epoch 9/10\n    25000/25000 [==============================] - 146s - loss: 0.1163 - acc: 0.9548 - val_loss: 0.5354 - val_acc: 0.8227\n    Epoch 10/10\n    25000/25000 [==============================] - 153s - loss: 0.1047 - acc: 0.9618 - val_loss: 0.5375 - val_acc: 0.8236\n    25000/25000 [==============================] - 34s    \n    Test score: 0.537465095153\n    Test accuracy: 0.82356\n    \n\n## Understanding the source code\n** 1. Preprocessing**\n * Original Dataset is provided by a [Stanford research group](http://ai.stanford.edu/~amaas//data/sentiment/) [3], which provided a set of 25,000 preclassified IMDB movie reviews for training, and 25,000 for testing.\n * [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py)[2] performs the following tasks:\n     * Download tokenized the IMDB movie review pre-supplied by Keras, where each review is encoded as word indices. An example training review would look like [3, 213, 43, 324, 12, 4, ...] where 3 represents the third most popular word in the dataset.\n     * Shuffle the training and testing dataset.\n     * Include only *nb_words* most frequent word for ouput\n     * Add a [1] to each review to mark the start of the sequence\n     * Add a [2] to replace the words that are cut off because of *nb_words*\n     * Index actual words with *\"index_from = 3*\" and higher. (We think this part is **very problematic**, which is mentioned in the next code cell)\n\n\n```python\n# From imdb.py, it has a preprocessing code like this, where they use index_from = 3 as the default parameter\n# if start_char is not None:\n#     X = [[start_char] + [w + index_from for w in x] for x in X]\nindex_from = 3\nexample_review = [6, 499, 6, 99, 221, 9, 22, 501, 2, 3]\nexample_review = [[1] + [w + index_from for w in example_review]]\nprint(example_review)\n```\n\n    [[1, 9, 502, 9, 102, 224, 12, 25, 504, 5, 6]]\n    \n\n> As we demoed, this particular parameter \"shifted\" each word index of the review up by *\"index_from = 3\"*, which is totally nonsense in our opinion because it almost changes everything about that review. We also couldn't find any documentation or papers to support such parameter. As a result, in our experiment, we just simply set *\"index_from = 0\"*.\n\n**2. Model Building**\n * [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py) builds the model:\n     * Limit top most frequent words: Each review only has *\"max_features = nb_words = 20000\"* most frequent word index\n     * Pad Sequence: Each review is padded with a maximum length of *\"maxlen = 80\"*\n     * Embedding layer: Map each word index into *\"p = 128\"* dimentional vector space\n       * The original idea came from this landmark paper [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)[6] where they show a good vector representation for text sequences is achievable.\n     * LSTM layer\n        * Hochreiter & Schmidhuber [7] first introduced this special kind of RNN to solve the long-term dependency problem. Unlike traditional RNN, LSTM has a memory cell that it uses to determine what information is important by using \"forget gate layer\" and \"input gate layer\". [8]\n     * Dense(1): fully connect LSTM to create one demensional output\n     * Sigmoid\n     * Adam optimizer for training.\n\n## Improving the algorithm\n**_Activation functions_** by [Keras.layers.core.Activation](https://keras.io/activations/)\n1. Sigmoid  \n   1. σ(x) = 1/(1+e−x)  \n   2. The sigmoid function is a squashing function. We discovered that this activation function jumps up in test accuracy and converges to about 82.6% after 8 generations.     \n   \n2. ReLU  \n  1. f(x) = max(0,x)  \n  1. For the ReLU activation function our results show dramatic improvement in later generations. It jumps from very low results to much higher accuracy in both testing and training. The final accuracy is not as good as the results from the sigmoid function, it converges to 73.8% in the last generation.      \n  \n3. Tanh  \n  1. tanh(x) = 2σ(2x)−1tanh(x)=2σ(2x)−1  \n  1. The tanh function was slightly better than ReLU with a final accuracy of 77.6%. During our 8 generations, this function seemed to decrease in overall test accuracy perhaps due to overfitting.      \n  \n4. Hard sigmoid    \n  1. The hard sigmoid function performed only slightly better than the sigmoid with a final test accuracy of 83%. From our understanding, it should have converged faster due to increased speed since the precision isn't as important. Perhaps the lower test accuracy is because of the loss in precision rather than the speed.      \n\n[3]\nCheck out our [Activation functions data](https://docs.google.com/document/d/1uQ49jhYYOmwzSPxYYamTXYCq-07Dhhu75eleew3LBPU/edit?usp=sharing)\n\n\n**_Optimizer choices_** by [keras.optimizers](https://keras.io/optimizers/)\n1. SGD (Stochastic Gradient Decent)\n    1. This type of optimizer uses only a few training examples rather than the entire training set. It also can lead to fast convergence since it doesn't have to run back propagation over the full training set. \n    1. Sometimes a meaningful order leads to a bias for the gradient and causes poor convergence. Since `our data was randomly shuffled before each training`, this shouldn't have been an issue.\n        2. _Adam_\n            1. Adaptive Moment Estimation\n            1. This optimizer uses momentum and past gradients to help with learning by combining the two advantages of RMSprop and AdaGrad. \n            1. While we expected this optimizer to perfom the best since it combines the better parts of RMSprop and AdaGrad, it was actually one of our lowest perfoming tests. It showed very slow convergence to 82.6% during our 8 generations. Perhaps it would have converged to a higher accuracy than the others, given a higher number of generations. \n        3. _RMSprop_\n            1. Root Mean Square Propagation\n            1. This method adapts the learning rate for each parameter by using tha magnitude of recent gradients to normalize the gradients.\n            1. From out tests, RSMprop performed the best with a final testing accuracy converging to 84.8%. This optimizer had steady performance through the 8 generations with the lowest accuracy being 82.8%.\n        4. _AdaGrad_\n            1. Adaptive Gradient Algorithm\n            1. AdaGrad adapts to the data in order to chose a learning rate for each feature.\n            1. The performance of the AdaGrad optimizer was mediocre, with a final convergence at 83.4%.\n        5. _Nadam_\n            1. Nesterov Adaptive Moment Estimation\n            1. Nadam combines Adam with Nesterov's accelerated gradient.\n            1. Our results from this test were one of the lowest with Adam, converging to 82.3% and increasing by a whopping 0.3% over the course of 8 generations.\n\n[4]\nCheck out our [Optimizer choices data](https://docs.google.com/document/d/1YI5wluhh3rqHs8LBqqL2vOxqNwZdYiPHstWkS2_1s4s/edit?usp=sharing)\n\n** Preprocessing techniques**\n1. Change the padding(*\"maxlen = 80\"*) for each reviews\n    1. It is reasonable to assume when people write reviews, they would express their opinion at the start more often than at the end. So we can limit each review to some *\"maxlen\"* to eliminate noisy data. \n2. Change the top most frequent words(*\"max_features = nb_words = 20000\"*) for training and testing dataset to keep \n    1. Some words are essential than other words for expressing sentiment. For example, \"love\" would express more emotion than \"keyboard\". We therefore limit *\"max_features\"* to eliminate less sentimentally expresed words.\n3. Set *\"index_from = 0\"* as we explained previously\n    \nCheck out our [Preprocessing techniques data](https://docs.google.com/document/d/1pjZbvzbbHWRUvybd--uTJUhPKvMCjbjxacE3j_zQCM4/edit?usp=sharing)\n\n\n\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n\n\"\"\" get data from model generated text; we further used this script to get datas below. \"\"\"\nmodel_output = \"Epoch 1/8 - acc: 0.7090 - val_acc: 0.8205 Epoch 2/8 - acc: 0.8402 - val_acc: 0.8370 , ....\"\ndata_list = re.findall(r'acc: (d+.d+)', model_output)\ndata_list = list(map(float, data_list))\nprint(data_list[::2])\nprint(data_list[1::2])\n\n\"\"\" activation function data \"\"\"\ntrain_hard_sigmoid = pd.DataFrame({'hard_sigmoid': [0.72,0.8137,0.8403,0.8659,0.8892,0.8974,0.9116,0.9184]})\ntrain_sigmoid = pd.DataFrame({'sigmoid': [0.7316,0.8358,0.8737,0.8979,0.9180,0.9298,0.9423,0.9499]})\ntrain_tanh = pd.DataFrame({'tanh': [0.6574,0.7830,0.8335,0.8710,0.8882,0.8954,0.9052,0.8746]})\ntrain_relu = pd.DataFrame({'relu': [0.6524,0.7396,0.7273,0.5278,0.5663,0.6519,0.8128,0.8407]})\nactivation_train = pd.concat([train_hard_sigmoid,train_sigmoid,train_tanh,train_relu], axis = 1)\n\ntest_hard_sigmoid = pd.DataFrame({'hard_sigmoid': [0.7417,0.8195,0.8336,0.8351,0.8280,0.8241,0.8282,0.8302]})\ntest_sigmoid = pd.DataFrame({'sigmoid': [0.7898,0.8370,0.8360,0.8322,0.8330,0.8362,0.8280,0.8266]})\ntest_tanh = pd.DataFrame({'tanh': [0.7883,0.7926,0.8153,0.8193,0.7427,0.7679,0.7967,0.7760]})\ntest_relu = pd.DataFrame({'relu': [0.7634,0.7986,0.5040,0.4519,0.5000,0.7477,0.7482,0.7388]})\nactivation_test = pd.concat([test_hard_sigmoid,test_sigmoid, test_tanh, test_relu], axis = 1)\n\n\n\"\"\" optimizer data \"\"\"\ntrain_adam = pd.DataFrame({'adam': [0.7316, 0.8358, 0.8737, 0.8979, 0.918, 0.9298, 0.9423, 0.9499]})\ntrain_RMSprop = pd.DataFrame({'RMSprop': [0.7357, 0.8212, 0.8461, 0.8664, 0.8764, 0.8844, 0.8927, 0.8998]})\ntrain_Adagrad = pd.DataFrame({'Adagrad': [0.7349, 0.8347, 0.8654, 0.8828, 0.8963, 0.902, 0.9118, 0.9164]})\ntrain_Nadam = pd.DataFrame({'Nadam': [0.709, 0.8402, 0.8871, 0.9174, 0.9358, 0.947, 0.957, 0.9625]})\noptimizer_train = pd.concat([train_adam,train_RMSprop,train_Adagrad,train_Nadam], axis = 1)\n\n\ntest_adam = pd.DataFrame({'adam': [0.7898, 0.837, 0.836, 0.8322, 0.833, 0.8362, 0.828, 0.8266]})\ntest_RMSprop = pd.DataFrame({'RMSprop': [0.8288, 0.831, 0.8512, 0.8317, 0.8529, 0.8401, 0.847, 0.849]})\ntest_Adagrad = pd.DataFrame({'Adagrad': [0.8264, 0.7822, 0.8432, 0.8435, 0.838, 0.8372, 0.8384, 0.835]})\ntest_Nadam = pd.DataFrame({'Nadam': [0.8205, 0.837, 0.833, 0.8366, 0.8325, 0.8289, 0.8276, 0.8238]})\noptimizer_test = pd.concat([test_adam,test_RMSprop,test_Adagrad,test_Nadam], axis = 1)\n\n\n\"\"\" preprocessing data \"\"\"\ntrain_max_len_120 = pd.DataFrame({'max_len_120': [0.7041, 0.8279, 0.8577, 0.8767, 0.889, 0.8983, 0.9042, 0.9142, 0.9198, 0.9247]})\ntrain_max_len_160 = pd.DataFrame({'max_len_160': [0.722, 0.8315, 0.863, 0.8832, 0.8937, 0.9032, 0.9124, 0.9202, 0.9259, 0.9305]})\ntrain_max_len_200 = pd.DataFrame({'max_len_200': [0.7176, 0.8294, 0.8623, 0.885, 0.8975, 0.9068, 0.9131, 0.9214, 0.9265, 0.9327]})\ntrain_max_len = pd.concat([train_max_len_120,train_max_len_160,train_max_len_200], axis = 1)\n\ntest_max_len_120 = pd.DataFrame({'max_len_120': [0.8318, 0.8538, 0.8657, 0.8227, 0.8681, 0.8693, 0.8731, 0.8708, 0.8718, 0.8584]})\ntest_max_len_160 = pd.DataFrame({'max_len_160': [0.8481, 0.694, 0.8731, 0.8752, 0.8768, 0.8794, 0.8779, 0.88, 0.8834, 0.8792]})\ntest_max_len_200 = pd.DataFrame({'max_len_200': [0.8254, 0.8498, 0.877, 0.8671, 0.8828, 0.8854, 0.8891, 0.8862, 0.8877, 0.8769]})\ntest_max_len = pd.concat([test_max_len_120,test_max_len_160,test_max_len_200], axis = 1)\n\ntrain_max_features_4000 = pd.DataFrame({'max_features_4000': [0.7287, 0.8087, 0.8272, 0.8416, 0.8522, 0.857, 0.8608, 0.8672, 0.8729, 0.8763]})\ntrain_max_features_3000 = pd.DataFrame({'max_features_3000': [0.7218, 0.8039, 0.823, 0.8349, 0.8422, 0.8492, 0.8562, 0.8607, 0.8638, 0.8696]})\ntrain_max_features_2000 = pd.DataFrame({'max_features_2000': [0.7016, 0.7971, 0.8116, 0.8214, 0.8314, 0.8364, 0.8417, 0.8476, 0.8466, 0.8499]})\ntrain_max_features = pd.concat([train_max_features_4000,train_max_features_3000,train_max_features_2000], axis = 1)\n\ntest_max_features_4000 = pd.DataFrame({'max_features_4000': [0.7834, 0.821, 0.8296, 0.8473, 0.8521, 0.8435, 0.8461, 0.85, 0.8545, 0.8486]})\ntest_max_features_3000 = pd.DataFrame({'max_features_3000': [0.7859, 0.824, 0.8231, 0.8448, 0.8465, 0.84, 0.8501, 0.8412, 0.8488, 0.8536]})\ntest_max_features_2000 = pd.DataFrame({'max_features_2000': [0.8152, 0.8274, 0.83, 0.8392, 0.8268, 0.8376, 0.8452, 0.8457, 0.8424, 0.8413]})\ntest_max_features = pd.concat([test_max_features_4000,test_max_features_3000,test_max_features_2000], axis = 1)\n\n\"\"\" plotting \"\"\"\n%matplotlib inline\nf1, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\nf1.set_figwidth(15)\nactivation_train.plot(ax = ax1, title = \"activation_train\")\nactivation_test.plot(ax = ax2, title = \"activation_test\")\n\nf2, (ax3, ax4) = plt.subplots(1, 2, sharey=True)\nf2.set_figwidth(15)\noptimizer_train.plot(ax = ax3,title = \"optimizer_train\")\noptimizer_test.plot(ax = ax4,title = \"optimizer_test\")\n\nf3, (ax5, ax6) = plt.subplots(1, 2, sharey=True)\nf3.set_figwidth(15)\ntrain_max_len.plot(ax = ax5,title = \"train_max_len\")\ntest_max_len.plot(ax = ax6,title = \"test_max_len\")\n\nf4, (ax7, ax8) = plt.subplots(1, 2, sharey=True)\nf4.set_figwidth(15)\ntrain_max_features.plot(ax = ax7,title = \"train_max_features\")\ntest_max_features.plot(ax = ax8,title = \"test_max_features\")\n```\n\n    [0.709, 0.8402]\n    [0.8205, 0.837]\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x1d192d00f28>\n\n![png]("+t("CrrM")+")\n\n\n\n![png]("+t("EpOa")+")\n\n\n\n![png]("+t("Ngq9")+")\n\n\n\n![png]("+t("BI+w")+")\n\n\n## Building New Model \nAfter such comparision, we decided to build model using *RMSprop* optimizer, *sigmoid* activation function, *max_features* = 3000, and *max_len*= 200.\n\n\n```python\nfrom __future__ import print_function\nimport numpy as np\nnp.random.seed(1337)  # for reproducibility\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import LSTM, SimpleRNN, GRU\nfrom keras.datasets import imdb\nfrom keras.layers.wrappers import TimeDistributed\nimport pickle\nfrom six.moves import cPickle\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\n\nmax_features = 3000\nmaxlen = 200  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\nprint('Loading data...')\n(XX_train, y_train), (XX_test, y_test) = imdb.load_data(path='imdb_full.pkl', nb_words=max_features, skip_top=10,\n              maxlen=None, seed=113,\n              start_char=1, oov_char=2, index_from=0)\nprint(len(XX_train), 'train sequences')\nprint(len(XX_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nX_train = sequence.pad_sequences(XX_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(XX_test, maxlen=maxlen)\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\n\nprint('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, dropout=0.2))\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\n          validation_data=(X_test, y_test))\n```\n\n    Loading data...\n    25000 train sequences\n    25000 test sequences\n    Pad sequences (samples x time)\n    X_train shape: (25000, 200)\n    X_test shape: (25000, 200)\n    Build model...\n    Train...\n    Train on 25000 samples, validate on 25000 samples\n    Epoch 1/10\n    25000/25000 [==============================] - 288s - loss: 0.5716 - acc: 0.6874 - val_loss: 0.3979 - val_acc: 0.8376\n    Epoch 2/10\n    25000/25000 [==============================] - 289s - loss: 0.4373 - acc: 0.8071 - val_loss: 0.3447 - val_acc: 0.8603\n    Epoch 3/10\n    25000/25000 [==============================] - 297s - loss: 0.3914 - acc: 0.8336 - val_loss: 0.3410 - val_acc: 0.8577\n    Epoch 4/10\n    25000/25000 [==============================] - 301s - loss: 0.3526 - acc: 0.8508 - val_loss: 0.3228 - val_acc: 0.8604\n    Epoch 5/10\n    25000/25000 [==============================] - 305s - loss: 0.3224 - acc: 0.8631 - val_loss: 0.2915 - val_acc: 0.8782\n    Epoch 6/10\n    25000/25000 [==============================] - 302s - loss: 0.3031 - acc: 0.8756 - val_loss: 0.3119 - val_acc: 0.8671\n    Epoch 7/10\n    25000/25000 [==============================] - 305s - loss: 0.2868 - acc: 0.8830 - val_loss: 0.2771 - val_acc: 0.8833\n    Epoch 8/10\n    25000/25000 [==============================] - 290s - loss: 0.2745 - acc: 0.8867 - val_loss: 0.2739 - val_acc: 0.8842\n    Epoch 9/10\n    25000/25000 [==============================] - 275s - loss: 0.2649 - acc: 0.8904 - val_loss: 0.2715 - val_acc: 0.8844\n    Epoch 10/10\n    25000/25000 [==============================] - 284s - loss: 0.2587 - acc: 0.8937 - val_loss: 0.2718 - val_acc: 0.8875\n    \n\n\n\n\n    <keras.callbacks.History at 0x1d1950c3588>\n\n\n\nAs we can see, our new model has a better prediction of 88.75% accuracy versus the original model.\n\n## Further Applications\nVisualization of sentiment analysis ,inspired by [Taylor Arnold's jupyter notebook](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/notebook21.html) [9]\n\n\n```python\n%matplotlib inline\nfrom keras.layers.wrappers import TimeDistributed\nimport pickle\nfrom six.moves import cPickle\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\ndef reconstruct_text(index, index_to_word):\n    text = []\n    for ind in index:\n        if ind != 0:\n            text += [index_to_word[ind]]\n        else:\n            text += [\"\"]\n    return text\n\nword_to_index = imdb.get_word_index()\nindex_to_word = {k:v for v,k in word_to_index.items()}\nf = open('imdb_full.pkl', 'rb')\n(x_train, labels_train), (x_test, labels_test) = cPickle.load(f)\nf.close()\ndf = pd.DataFrame(x_train)\n\nmodel2 = Sequential()\nmodel2.add(Embedding(max_features, 128, dropout=0.2))\nmodel2.add(LSTM(128, dropout_W=0.2, dropout_U=0.2, return_sequences=True))  # try using a GRU instead, for fun\nmodel2.add(TimeDistributed(Dense(1)))\nmodel2.add(Activation('sigmoid'))\nmodel2.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel2.set_weights(model.get_weights())\ny_hat2 = model2.predict(X_train)\n\nind = 100\ntokens = reconstruct_text(X_train[ind], index_to_word)\n\nplt.figure(figsize=(16, 10))\nplt.plot(y_hat2[ind],alpha=0.5)\nfor i in range(len(tokens)):\n    plt.text(i,0.5,tokens[i],rotation=90)\n```\n\n\n![png]("+t("jI8F")+")\n\n\n## Apply Current Model to Amazon Review Data\nNow that we have successfully improved the model, we further apply our new model to Amazon Review data provided by Dr. Allen.\nAmazon Review Dataset has preclassified sentiment value on 6 categories: books, camera, dvd, health, music, software. We need to complie data in a specific way so that our current model can directly use it.\n\n### 1. Change File Structure\nWe will split the reviews into training set and testing set by changing the file structure\n\n\n```python\nfrom IPython.display import Image\ni = Image(filename='file_structure.png')\ni\n```\n\n\n\n\n![png]("+t("/DX2")+")\n\n\n\n### 2. Preprocess Amazon Reviews\n**Load reviews:**\n\n\n```python\nfrom six.moves import cPickle\nimport numpy\nimport os\nimport pandas as pd\nimport string\nfrom collections import Counter\nfrom keras.preprocessing.text import Tokenizer\n\npath = [\"books/\", \"camera/\", \"dvd/\", \"health/\", \"music/\", \"software/\"]  # camera, has missing data\nff = []\ninput_label = []\nfor i in range(6):\n    ff += [path[i] + \"train/pos/\" + x for x in os.listdir(path[i] + \"train/pos\")] +          [path[i] + \"train/neg/\" + x for x in os.listdir(path[i] + \"train/neg\")] +          [path[i] + \"test/pos/\" + x for x in os.listdir(path[i] + \"test/pos\")] +          [path[i] + \"test/neg/\" + x for x in os.listdir(path[i] + \"test/neg\")]\n    \n    # Because of missing data, we need to measure how many reviews are there in each folder in order to label them correctly.\n    train_pos = len(os.listdir(path[i] + \"train/pos\"))\n    train_neg = len(os.listdir(path[i] + \"train/neg\"))\n    test_pos = len(os.listdir(path[i] + \"test/pos\"))\n    test_neg = len(os.listdir(path[i] + \"test/neg\"))\n    input_label += [1] * train_pos + [0] * train_neg + [1] * test_pos + [0] * test_neg\n    \n      \ninput_text  = []\nfor f in ff:\n    with open(f, 'rb') as fin:\n        temp = fin.read().splitlines()\n        x = \" \".join([x.decode(\"utf-8\", errors = 'ignore') for x in temp])\n        input_text += [x]\n        \nprint(input_text[0])\n```\n\n    I wish I could give this book four and a half stars instead of four; I can't quite justify five stars in my mind.  Two of the stories were definitely good: the first, \"masked Riders\" by Parhelion, and the third, \"Ricochet\" by BA Tortuga.  I enjoyed both, but was not particularly snowed by the intensity of the conflict/plot line or the main characters and their relationships.  Not so the second story: \"Hung Up\" by Cat Kane was a powerful, intense and moving story about two delightful yet flawed characters who had secrets that tore their relationship up until they were able to work their ways (separately) around the issues.  That story deserved a five stars plus rating and I wish Kane would turn it into a full length novel and develop the characters and their backgrounds more.  This book is an excellent read both for the plot line and the erotic substance.  Enjoy\n    \n\n**Tokenize reviews:**\n\n\n```python\ninput_text  = []\nfor f in ff:\n    with open(f, 'rb') as fin:\n        temp = fin.read().splitlines()\n        x = \" \".join([x.decode(\"utf-8\", errors = 'ignore') for x in temp])\n        input_text += [x]\n\ncut_index = int(len(input_text)/2)\ntok = Tokenizer()\ntok.fit_on_texts(input_text[:cut_index])\n\nX_train = tok.texts_to_sequences(input_text[:cut_index])\nX_test  = tok.texts_to_sequences(input_text[cut_index:])\ny_train = input_label[:cut_index]\ny_test  = input_label[cut_index:]\n\nprint(\"As you can see, the first review has been encoded as their word index:\")\nprint(X_train[0][:10])\n```\n\n    As you can see, the first review has been encoded as their word index:\n    [6, 499, 6, 99, 221, 9, 22, 501, 2, 3]\n    \n\n**Reconstruct reviews:**\n\n\n```python\nwords = {k:v for v,k in tok.word_index.items()}\ndef reconstruct_text(index, words):\n    text = []\n    for ind in index:\n        if ind != 0:\n            text += [words[ind]]\n        else:\n            text += [\"\"]\n    return text\n    \nprint(input_text[100])\nprint(reconstruct_text(X_train[100], words))\n```\n\n    Peck relates growing up in rural/small town Vermont with a best friend who gets him into lots of trouble. Humor and pranks abound in between lessons learned\n    ['peck', 'relates', 'growing', 'up', 'in', 'rural', 'small', 'town', 'vermont', 'with', 'a', 'best', 'friend', 'who', 'gets', 'him', 'into', 'lots', 'of', 'trouble', 'humor', 'and', 'pranks', 'abound', 'in', 'between', 'lessons', 'learned']\n    \n\n**Store reviews: **\n\n\n```python\nf = open('amzn_full.pkl', 'wb')\ntrain_tuple = (X_train, y_train)\ntest_tuple = (X_test, y_test)\ncombine = (train_tuple, test_tuple)\ncPickle.dump(combine, f)\nf.close()\n```\n\n### 3. Apply Current Model to Amazon Reviews:\nFirstly we need to make a copy of [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py)(the script that loads imdb_full.pkl data) and make it loads our amzn_full.pkl instead. Name the new file amzn.py. Then we apply our model:\n\n\n```python\nfrom __future__ import print_function\nimport numpy as np\nnp.random.seed(1337)  # for reproducibility\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import LSTM, SimpleRNN, GRU\nimport amzn\nfrom keras.layers.wrappers import TimeDistributed\nimport pickle\nfrom six.moves import cPickle\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\n\nmax_features = 3000\nmaxlen = 200  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\nprint('Loading data...')\n(XX_train, y_train), (XX_test, y_test) = amzn.load_data(path='amzn_full.pkl', nb_words=max_features)\nprint(len(XX_train), 'train sequences')\nprint(len(XX_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nX_train = sequence.pad_sequences(XX_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(XX_test, maxlen=maxlen)\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\n\nprint('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, dropout=0.2))\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy',\n              optimizer='RMSprop',\n              metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\n          validation_data=(X_test, y_test))\n```\n\n    Loading data...\n    5957 train sequences\n    5957 test sequences\n    Pad sequences (samples x time)\n    X_train shape: (5957, 200)\n    X_test shape: (5957, 200)\n    Build model...\n    Train...\n    Train on 5957 samples, validate on 5957 samples\n    Epoch 1/10\n    5957/5957 [==============================] - 63s - loss: 0.6823 - acc: 0.5587 - val_loss: 0.6493 - val_acc: 0.6325\n    Epoch 2/10\n    5957/5957 [==============================] - 63s - loss: 0.5696 - acc: 0.7082 - val_loss: 0.5350 - val_acc: 0.7386\n    Epoch 3/10\n    5957/5957 [==============================] - 68s - loss: 0.4870 - acc: 0.7692 - val_loss: 0.5081 - val_acc: 0.7480\n    Epoch 4/10\n    5957/5957 [==============================] - 70s - loss: 0.4290 - acc: 0.8054 - val_loss: 0.8713 - val_acc: 0.6157\n    Epoch 5/10\n    5957/5957 [==============================] - 60s - loss: 0.3910 - acc: 0.8348 - val_loss: 0.5099 - val_acc: 0.7646\n    Epoch 6/10\n    5957/5957 [==============================] - 60s - loss: 0.3738 - acc: 0.8385 - val_loss: 0.5034 - val_acc: 0.7702\n    Epoch 7/10\n    5957/5957 [==============================] - 59s - loss: 0.3465 - acc: 0.8531 - val_loss: 0.4990 - val_acc: 0.7875\n    Epoch 8/10\n    5957/5957 [==============================] - 62s - loss: 0.3143 - acc: 0.8724 - val_loss: 0.5216 - val_acc: 0.7747\n    Epoch 9/10\n    5957/5957 [==============================] - 64s - loss: 0.2992 - acc: 0.8795 - val_loss: 0.4989 - val_acc: 0.7900\n    Epoch 10/10\n    5957/5957 [==============================] - 64s - loss: 0.2846 - acc: 0.8815 - val_loss: 0.4971 - val_acc: 0.7890\n    \n\n\n\n\n    <keras.callbacks.History at 0x1d1f921a668>\n\n\n\n## Compare Result to Original Model\n\n\n```python\nfrom __future__ import print_function\nimport numpy as np\nnp.random.seed(1337)  # for reproducibility\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import LSTM, SimpleRNN, GRU\nimport amzn\n\nmax_features = 20000\nmaxlen = 80  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\nprint('Loading data...')\n(X_train, y_train), (X_test, y_test) = amzn.load_data(path='amzn_full.pkl', nb_words=max_features)\nprint(len(X_train), 'train sequences')\nprint(len(X_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\n\nprint('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, dropout=0.2))\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\n          validation_data=(X_test, y_test))\n```\n\n    Loading data...\n    5957 train sequences\n    5957 test sequences\n    Pad sequences (samples x time)\n    X_train shape: (5957, 80)\n    X_test shape: (5957, 80)\n    Build model...\n    Train...\n    Train on 5957 samples, validate on 5957 samples\n    Epoch 1/10\n    5957/5957 [==============================] - 31s - loss: 0.6641 - acc: 0.5949 - val_loss: 0.5708 - val_acc: 0.7077\n    Epoch 2/10\n    5957/5957 [==============================] - 31s - loss: 0.4679 - acc: 0.7823 - val_loss: 0.5437 - val_acc: 0.7326\n    Epoch 3/10\n    5957/5957 [==============================] - 31s - loss: 0.3293 - acc: 0.8687 - val_loss: 0.5521 - val_acc: 0.7521\n    Epoch 4/10\n    5957/5957 [==============================] - 31s - loss: 0.2428 - acc: 0.9077 - val_loss: 0.5821 - val_acc: 0.7601\n    Epoch 5/10\n    5957/5957 [==============================] - 31s - loss: 0.1790 - acc: 0.9347 - val_loss: 0.7185 - val_acc: 0.7408\n    Epoch 6/10\n    5957/5957 [==============================] - 31s - loss: 0.1270 - acc: 0.9542 - val_loss: 0.7469 - val_acc: 0.7519\n    Epoch 7/10\n    5957/5957 [==============================] - 32s - loss: 0.1136 - acc: 0.9599 - val_loss: 0.8070 - val_acc: 0.7430\n    Epoch 8/10\n    5957/5957 [==============================] - 31s - loss: 0.0890 - acc: 0.9694 - val_loss: 0.9022 - val_acc: 0.7521\n    Epoch 9/10\n    5957/5957 [==============================] - 31s - loss: 0.0721 - acc: 0.9758 - val_loss: 0.9214 - val_acc: 0.7275\n    Epoch 10/10\n    5957/5957 [==============================] - 32s - loss: 0.0583 - acc: 0.9802 - val_loss: 0.9235 - val_acc: 0.7334\n    \n\n\n\n\n    <keras.callbacks.History at 0x1d20002c588>\n\n\n\n\n```python\n%%html\n<style>\ntable {float:left}\n</style>\n```\n\n\n<style>\ntable {float:left}\n</style>\n\n\n# Conclusion\nOur new model is based on two main hypothesis:\n 1. It is reasonable to assume when people write reviews, they would express their opinion at the start more often than at the end. So we can limit each review to some *\"maxlen\"* to eliminate noisy data. \n 2. Some words are essential than other words for expressing sentiment. For example, \"love\" would express more emotion than \"keyboard\". We therefore limit *\"max_features\"* to eliminate less sentimentally expresed words.\n\n\nTherefore by experimenting different *\"maxlen\"*, *\"max_features\"* , and optimizers, our new model improves the prediction accuracy on both IMDB and Amazon Review dataset, hence validating our hypothesis.\n\n\n\n|                        | Original Model Provided by Keras| Our New Model\n| :------                | :-----------                    | ----       |\n| **Amazon Review **     | 73.34%                          | 78.90%     |\n| **IMDB Movie Review ** | 82.35%                          | 88.75%     |\n\n\n\n<br/>\n<br/>\n<br/>\n\n# Reference\n\n\n[1] Fran¸cois Chollet. Keras. [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py), 2015 \n\n[2] Fran¸cois Chollet. Keras. [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py), 2015 \n\n[3] \"Convolutional Neural Networks for Visual Recognition.\" Web. 06 Dec. 2016. (http://cs231n.github.io/neural-networks-1).\n\n[4] Ruder, Sebastian. \"An Overview of Gradient Descent Optimization Algorithms.\" 30 Sept. 2016. Web. (http://sebastianruder.com/optimizing-gradient-descent/).\n\n[5] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas//papers/wvSent_acl2011.pdf). *The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)*, 2011\n\n[6] T Mikolov, I Sutskever, K Chen, GS Corrado, J Dean - Advances in neural information processing systems, [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), 2013\n\n[7] Hochreiter, S., & Schmidhuber, J. [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf). Neural computation, 9(8), 1735-1780. 1997\n\n[8] Olah C [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), 2015\n\n[9] Arnold T, *STAT 365/665: Data Mining and Machine Learning*, [Recurrent neural networks](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/notebook21.html), 2016\n";n.a={data:function(){return{articleStr:a}}}},Ngq9:function(e,n,t){e.exports=t.p+"img/output_11_4.f5d8933.png"},fjo1:function(e,n,t){"use strict";Object.defineProperty(n,"__esModule",{value:!0});var a=t("FIMK"),s=t("ukiP"),i=t("VU/8"),o=i(a.a,s.a,null,null,null);n.default=o.exports},jI8F:function(e,n,t){e.exports=t.p+"img/output_16_0.a4a67e4.png"},ukiP:function(e,n,t){"use strict";var a=function(){var e=this,n=e.$createElement;return(e._self._c||n)("article-view",{attrs:{article:e.articleStr}})},s=[],i={render:a,staticRenderFns:s};n.a=i}});
//# sourceMappingURL=index.6452176b7dafefc296f7.js.map