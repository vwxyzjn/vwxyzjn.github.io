{"version":3,"sources":["webpack:///static/js/1.c694b068eb9bba984b39.js","webpack:///./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/article.vue?b15e","webpack:///./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/article.vue","webpack:///./~/prismjs/prism.js?5293","webpack:///./~/prismjs/themes/prism-coy.css?2e30","webpack:///./~/prismjs/themes/prism-coy.css?e5a5*","webpack:///./~/marked/lib/marked.js?bdc8","webpack:///./~/prismjs/components/prism-python.js?d569","webpack:///article.vue?285e","webpack:///./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/article.vue?979c","webpack:///./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/article.vue?1f62","webpack:///./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_11_2.png","webpack:///./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_11_3.png","webpack:///./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_11_4.png","webpack:///./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_11_5.png","webpack:///./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_16_0.png","webpack:///./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_18_0.png"],"names":["webpackJsonp","115","module","exports","render","_vm","this","_h","$createElement","_c","_self","staticClass","domProps","innerHTML","_s","rawHtml","staticRenderFns","15","__webpack_require__","Component","19","global","window","WorkerGlobalScope","self","Prism","lang","uniqueId","_","util","encode","tokens","Token","type","content","alias","map","replace","o","Object","prototype","toString","call","match","objId","obj","defineProperty","value","clone","key","hasOwnProperty","v","languages","extend","id","redef","insertBefore","inside","before","insert","root","grammar","arguments","length","newToken","ret","token","DFS","callback","visited","i","plugins","highlightAll","async","env","selector","hooks","run","element","elements","document","querySelectorAll","highlightElement","language","parent","test","className","parentNode","toLowerCase","nodeName","code","textContent","Worker","worker","filename","onmessage","evt","highlightedCode","data","postMessage","JSON","stringify","immediateClose","highlight","text","tokenize","strarr","rest","tokenloop","patterns","j","pattern","lookbehind","greedy","lookbehindLength","flags","RegExp","source","pos","str","lastIndex","exec","delNum","from","index","to","k","p","len","slice","after","args","push","wrapped","Array","splice","apply","all","add","name","callbacks","matchedStr","join","tag","classes","attributes","aliases","keys","addEventListener","message","parse","close","script","currentScript","getElementsByTagName","pop","src","hasAttribute","readyState","requestAnimationFrame","setTimeout","markup","comment","prolog","doctype","cdata","punctuation","namespace","attr-value","attr-name","entity","xml","html","mathml","svg","css","atrule","rule","url","string","property","important","function","style","style-attr","clike","class-name","keyword","boolean","number","operator","javascript","regex","template-string","interpolation","interpolation-punctuation","js","querySelector","fileHighlight","Extensions","py","rb","ps1","psm1","sh","bat","h","tex","forEach","pre","getAttribute","extension","createElement","appendChild","xhr","XMLHttpRequest","open","onreadystatechange","status","responseText","statusText","send","20","version","sources","names","mappings","file","sourcesContent","sourceRoot","21","locals","22","Lexer","options","links","marked","defaults","rules","block","normal","gfm","tables","InlineLexer","inline","renderer","Renderer","Error","breaks","pedantic","Parser","escape","unescape","n","charAt","String","fromCharCode","parseInt","substring","opt","val","noop","merge","target","pending","lex","e","done","err","out","escaped","silent","newline","fences","hr","heading","nptable","lheading","blockquote","list","def","table","paragraph","bullet","item","_tag","top","bq","next","loose","cap","bull","b","space","l","depth","header","split","align","cells","ordered","indexOf","smartLists","sanitize","sanitizer","href","title","charCodeAt","autolink","link","reflink","nolink","strong","em","br","del","_inside","_href","output","mangle","inLink","outputLink","codespan","smartypants","image","ch","Math","random","langPrefix","quote","level","raw","headerPrefix","xhtml","body","listitem","tablerow","tablecell","prot","decodeURIComponent","reverse","tok","peek","parseText","row","cell","setOptions","parser","lexer","inlineLexer","23","python","triple-quoted-string","40","__webpack_exports__","__WEBPACK_IMPORTED_MODULE_0_marked__","__WEBPACK_IMPORTED_MODULE_0_marked___default","__WEBPACK_IMPORTED_MODULE_1_prismjs__","__WEBPACK_IMPORTED_MODULE_1_prismjs___default","__WEBPACK_IMPORTED_MODULE_2_prismjs_themes_prism_coy_css__","__WEBPACK_IMPORTED_MODULE_3_prismjs_prism_js__","__WEBPACK_IMPORTED_MODULE_4_prismjs_components_prism_python_js__","article","mounted","a","48","53","57","58","59","60","61","62"],"mappings":"AAAAA,cAAc,IAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAgBC,OAAA,WAAmB,GAAAC,GAAAC,KAAaC,EAAAF,EAAAG,eAA0BC,EAAAJ,EAAAK,MAAAD,IAAAF,CAC1E,OAAAE,GAAA,OACAE,YAAA,qBACGF,EAAA,OACHG,UACAC,UAAAR,EAAAS,GAAAT,EAAAU,eAGCC,qBDSKC,GACA,SAAUf,EAAQC,EAASe,GEhBjCA,EAAA,GAEA,IAAAC,GAAAD,EAAA,GAEAA,EAAA,IAEAA,EAAA,KAEA,kBAEA,KAGAhB,GAAAC,QAAAgB,EAAAhB,SFyBMiB,GACA,SAAUlB,EAAQC,EAASe,IAEL,SAASG,GGtCrC,GAAAX,GAAA,mBAAAY,QACAA,OAEA,mBAAAC,oBAAAC,eAAAD,mBACAC,QAUAC,EAAA,WAGA,GAAAC,GAAA,2BACAC,EAAA,EAEAC,EAAAlB,EAAAe,OACAI,MACAC,OAAA,SAAAC,GACA,MAAAA,aAAAC,GACA,GAAAA,GAAAD,EAAAE,KAAAL,EAAAC,KAAAC,OAAAC,EAAAG,SAAAH,EAAAI,OACI,UAAAP,EAAAC,KAAAI,KAAAF,GACJA,EAAAK,IAAAR,EAAAC,KAAAC,QAEAC,EAAAM,QAAA,cAAsCA,QAAA,aAAsBA,QAAA,gBAI5DJ,KAAA,SAAAK,GACA,MAAAC,QAAAC,UAAAC,SAAAC,KAAAJ,GAAAK,MAAA,wBAGAC,MAAA,SAAAC,GAIA,MAHAA,GAAA,MACAN,OAAAO,eAAAD,EAAA,QAAwCE,QAAApB,IAExCkB,EAAA,MAIAG,MAAA,SAAAV,GAGA,OAFAV,EAAAC,KAAAI,KAAAK,IAGA,aACA,GAAAU,KAEA,QAAAC,KAAAX,GACAA,EAAAY,eAAAD,KACAD,EAAAC,GAAArB,EAAAC,KAAAmB,MAAAV,EAAAW,IAIA,OAAAD,EAEA,aAEA,MAAAV,GAAAF,KAAAE,EAAAF,IAAA,SAAAe,GAAwC,MAAAvB,GAAAC,KAAAmB,MAAAG,KAGxC,MAAAb,KAIAc,WACAC,OAAA,SAAAC,EAAAC,GACA,GAAA7B,GAAAE,EAAAC,KAAAmB,MAAApB,EAAAwB,UAAAE,GAEA,QAAAL,KAAAM,GACA7B,EAAAuB,GAAAM,EAAAN,EAGA,OAAAvB,IAYA8B,aAAA,SAAAC,EAAAC,EAAAC,EAAAC,GACAA,KAAAhC,EAAAwB,SACA,IAAAS,GAAAD,EAAAH,EAEA,OAAAK,UAAAC,OAAA,CACAJ,EAAAG,UAAA,EAEA,QAAAE,KAAAL,GACAA,EAAAT,eAAAc,KACAH,EAAAG,GAAAL,EAAAK,GAIA,OAAAH,GAGA,GAAAI,KAEA,QAAAC,KAAAL,GAEA,GAAAA,EAAAX,eAAAgB,GAAA,CAEA,GAAAA,GAAAR,EAEA,OAAAM,KAAAL,GAEAA,EAAAT,eAAAc,KACAC,EAAAD,GAAAL,EAAAK,GAKAC,GAAAC,GAAAL,EAAAK,GAWA,MANAtC,GAAAwB,UAAAe,IAAAvC,EAAAwB,UAAA,SAAAH,EAAAF,GACAA,IAAAa,EAAAH,IAAAR,GAAAQ,IACAnD,KAAA2C,GAAAgB,KAIAL,EAAAH,GAAAQ,GAIAE,IAAA,SAAA7B,EAAA8B,EAAAnC,EAAAoC,GACAA,OACA,QAAAC,KAAAhC,GACAA,EAAAY,eAAAoB,KACAF,EAAA1B,KAAAJ,EAAAgC,EAAAhC,EAAAgC,GAAArC,GAAAqC,GAEA,WAAA1C,EAAAC,KAAAI,KAAAK,EAAAgC,KAAAD,EAAAzC,EAAAC,KAAAe,MAAAN,EAAAgC,KAIA,UAAA1C,EAAAC,KAAAI,KAAAK,EAAAgC,KAAAD,EAAAzC,EAAAC,KAAAe,MAAAN,EAAAgC,OACAD,EAAAzC,EAAAC,KAAAe,MAAAN,EAAAgC,MAAA,EACA1C,EAAAwB,UAAAe,IAAA7B,EAAAgC,GAAAF,EAAAE,EAAAD,KALAA,EAAAzC,EAAAC,KAAAe,MAAAN,EAAAgC,MAAA,EACA1C,EAAAwB,UAAAe,IAAA7B,EAAAgC,GAAAF,EAAA,KAAAC,OAUAE,WAEAC,aAAA,SAAAC,EAAAL,GACA,GAAAM,IACAN,WACAO,SAAA,mGAGA/C,GAAAgD,MAAAC,IAAA,sBAAAH,EAIA,QAAAI,GAFAC,EAAAL,EAAAK,UAAAC,SAAAC,iBAAAP,EAAAC,UAEAL,EAAA,EAAwBQ,EAAAC,EAAAT,MACxB1C,EAAAsD,iBAAAJ,GAAA,IAAAL,EAAAC,EAAAN,WAIAc,iBAAA,SAAAJ,EAAAL,EAAAL,GAIA,IAFA,GAAAe,GAAAtB,EAAAuB,EAAAN,EAEAM,IAAA1D,EAAA2D,KAAAD,EAAAE,YACAF,IAAAG,UAGAH,KACAD,GAAAC,EAAAE,UAAA3C,MAAAjB,KAAA,SAAA8D,cACA3B,EAAAjC,EAAAwB,UAAA+B,IAIAL,EAAAQ,UAAAR,EAAAQ,UAAAjD,QAAAX,EAAA,IAAAW,QAAA,yBAAA8C,EAGAC,EAAAN,EAAAS,WAEA,OAAAF,KAAAD,EAAAK,YACAL,EAAAE,UAAAF,EAAAE,UAAAjD,QAAAX,EAAA,IAAAW,QAAA,yBAAA8C,EAGA,IAAAO,GAAAZ,EAAAa,YAEAjB,GACAI,UACAK,WACAtB,UACA6B,OAKA,IAFA9D,EAAAgD,MAAAC,IAAA,sBAAAH,IAEAA,EAAAgB,OAAAhB,EAAAb,QAKA,MAJAa,GAAAgB,OACAhB,EAAAI,QAAAa,YAAAjB,EAAAgB,UAEA9D,GAAAgD,MAAAC,IAAA,WAAAH,EAMA,IAFA9C,EAAAgD,MAAAC,IAAA,mBAAAH,GAEAD,GAAA/D,EAAAkF,OAAA,CACA,GAAAC,GAAA,GAAAD,QAAAhE,EAAAkE,SAEAD,GAAAE,UAAA,SAAAC,GACAtB,EAAAuB,gBAAAD,EAAAE,KAEAtE,EAAAgD,MAAAC,IAAA,gBAAAH,GAEAA,EAAAI,QAAAjE,UAAA6D,EAAAuB,gBAEA7B,KAAA1B,KAAAgC,EAAAI,SACAlD,EAAAgD,MAAAC,IAAA,kBAAAH,GACA9C,EAAAgD,MAAAC,IAAA,WAAAH,IAGAmB,EAAAM,YAAAC,KAAAC,WACAlB,SAAAT,EAAAS,SACAO,KAAAhB,EAAAgB,KACAY,gBAAA,SAIA5B,GAAAuB,gBAAArE,EAAA2E,UAAA7B,EAAAgB,KAAAhB,EAAAb,QAAAa,EAAAS,UAEAvD,EAAAgD,MAAAC,IAAA,gBAAAH,GAEAA,EAAAI,QAAAjE,UAAA6D,EAAAuB,gBAEA7B,KAAA1B,KAAAoC,GAEAlD,EAAAgD,MAAAC,IAAA,kBAAAH,GACA9C,EAAAgD,MAAAC,IAAA,WAAAH,IAIA6B,UAAA,SAAAC,EAAA3C,EAAAsB,GACA,GAAApD,GAAAH,EAAA6E,SAAAD,EAAA3C,EACA,OAAA7B,GAAAqE,UAAAzE,EAAAC,KAAAC,OAAAC,GAAAoD,IAGAsB,SAAA,SAAAD,EAAA3C,EAAAsB,GACA,GAAAnD,GAAAJ,EAAAI,MAEA0E,GAAAF,GAEAG,EAAA9C,EAAA8C,IAEA,IAAAA,EAAA,CACA,OAAAzC,KAAAyC,GACA9C,EAAAK,GAAAyC,EAAAzC,SAGAL,GAAA8C,KAGAC,EAAA,OAAA1C,KAAAL,GACA,GAAAA,EAAAX,eAAAgB,IAAAL,EAAAK,GAAA,CAIA,GAAA2C,GAAAhD,EAAAK,EACA2C,GAAA,UAAAjF,EAAAC,KAAAI,KAAA4E,QAEA,QAAAC,GAAA,EAAkBA,EAAAD,EAAA9C,SAAqB+C,EAAA,CACvC,GAAAC,GAAAF,EAAAC,GACArD,EAAAsD,EAAAtD,OACAuD,IAAAD,EAAAC,WACAC,IAAAF,EAAAE,OACAC,EAAA,EACA/E,EAAA4E,EAAA5E,KAEA,IAAA8E,IAAAF,UAAA1F,OAAA,CAEA,GAAA8F,GAAAJ,UAAAtE,WAAAE,MAAA,cACAoE,WAAAK,OAAAL,UAAAM,OAAAF,EAAA,KAGAJ,cAGA,QAAAzC,GAAA,EAAAgD,EAAA,EAA0BhD,EAAAoC,EAAA3C,OAAiBuD,GAAAZ,EAAApC,GAAAP,SAAAO,EAAA,CAE3C,GAAAiD,GAAAb,EAAApC,EAEA,IAAAoC,EAAA3C,OAAAyC,EAAAzC,OAEA,KAAA6C,EAGA,MAAAW,YAAAvF,IAAA,CAIA+E,EAAAS,UAAA,CAEA,IAAA7E,GAAAoE,EAAAU,KAAAF,GACAG,EAAA,CAGA,KAAA/E,GAAAsE,GAAA3C,GAAAoC,EAAA3C,OAAA,GAGA,GAFAgD,EAAAS,UAAAF,IACA3E,EAAAoE,EAAAU,KAAAjB,IAEA,KAQA,QALAmB,GAAAhF,EAAAiF,OAAAZ,EAAArE,EAAA,GAAAoB,OAAA,GACA8D,EAAAlF,EAAAiF,MAAAjF,EAAA,GAAAoB,OACA+D,EAAAxD,EACAyD,EAAAT,EAEAU,EAAAtB,EAAA3C,OAAmC+D,EAAAE,GAAAD,EAAAF,IAAmBC,EACtDC,GAAArB,EAAAoB,GAAA/D,OAEA4D,GAAAI,MACAzD,EACAgD,EAAAS,EAQA,IAAArB,EAAApC,YAAAtC,IAAA0E,EAAAoB,EAAA,GAAAb,OACA,QAIAS,GAAAI,EAAAxD,EACAiD,EAAAf,EAAAyB,MAAAX,EAAAS,GACApF,EAAAiF,OAAAN,EAGA,GAAA3E,EAAA,CAIAqE,IACAE,EAAAvE,EAAA,GAAAoB,OAGA,IAAA4D,GAAAhF,EAAAiF,MAAAV,EACAvE,IAAA,GAAAsF,MAAAf,GACAW,EAAAF,EAAAhF,EAAAoB,OACAL,EAAA6D,EAAAU,MAAA,EAAAN,GACAO,EAAAX,EAAAU,MAAAJ,GAEAM,GAAA7D,EAAAoD,EAEAhE,IACAyE,EAAAC,KAAA1E,EAGA,IAAA2E,GAAA,GAAArG,GAAAkC,EAAAT,EAAA7B,EAAA6E,SAAA9D,EAAAc,GAAAd,EAAAR,EAAAQ,EAAAsE,EAEAkB,GAAAC,KAAAC,GAEAH,GACAC,EAAAC,KAAAF,GAGAI,MAAA9F,UAAA+F,OAAAC,MAAA9B,EAAAyB,OAKA,MAAAzB,IAGA9B,OACA6D,OAEAC,IAAA,SAAAC,EAAAvE,GACA,GAAAQ,GAAAhD,EAAAgD,MAAA6D,GAEA7D,GAAA+D,GAAA/D,EAAA+D,OAEA/D,EAAA+D,GAAAP,KAAAhE,IAGAS,IAAA,SAAA8D,EAAAjE,GACA,GAAAkE,GAAAhH,EAAAgD,MAAA6D,IAAAE,EAEA,IAAAC,KAAA7E,OAIA,OAAAK,GAAAE,EAAA,EAA0BF,EAAAwE,EAAAtE,MAC1BF,EAAAM,MAMA1C,EAAAJ,EAAAI,MAAA,SAAAC,EAAAC,EAAAC,EAAA0G,EAAA5B,GACA3G,KAAA2B,OACA3B,KAAA4B,UACA5B,KAAA6B,QAEA7B,KAAAyD,OAAA,GAAA8E,GAAA,IAAA9E,OACAzD,KAAA2G,WA2CA,IAxCAjF,EAAAqE,UAAA,SAAA/D,EAAA6C,EAAAC,GACA,mBAAA9C,GACA,MAAAA,EAGA,cAAAV,EAAAC,KAAAI,KAAAK,GACA,MAAAA,GAAAF,IAAA,SAAA0C,GACA,MAAA9C,GAAAqE,UAAAvB,EAAAK,EAAA7C,KACGwG,KAAA,GAGH,IAAApE,IACAzC,KAAAK,EAAAL,KACAC,QAAAF,EAAAqE,UAAA/D,EAAAJ,QAAAiD,EAAAC,GACA2D,IAAA,OACAC,SAAA,QAAA1G,EAAAL,MACAgH,cACA9D,WACAC,SAOA,IAJA,WAAAV,EAAAzC,OACAyC,EAAAuE,WAAA,mBAGA3G,EAAAH,MAAA,CACA,GAAA+G,GAAA,UAAAtH,EAAAC,KAAAI,KAAAK,EAAAH,OAAAG,EAAAH,OAAAG,EAAAH,MACAmG,OAAA9F,UAAA4F,KAAAI,MAAA9D,EAAAsE,QAAAE,GAGAtH,EAAAgD,MAAAC,IAAA,OAAAH,EAEA,IAAAuE,GAAA1G,OAAA4G,KAAAzE,EAAAuE,YAAA7G,IAAA,SAAAuG,GACA,MAAAA,GAAA,MAAAjE,EAAAuE,WAAAN,IAAA,IAAAtG,QAAA,eAAyE,MACvEyG,KAAA,IAEF,WAAApE,EAAAqE,IAAA,WAAArE,EAAAsE,QAAAF,KAAA,UAAAG,EAAA,IAAAA,EAAA,QAAAvE,EAAAxC,QAAA,KAAAwC,EAAAqE,IAAA,MAIArI,EAAAsE,SACA,MAAAtE,GAAA0I,kBAKA1I,EAAA0I,iBAAA,mBAAApD,GACA,GAAAqD,GAAAjD,KAAAkD,MAAAtD,EAAAE,MACAxE,EAAA2H,EAAAlE,SACAO,EAAA2D,EAAA3D,KACAY,EAAA+C,EAAA/C,cAEA5F,GAAAyF,YAAAvE,EAAA2E,UAAAb,EAAA9D,EAAAwB,UAAA1B,OACA4E,GACA5F,EAAA6I,UAEE,GAEF7I,EAAAe,OAfAf,EAAAe,KAmBA,IAAA+H,GAAAxE,SAAAyE,kBAAAxB,MAAAvF,KAAAsC,SAAA0E,qBAAA,WAAAC,KAmBA,OAjBAH,KACA5H,EAAAkE,SAAA0D,EAAAI,IAEA5E,SAAAoE,mBAAAI,EAAAK,aAAA,iBACA,YAAA7E,SAAA8E,WACAxI,OAAAyI,sBACAzI,OAAAyI,sBAAAnI,EAAA4C,cAEAlD,OAAA0I,WAAApI,EAAA4C,aAAA,IAIAQ,SAAAoE,iBAAA,mBAAAxH,EAAA4C,gBAKA9D,EAAAe,aAIA,KAAAvB,KAAAC,UACAD,EAAAC,QAAAsB,OAIA,KAAAJ,IACAA,EAAAI,SAQAA,EAAA2B,UAAA6G,QACAC,QAAA,kBACAC,OAAA,iBACAC,QAAA,sBACAC,MAAA,0BACAtB,KACAhC,QAAA,wGACAtD,QACAsF,KACAhC,QAAA,kBACAtD,QACA6G,YAAA,QACAC,UAAA,iBAGAC,cACAzD,QAAA,kCACAtD,QACA6G,YAAA,WAGAA,YAAA,OACAG,aACA1D,QAAA,YACAtD,QACA8G,UAAA,mBAMAG,OAAA,qBAIAjJ,EAAAmD,MAAA8D,IAAA,gBAAAhE,GAEA,WAAAA,EAAAzC,OACAyC,EAAAuE,WAAA,MAAAvE,EAAAxC,QAAAG,QAAA,QAAsD,QAItDZ,EAAA2B,UAAAuH,IAAAlJ,EAAA2B,UAAA6G,OACAxI,EAAA2B,UAAAwH,KAAAnJ,EAAA2B,UAAA6G,OACAxI,EAAA2B,UAAAyH,OAAApJ,EAAA2B,UAAA6G,OACAxI,EAAA2B,UAAA0H,IAAArJ,EAAA2B,UAAA6G,OAOAxI,EAAA2B,UAAA2H,KACAb,QAAA,mBACAc,QACAjE,QAAA,4BACAtD,QACAwH,KAAA,YAIAC,IAAA,+DACAvG,SAAA,+BACAwG,QACApE,QAAA,8CACAE,QAAA,GAEAmE,SAAA,yBACAC,UAAA,kBACAC,SAAA,oBACAhB,YAAA,YAGA7I,EAAA2B,UAAA2H,IAAA,OAAAtH,OAAAkD,KAAAlF,EAAAI,KAAAmB,MAAAvB,EAAA2B,UAAA2H,KAEAtJ,EAAA2B,UAAA6G,SACAxI,EAAA2B,UAAAI,aAAA,gBACA+H,OACAxE,QAAA,0CACAC,YAAA,EACAvD,OAAAhC,EAAA2B,UAAA2H,IACA5I,MAAA,kBAIAV,EAAA2B,UAAAI,aAAA,uBACAgI,cACAzE,QAAA,uBACAtD,QACAgH,aACA1D,QAAA,aACAtD,OAAAhC,EAAA2B,UAAA6G,OAAAlB,IAAAtF,QAEA6G,YAAA,wBACAE,cACAzD,QAAA,MACAtD,OAAAhC,EAAA2B,UAAA2H,MAGA5I,MAAA,iBAEEV,EAAA2B,UAAA6G,OAAAlB,MAOFtH,EAAA2B,UAAAqI,OACAvB,UAEAnD,QAAA,4BACAC,YAAA,IAGAD,QAAA,mBACAC,YAAA,IAGAmE,QACApE,QAAA,+CACAE,QAAA,GAEAyE,cACA3E,QAAA,uGACAC,YAAA,EACAvD,QACA6G,YAAA,YAGAqB,QAAA,2GACAC,QAAA,mBACAN,SAAA,oBACAO,OAAA,gDACAC,SAAA,0DACAxB,YAAA,iBAQA7I,EAAA2B,UAAA2I,WAAAtK,EAAA2B,UAAAC,OAAA,SACAsI,QAAA,4TACAE,OAAA,+EAEAP,SAAA,wDACAQ,SAAA,qEAGArK,EAAA2B,UAAAI,aAAA,wBACAwI,OACAjF,QAAA,iFACAC,YAAA,EACAC,QAAA,KAIAxF,EAAA2B,UAAAI,aAAA,uBACAyI,mBACAlF,QAAA,wBACAE,QAAA,EACAxD,QACAyI,eACAnF,QAAA,cACAtD,QACA0I,6BACApF,QAAA,YACA5E,MAAA,eAEAwE,KAAAlF,EAAA2B,UAAA2I,aAGAZ,OAAA,cAKA1J,EAAA2B,UAAA6G,QACAxI,EAAA2B,UAAAI,aAAA,gBACAgG,QACAzC,QAAA,4CACAC,YAAA,EACAvD,OAAAhC,EAAA2B,UAAA2I,WACA5J,MAAA,yBAKAV,EAAA2B,UAAAgJ,GAAA3K,EAAA2B,UAAA2I,WAMA,WACA,mBAAAvK,YAAAC,OAAAD,KAAAwD,mBAAAqH,gBAIA7K,KAAAC,MAAA6K,cAAA,WAEA,GAAAC,IACAH,GAAA,aACAI,GAAA,SACAC,GAAA,OACAC,IAAA,aACAC,KAAA,aACAC,GAAA,OACAC,IAAA,QACAC,EAAA,IACAC,IAAA,QAGAzE,OAAA9F,UAAAwK,SACA1E,MAAA9F,UAAAyF,MAAAvF,KAAAsC,SAAAC,iBAAA,kBAAA+H,QAAA,SAAAC,GAKA,IAJA,GAEA9H,GAFAyE,EAAAqD,EAAAC,aAAA,YAEA9H,EAAA6H,EACAvL,EAAA,iCACA0D,IAAA1D,EAAA2D,KAAAD,EAAAE,YACAF,IAAAG,UAOA,IAJAH,IACAD,GAAA8H,EAAA3H,UAAA3C,MAAAjB,KAAA,WAGAyD,EAAA,CACA,GAAAgI,IAAAvD,EAAAjH,MAAA,sBACAwC,GAAAoH,EAAAY,MAGA,GAAAzH,GAAAV,SAAAoI,cAAA,OACA1H,GAAAJ,UAAA,YAAAH,EAEA8H,EAAAtH,YAAA,GAEAD,EAAAC,YAAA,WAEAsH,EAAAI,YAAA3H,EAEA,IAAA4H,GAAA,GAAAC,eAEAD,GAAAE,KAAA,MAAA5D,GAAA,GAEA0D,EAAAG,mBAAA,WACA,GAAAH,EAAAxD,aAEAwD,EAAAI,OAAA,KAAAJ,EAAAK,cACAjI,EAAAC,YAAA2H,EAAAK,aAEAlM,EAAAyD,iBAAAQ,IAEA4H,EAAAI,QAAA,IACAhI,EAAAC,YAAA,WAAA2H,EAAAI,OAAA,yBAAAJ,EAAAM,WAGAlI,EAAAC,YAAA,6CAKA2H,EAAAO,KAAA,SAMA7I,SAAAoE,iBAAA,mBAAA5H,KAAAC,MAAA6K,qBH+C6B5J,KAAKvC,EAASe,EAAoB,KAIzD4M,GACA,SAAU5N,EAAQC,EAASe,GI50BjCf,EAAAD,EAAAC,QAAAe,EAAA,QAKAf,EAAAiI,MAAAlI,EAAAoE,EAAA,49FAAq/F,IAAQyJ,QAAA,EAAAC,SAAA,8DAAAC,SAAAC,SAAA,u3CAAAC,KAAA,gBAAAC,gBAAA,mwIAAqyLC,WAAA,OJq1B5xRC,GACA,SAAUpO,EAAQC,EAASe,GKx1BjC,GAAAgB,GAAAhB,EAAA,GACA,iBAAAgB,SAAAhC,EAAAoE,EAAApC,EAAA,MACAA,EAAAqM,SAAArO,EAAAC,QAAA+B,EAAAqM,OAEArN,GAAA,eAAAgB,GAAA,ILi2BMsM,GACA,SAAUtO,EAAQC,EAASe,IMz2BjC,SAAAG,IAMC,WA+FD,QAAAoN,GAAAC,GACApO,KAAAyB,UACAzB,KAAAyB,OAAA4M,SACArO,KAAAoO,WAAAE,EAAAC,SACAvO,KAAAwO,MAAAC,EAAAC,OAEA1O,KAAAoO,QAAAO,MACA3O,KAAAoO,QAAAQ,OACA5O,KAAAwO,MAAAC,EAAAG,OAEA5O,KAAAwO,MAAAC,EAAAE,KAwZA,QAAAE,GAAAR,EAAAD,GAOA,GANApO,KAAAoO,WAAAE,EAAAC,SACAvO,KAAAqO,QACArO,KAAAwO,MAAAM,EAAAJ,OACA1O,KAAA+O,SAAA/O,KAAAoO,QAAAW,UAAA,GAAAC,GACAhP,KAAA+O,SAAAX,QAAApO,KAAAoO,SAEApO,KAAAqO,MACA,SACAY,OAAA,4CAGAjP,MAAAoO,QAAAO,IACA3O,KAAAoO,QAAAc,OACAlP,KAAAwO,MAAAM,EAAAI,OAEAlP,KAAAwO,MAAAM,EAAAH,IAEG3O,KAAAoO,QAAAe,WACHnP,KAAAwO,MAAAM,EAAAK,UA6NA,QAAAH,GAAAZ,GACApO,KAAAoO,cAkJA,QAAAgB,GAAAhB,GACApO,KAAAyB,UACAzB,KAAA4D,MAAA,KACA5D,KAAAoO,WAAAE,EAAAC,SACAvO,KAAAoO,QAAAW,SAAA/O,KAAAoO,QAAAW,UAAA,GAAAC,GACAhP,KAAA+O,SAAA/O,KAAAoO,QAAAW,SACA/O,KAAA+O,SAAAX,QAAApO,KAAAoO,QA8KA,QAAAiB,GAAA/E,EAAA9I,GACA,MAAA8I,GACAvI,QAAAP,EAAkC,KAAlC,eAAkC,SAClCO,QAAA,aACAA,QAAA,aACAA,QAAA,eACAA,QAAA,cAGA,QAAAuN,GAAAhF,GAEA,MAAAA,GAAAvI,QAAA,4CAA8D,SAAAT,EAAAiO,GAE9D,MADAA,KAAArK,cACA,UAAAqK,EAAA,IACA,MAAAA,EAAAC,OAAA,GACA,MAAAD,EAAAC,OAAA,GACAC,OAAAC,aAAAC,SAAAJ,EAAAK,UAAA,QACAH,OAAAC,cAAAH,EAAAK,UAAA,IAEA,KAIA,QAAA7N,GAAA2J,EAAAmE,GAGA,MAFAnE,KAAA3E,OACA8I,KAAA,GACA,QAAA3O,GAAAmH,EAAAyH,GACA,MAAAzH,IACAyH,IAAA/I,QAAA+I,EACAA,IAAA/N,QAAA,qBACA2J,IAAA3J,QAAAsG,EAAAyH,GACA5O,GAJA,GAAA4F,QAAA4E,EAAAmE,IAQA,QAAAE,MAGA,QAAAC,GAAAzN,GAKA,IAJA,GACA0N,GACAtN,EAFAqB,EAAA,EAIQA,EAAAR,UAAAC,OAAsBO,IAAA,CAC9BiM,EAAAzM,UAAAQ,EACA,KAAArB,IAAAsN,GACAhO,OAAAC,UAAAU,eAAAR,KAAA6N,EAAAtN,KACAJ,EAAAI,GAAAsN,EAAAtN,IAKA,MAAAJ,GAQA,QAAA+L,GAAAhF,EAAAuG,EAAA/L,GACA,GAAAA,GAAA,kBAAA+L,GAAA,CACA/L,IACAA,EAAA+L,EACAA,EAAA,MAGAA,EAAAG,KAAkB1B,EAAAC,SAAAsB,MAElB,IACApO,GACAyO,EAFAjK,EAAA4J,EAAA5J,UAGAjC,EAAA,CAEA,KACAvC,EAAA0M,EAAAgC,IAAA7G,EAAAuG,GACK,MAAAO,GACL,MAAAtM,GAAAsM,GAGAF,EAAAzO,EAAAgC,MAEA,IAAA4M,GAAA,SAAAC,GACA,GAAAA,EAEA,MADAT,GAAA5J,YACAnC,EAAAwM,EAGA,IAAAC,EAEA,KACAA,EAAAnB,EAAApG,MAAAvH,EAAAoO,GACO,MAAAO,GACPE,EAAAF,EAKA,MAFAP,GAAA5J,YAEAqK,EACAxM,EAAAwM,GACAxM,EAAA,KAAAyM,GAGA,KAAAtK,KAAAxC,OAAA,EACA,MAAA4M,IAKA,UAFAR,GAAA5J,WAEAiK,EAAA,MAAAG,IAEA,MAAUrM,EAAAvC,EAAAgC,OAAmBO,KAC7B,SAAAJ,GACA,SAAAA,EAAAjC,OACAuO,GAAAG,IAEApK,EAAArC,EAAAsC,KAAAtC,EAAAxC,KAAA,SAAAkP,EAAAlL,GACA,MAAAkL,GAAAD,EAAAC,GACA,MAAAlL,OAAAxB,EAAAsC,OACAgK,GAAAG,KAEAzM,EAAAsC,KAAAd,EACAxB,EAAA4M,SAAA,SACAN,GAAAG,SAEO5O,EAAAuC,QAKP,KAEA,MADA6L,OAAAG,KAA2B1B,EAAAC,SAAAsB,IAC3BT,EAAApG,MAAAmF,EAAAgC,IAAA7G,EAAAuG,MACG,MAAAO,GAEH,GADAA,EAAArH,SAAA,2DACA8G,GAAAvB,EAAAC,UAAAkC,OACA,sCACApB,EAAAe,EAAArH,QAAA,OACA,QAEA,MAAAqH,IA9rCA,GAAA3B,IACAiC,QAAA,OACAtL,KAAA,oBACAuL,OAAAZ,EACAa,GAAA,4BACAC,QAAA,wCACAC,QAAAf,EACAgB,SAAA,oCACAC,WAAA,qCACAC,KAAA,gEACA3G,KAAA,+EACA4G,IAAA,oEACAC,MAAApB,EACAqB,UAAA,iEACAlL,KAAA,UAGAuI,GAAA4C,OAAA,kBACA5C,EAAA6C,KAAA,6CACA7C,EAAA6C,KAAAvP,EAAA0M,EAAA6C,KAAA,MACA,QAAA7C,EAAA4C,UAGA5C,EAAAwC,KAAAlP,EAAA0M,EAAAwC,MACA,QAAAxC,EAAA4C,QACA,8CACA,gBAAA5C,EAAAyC,IAAAnK,OAAA,OAGA0H,EAAAuC,WAAAjP,EAAA0M,EAAAuC,YACA,MAAAvC,EAAAyC,OAGAzC,EAAA8C,KAAA,qKAKA9C,EAAAnE,KAAAvI,EAAA0M,EAAAnE,MACA,6BACA,iCACA,+CACA,OAAAmE,EAAA8C,QAGA9C,EAAA2C,UAAArP,EAAA0M,EAAA2C,WACA,KAAA3C,EAAAmC,IACA,UAAAnC,EAAAoC,SACA,WAAApC,EAAAsC,UACA,aAAAtC,EAAAuC,YACA,UAAAvC,EAAA8C,MACA,MAAA9C,EAAAyC,OAOAzC,EAAAC,OAAAsB,KAAuBvB,GAMvBA,EAAAE,IAAAqB,KAAoBvB,EAAAC,QACpBiC,OAAA,6DACAS,UAAA,IACAP,QAAA,0CAGApC,EAAAE,IAAAyC,UAAArP,EAAA0M,EAAA2C,WACA,YACA3C,EAAAE,IAAAgC,OAAA5J,OAAAhF,QAAA,iBACA0M,EAAAwC,KAAAlK,OAAAhF,QAAA,oBAOA0M,EAAAG,OAAAoB,KAAuBvB,EAAAE,KACvBmC,QAAA,gEACAK,MAAA,8DA0BAhD,EAAAK,MAAAC,EAMAN,EAAAgC,IAAA,SAAA7G,EAAA8E,GAEA,MADA,IAAAD,GAAAC,GACA+B,IAAA7G,IAOA6E,EAAAjM,UAAAiO,IAAA,SAAA7G,GAOA,MANAA,KACAvH,QAAA,iBACAA,QAAA,cACAA,QAAA,eACAA,QAAA,gBAEA/B,KAAA4D,MAAA0F,GAAA,IAOA6E,EAAAjM,UAAA0B,MAAA,SAAA0F,EAAAkI,EAAAC,GAYA,IAXA,GACAC,GACAC,EACAC,EACAC,EACAC,EACAR,EACAS,EACA/N,EACAgO,EATA1I,IAAAvH,QAAA,aAWAuH,GAYA,IAVAsI,EAAA5R,KAAAwO,MAAAkC,QAAAvJ,KAAAmC,MACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAmO,EAAA,GAAAnO,OAAA,GACAzD,KAAAyB,OAAAqG,MACAnG,KAAA,WAMAiQ,EAAA5R,KAAAwO,MAAApJ,KAAA+B,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAmO,IAAA,GAAA7P,QAAA,UAAiC,IACjC/B,KAAAyB,OAAAqG,MACAnG,KAAA,OACAuE,KAAAlG,KAAAoO,QAAAe,SAEAyC,EADAA,EAAA7P,QAAA,iBAOA,IAAA6P,EAAA5R,KAAAwO,MAAAmC,OAAAxJ,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAzD,KAAAyB,OAAAqG,MACAnG,KAAA,OACAP,KAAAwQ,EAAA,GACA1L,KAAA0L,EAAA,aAMA,IAAAA,EAAA5R,KAAAwO,MAAAqC,QAAA1J,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAzD,KAAAyB,OAAAqG,MACAnG,KAAA,UACAsQ,MAAAL,EAAA,GAAAnO,OACAyC,KAAA0L,EAAA,SAMA,IAAAJ,IAAAI,EAAA5R,KAAAwO,MAAAsC,QAAA3J,KAAAmC,IAAA,CAUA,IATAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QAEA6N,GACA3P,KAAA,QACAuQ,OAAAN,EAAA,GAAA7P,QAAA,mBAAAoQ,MAAA,UACAC,MAAAR,EAAA,GAAA7P,QAAA,iBAAAoQ,MAAA,UACAE,MAAAT,EAAA,GAAA7P,QAAA,UAAAoQ,MAAA,OAGAnO,EAAA,EAAiBA,EAAAsN,EAAAc,MAAA3O,OAAuBO,IACxC,YAAAe,KAAAuM,EAAAc,MAAApO,IACAsN,EAAAc,MAAApO,GAAA,QACS,aAAAe,KAAAuM,EAAAc,MAAApO,IACTsN,EAAAc,MAAApO,GAAA,SACS,YAAAe,KAAAuM,EAAAc,MAAApO,IACTsN,EAAAc,MAAApO,GAAA,OAEAsN,EAAAc,MAAApO,GAAA,IAIA,KAAAA,EAAA,EAAiBA,EAAAsN,EAAAe,MAAA5O,OAAuBO,IACxCsN,EAAAe,MAAArO,GAAAsN,EAAAe,MAAArO,GAAAmO,MAAA,SAGAnS,MAAAyB,OAAAqG,KAAAwJ,OAMA,IAAAM,EAAA5R,KAAAwO,MAAAuC,SAAA5J,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAzD,KAAAyB,OAAAqG,MACAnG,KAAA,UACAsQ,MAAA,MAAAL,EAAA,OACA1L,KAAA0L,EAAA,SAMA,IAAAA,EAAA5R,KAAAwO,MAAAoC,GAAAzJ,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAzD,KAAAyB,OAAAqG,MACAnG,KAAA,WAMA,IAAAiQ,EAAA5R,KAAAwO,MAAAwC,WAAA7J,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QAEAzD,KAAAyB,OAAAqG,MACAnG,KAAA,qBAGAiQ,IAAA,GAAA7P,QAAA,eAKA/B,KAAA4D,MAAAgO,EAAAJ,GAAA,GAEAxR,KAAAyB,OAAAqG,MACAnG,KAAA,uBAOA,IAAAiQ,EAAA5R,KAAAwO,MAAAyC,KAAA9J,KAAAmC,GAAA,CAgBA,IAfAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAoO,EAAAD,EAAA,GAEA5R,KAAAyB,OAAAqG,MACAnG,KAAA,aACA2Q,QAAAT,EAAApO,OAAA,IAIAmO,IAAA,GAAAvP,MAAArC,KAAAwO,MAAA8C,MAEAI,GAAA,EACAM,EAAAJ,EAAAnO,OACAO,EAAA,EAEYA,EAAAgO,EAAOhO,IACnBsN,EAAAM,EAAA5N,GAIA+N,EAAAT,EAAA7N,OACA6N,IAAAvP,QAAA,0BAIAuP,EAAAiB,QAAA,SACAR,GAAAT,EAAA7N,OACA6N,EAAAtR,KAAAoO,QAAAe,SAEAmC,EAAAvP,QAAA,YAAmC,IADnCuP,EAAAvP,QAAA,GAAA+E,QAAA,QAA0CiL,EAAA,IAAgB,WAM1D/R,KAAAoO,QAAAoE,YAAAxO,IAAAgO,EAAA,IACAF,EAAArD,EAAA4C,OAAAlK,KAAAyK,EAAA5N,EAAA,OACA6N,IAAAC,GAAAD,EAAApO,OAAA,GAAAqO,EAAArO,OAAA,IACA6F,EAAAsI,EAAAjK,MAAA3D,EAAA,GAAAwE,KAAA,MAAAc,EACAtF,EAAAgO,EAAA,IAOAL,EAAAD,GAAA,eAAA3M,KAAAuM,GACAtN,IAAAgO,EAAA,IACAN,EAAA,OAAAJ,EAAA9B,OAAA8B,EAAA7N,OAAA,GACAkO,MAAAD,IAGA1R,KAAAyB,OAAAqG,MACAnG,KAAAgQ,EACA,mBACA,oBAIA3R,KAAA4D,MAAA0N,GAAA,EAAAG,GAEAzR,KAAAyB,OAAAqG,MACAnG,KAAA,iBAIA3B,MAAAyB,OAAAqG,MACAnG,KAAA,iBAOA,IAAAiQ,EAAA5R,KAAAwO,MAAAlE,KAAAnD,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAzD,KAAAyB,OAAAqG,MACAnG,KAAA3B,KAAAoO,QAAAqE,SACA,YACA,OACA9F,KAAA3M,KAAAoO,QAAAsE,YACA,QAAAd,EAAA,eAAAA,EAAA,cAAAA,EAAA,IACA1L,KAAA0L,EAAA,SAMA,KAAAH,GAAAD,IAAAI,EAAA5R,KAAAwO,MAAA0C,IAAA/J,KAAAmC,IACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAzD,KAAAyB,OAAA4M,MAAAuD,EAAA,GAAA1M,gBACAyN,KAAAf,EAAA,GACAgB,MAAAhB,EAAA,QAMA,IAAAJ,IAAAI,EAAA5R,KAAAwO,MAAA2C,MAAAhK,KAAAmC,IAAA,CAUA,IATAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QAEA6N,GACA3P,KAAA,QACAuQ,OAAAN,EAAA,GAAA7P,QAAA,mBAAAoQ,MAAA,UACAC,MAAAR,EAAA,GAAA7P,QAAA,iBAAAoQ,MAAA,UACAE,MAAAT,EAAA,GAAA7P,QAAA,qBAAAoQ,MAAA,OAGAnO,EAAA,EAAiBA,EAAAsN,EAAAc,MAAA3O,OAAuBO,IACxC,YAAAe,KAAAuM,EAAAc,MAAApO,IACAsN,EAAAc,MAAApO,GAAA,QACS,aAAAe,KAAAuM,EAAAc,MAAApO,IACTsN,EAAAc,MAAApO,GAAA,SACS,YAAAe,KAAAuM,EAAAc,MAAApO,IACTsN,EAAAc,MAAApO,GAAA,OAEAsN,EAAAc,MAAApO,GAAA,IAIA,KAAAA,EAAA,EAAiBA,EAAAsN,EAAAe,MAAA5O,OAAuBO,IACxCsN,EAAAe,MAAArO,GAAAsN,EAAAe,MAAArO,GACAjC,QAAA,uBACAoQ,MAAA,SAGAnS,MAAAyB,OAAAqG,KAAAwJ,OAMA,IAAAE,IAAAI,EAAA5R,KAAAwO,MAAA4C,UAAAjK,KAAAmC,IACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAzD,KAAAyB,OAAAqG,MACAnG,KAAA,YACAuE,KAAA,OAAA0L,EAAA,GAAApC,OAAAoC,EAAA,GAAAnO,OAAA,GACAmO,EAAA,GAAAjK,MAAA,MACAiK,EAAA,SAMA,IAAAA,EAAA5R,KAAAwO,MAAAtI,KAAAiB,KAAAmC,GAEAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAzD,KAAAyB,OAAAqG,MACAnG,KAAA,OACAuE,KAAA0L,EAAA,SAKA,IAAAtI,EACA,SACA2F,OAAA,0BAAA3F,EAAAuJ,WAAA,GAIA,OAAA7S,MAAAyB,OAOA,IAAAqN,IACAO,OAAA,8BACAyD,SAAA,2BACAlI,IAAAmF,EACAtH,IAAA,yDACAsK,KAAA,0BACAC,QAAA,iCACAC,OAAA,mCACAC,OAAA,iDACAC,GAAA,wDACA/N,KAAA,mCACAgO,GAAA,mBACAC,IAAAtD,EACA7J,KAAA,qCAGA4I,GAAAwE,QAAA,yCACAxE,EAAAyE,MAAA,iDAEAzE,EAAAiE,KAAAhR,EAAA+M,EAAAiE,MACA,SAAAjE,EAAAwE,SACA,OAAAxE,EAAAyE,SAGAzE,EAAAkE,QAAAjR,EAAA+M,EAAAkE,SACA,SAAAlE,EAAAwE,WAOAxE,EAAAJ,OAAAsB,KAAwBlB,GAMxBA,EAAAK,SAAAa,KAA0BlB,EAAAJ,QAC1BwE,OAAA,iEACAC,GAAA,6DAOArE,EAAAH,IAAAqB,KAAqBlB,EAAAJ,QACrBW,OAAAtN,EAAA+M,EAAAO,QAAA,eACAzE,IAAA,uCACAyI,IAAA,0BACAnN,KAAAnE,EAAA+M,EAAA5I,MACA,YACA,uBAQA4I,EAAAI,OAAAc,KAAwBlB,EAAAH,KACxByE,GAAArR,EAAA+M,EAAAsE,IAAA,OAA8B,OAC9BlN,KAAAnE,EAAA+M,EAAAH,IAAAzI,MAAA,OAAsC,SAkCtC2I,EAAAL,MAAAM,EAMAD,EAAA2E,OAAA,SAAAlK,EAAA+E,EAAAD,GAEA,MADA,IAAAS,GAAAR,EAAAD,GACAoF,OAAAlK,IAOAuF,EAAA3M,UAAAsR,OAAA,SAAAlK,GAOA,IANA,GACAyJ,GACA7M,EACAyM,EACAf,EAJArB,EAAA,GAMAjH,GAEA,GAAAsI,EAAA5R,KAAAwO,MAAAa,OAAAlI,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACA8M,GAAAqB,EAAA,OAKA,IAAAA,EAAA5R,KAAAwO,MAAAsE,SAAA3L,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACA,MAAAmO,EAAA,IACA1L,EAAA,MAAA0L,EAAA,GAAApC,OAAA,GACAxP,KAAAyT,OAAA7B,EAAA,GAAAhC,UAAA,IACA5P,KAAAyT,OAAA7B,EAAA,IACAe,EAAA3S,KAAAyT,OAAA,WAAAvN,IAEAA,EAAAmJ,EAAAuC,EAAA,IACAe,EAAAzM,GAEAqK,GAAAvQ,KAAA+O,SAAAgE,KAAAJ,EAAA,KAAAzM,OAKA,IAAAlG,KAAA0T,UAAA9B,EAAA5R,KAAAwO,MAAA5D,IAAAzD,KAAAmC,KASA,GAAAsI,EAAA5R,KAAAwO,MAAA/F,IAAAtB,KAAAmC,IACAtJ,KAAA0T,QAAA,QAAA3O,KAAA6M,EAAA,IACA5R,KAAA0T,QAAA,EACO1T,KAAA0T,QAAA,UAAA3O,KAAA6M,EAAA,MACP5R,KAAA0T,QAAA,GAEApK,IAAAsG,UAAAgC,EAAA,GAAAnO,QACA8M,GAAAvQ,KAAAoO,QAAAqE,SACAzS,KAAAoO,QAAAsE,UACA1S,KAAAoO,QAAAsE,UAAAd,EAAA,IACAvC,EAAAuC,EAAA,IACAA,EAAA,OAKA,IAAAA,EAAA5R,KAAAwO,MAAAuE,KAAA5L,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAzD,KAAA0T,QAAA,EACAnD,GAAAvQ,KAAA2T,WAAA/B,GACAe,KAAAf,EAAA,GACAgB,MAAAhB,EAAA,KAEA5R,KAAA0T,QAAA,MAKA,KAAA9B,EAAA5R,KAAAwO,MAAAwE,QAAA7L,KAAAmC,MACAsI,EAAA5R,KAAAwO,MAAAyE,OAAA9L,KAAAmC,IADA,CAKA,GAHAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACAsP,GAAAnB,EAAA,IAAAA,EAAA,IAAA7P,QAAA,cACAgR,EAAA/S,KAAAqO,MAAA0E,EAAA7N,kBACA6N,EAAAJ,KAAA,CACApC,GAAAqB,EAAA,GAAApC,OAAA,GACAlG,EAAAsI,EAAA,GAAAhC,UAAA,GAAAtG,CACA,UAEAtJ,KAAA0T,QAAA,EACAnD,GAAAvQ,KAAA2T,WAAA/B,EAAAmB,GACA/S,KAAA0T,QAAA,MAKA,IAAA9B,EAAA5R,KAAAwO,MAAA0E,OAAA/L,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACA8M,GAAAvQ,KAAA+O,SAAAmE,OAAAlT,KAAAwT,OAAA5B,EAAA,IAAAA,EAAA,SAKA,IAAAA,EAAA5R,KAAAwO,MAAA2E,GAAAhM,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACA8M,GAAAvQ,KAAA+O,SAAAoE,GAAAnT,KAAAwT,OAAA5B,EAAA,IAAAA,EAAA,SAKA,IAAAA,EAAA5R,KAAAwO,MAAApJ,KAAA+B,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACA8M,GAAAvQ,KAAA+O,SAAA6E,SAAAvE,EAAAuC,EAAA,YAKA,IAAAA,EAAA5R,KAAAwO,MAAA4E,GAAAjM,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACA8M,GAAAvQ,KAAA+O,SAAAqE,SAKA,IAAAxB,EAAA5R,KAAAwO,MAAA6E,IAAAlM,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACA8M,GAAAvQ,KAAA+O,SAAAsE,IAAArT,KAAAwT,OAAA5B,EAAA,SAKA,IAAAA,EAAA5R,KAAAwO,MAAAtI,KAAAiB,KAAAmC,GACAA,IAAAsG,UAAAgC,EAAA,GAAAnO,QACA8M,GAAAvQ,KAAA+O,SAAA7I,KAAAmJ,EAAArP,KAAA6T,YAAAjC,EAAA,UAIA,IAAAtI,EACA,SACA2F,OAAA,0BAAA3F,EAAAuJ,WAAA,QAhGAvJ,KAAAsG,UAAAgC,EAAA,GAAAnO,QACAyC,EAAAmJ,EAAAuC,EAAA,IACAe,EAAAzM,EACAqK,GAAAvQ,KAAA+O,SAAAgE,KAAAJ,EAAA,KAAAzM,EAiGA,OAAAqK,IAOA1B,EAAA3M,UAAAyR,WAAA,SAAA/B,EAAAmB,GACA,GAAAJ,GAAAtD,EAAA0D,EAAAJ,MACAC,EAAAG,EAAAH,MAAAvD,EAAA0D,EAAAH,OAAA,IAEA,aAAAhB,EAAA,GAAApC,OAAA,GACAxP,KAAA+O,SAAAgE,KAAAJ,EAAAC,EAAA5S,KAAAwT,OAAA5B,EAAA,KACA5R,KAAA+O,SAAA+E,MAAAnB,EAAAC,EAAAvD,EAAAuC,EAAA,MAOA/C,EAAA3M,UAAA2R,YAAA,SAAA3N,GACA,MAAAlG,MAAAoO,QAAAyF,YACA3N,EAEAnE,QAAA,YAEAA,QAAA,WAEAA,QAAA,2BAA8B,OAE9BA,QAAA,UAEAA,QAAA,gCAA8B,OAE9BA,QAAA,UAEAA,QAAA,SAAmB,KAfnBmE,GAsBA2I,EAAA3M,UAAAuR,OAAA,SAAAvN,GACA,IAAAlG,KAAAoO,QAAAqF,OAAA,MAAAvN,EAMA,KALA,GAGA6N,GAHAxD,EAAA,GACAyB,EAAA9L,EAAAzC,OACAO,EAAA,EAGQA,EAAAgO,EAAOhO,IACf+P,EAAA7N,EAAA2M,WAAA7O,GACAgQ,KAAAC,SAAA,KACAF,EAAA,IAAAA,EAAA5R,SAAA,KAEAoO,GAAA,KAAAwD,EAAA,GAGA,OAAAxD,IAWAvB,EAAA9M,UAAAkD,KAAA,SAAAA,EAAAhE,EAAAoP,GACA,GAAAxQ,KAAAoO,QAAAnI,UAAA,CACA,GAAAsK,GAAAvQ,KAAAoO,QAAAnI,UAAAb,EAAAhE,EACA,OAAAmP,OAAAnL,IACAoL,GAAA,EACApL,EAAAmL,GAIA,MAAAnP,GAMA,qBACApB,KAAAoO,QAAA8F,WACA7E,EAAAjO,GAAA,GACA,MACAoP,EAAApL,EAAAiK,EAAAjK,GAAA,IACA,oBAVA,eACAoL,EAAApL,EAAAiK,EAAAjK,GAAA,IACA,mBAWA4J,EAAA9M,UAAA8O,WAAA,SAAAmD,GACA,uBAAAA,EAAA,mBAGAnF,EAAA9M,UAAAoI,KAAA,SAAAA,GACA,MAAAA,IAGA0E,EAAA9M,UAAA2O,QAAA,SAAA3K,EAAAkO,EAAAC,GACA,WACAD,EACA,QACApU,KAAAoO,QAAAkG,aACAD,EAAAnP,cAAAnD,QAAA,eACA,KACAmE,EACA,MACAkO,EACA,OAGApF,EAAA9M,UAAA0O,GAAA,WACA,MAAA5Q,MAAAoO,QAAAmG,MAAA,oBAGAvF,EAAA9M,UAAA+O,KAAA,SAAAuD,EAAAlC,GACA,GAAA3Q,GAAA2Q,EAAA,SACA,WAAA3Q,EAAA,MAAA6S,EAAA,KAAA7S,EAAA,OAGAqN,EAAA9M,UAAAuS,SAAA,SAAAvO,GACA,aAAAA,EAAA,WAGA8I,EAAA9M,UAAAkP,UAAA,SAAAlL,GACA,YAAAA,EAAA,UAGA8I,EAAA9M,UAAAiP,MAAA,SAAAe,EAAAsC,GACA,2BAEAtC,EACA,sBAEAsC,EACA,wBAIAxF,EAAA9M,UAAAwS,SAAA,SAAA9S,GACA,eAAAA,EAAA,WAGAoN,EAAA9M,UAAAyS,UAAA,SAAA/S,EAAAiF,GACA,GAAAlF,GAAAkF,EAAAqL,OAAA,SAIA,QAHArL,EAAAuL,MACA,IAAAzQ,EAAA,sBAAAkF,EAAAuL,MAAA,KACA,IAAAzQ,EAAA,KACAC,EAAA,KAAAD,EAAA,OAIAqN,EAAA9M,UAAAgR,OAAA,SAAAhN,GACA,iBAAAA,EAAA,aAGA8I,EAAA9M,UAAAiR,GAAA,SAAAjN,GACA,aAAAA,EAAA,SAGA8I,EAAA9M,UAAA0R,SAAA,SAAA1N,GACA,eAAAA,EAAA,WAGA8I,EAAA9M,UAAAkR,GAAA,WACA,MAAApT,MAAAoO,QAAAmG,MAAA,gBAGAvF,EAAA9M,UAAAmR,IAAA,SAAAnN,GACA,cAAAA,EAAA,UAGA8I,EAAA9M,UAAA6Q,KAAA,SAAAJ,EAAAC,EAAA1M,GACA,GAAAlG,KAAAoO,QAAAqE,SAAA,CACA,IACA,GAAAmC,GAAAC,mBAAAvF,EAAAqD,IACA5Q,QAAA,cACAmD,cACK,MAAAkL,GACL,SAEA,OAAAwE,EAAArC,QAAA,oBAAAqC,EAAArC,QAAA,aACA,SAGA,GAAAhC,GAAA,YAAAoC,EAAA,GAKA,OAJAC,KACArC,GAAA,WAAAqC,EAAA,KAEArC,GAAA,IAAArK,EAAA,QAIA8I,EAAA9M,UAAA4R,MAAA,SAAAnB,EAAAC,EAAA1M,GACA,GAAAqK,GAAA,aAAAoC,EAAA,UAAAzM,EAAA,GAKA,OAJA0M,KACArC,GAAA,WAAAqC,EAAA,KAEArC,GAAAvQ,KAAAoO,QAAAmG,MAAA,UAIAvF,EAAA9M,UAAAgE,KAAA,SAAAA,GACA,MAAAA,IAoBAkJ,EAAApG,MAAA,SAAAM,EAAA8E,EAAAW,GAEA,MADA,IAAAK,GAAAhB,EAAAW,GACA/F,MAAAM,IAOA8F,EAAAlN,UAAA8G,MAAA,SAAAM,GACAtJ,KAAA8O,OAAA,GAAAD,GAAAvF,EAAA+E,MAAArO,KAAAoO,QAAApO,KAAA+O,UACA/O,KAAAyB,OAAA6H,EAAAwL,SAGA,KADA,GAAAvE,GAAA,GACAvQ,KAAA0R,QACAnB,GAAAvQ,KAAA+U,KAGA,OAAAxE,IAOAnB,EAAAlN,UAAAwP,KAAA,WACA,MAAA1R,MAAA4D,MAAA5D,KAAAyB,OAAA4H,OAOA+F,EAAAlN,UAAA8S,KAAA,WACA,MAAAhV,MAAAyB,OAAAzB,KAAAyB,OAAAgC,OAAA,OAOA2L,EAAAlN,UAAA+S,UAAA,WAGA,IAFA,GAAAT,GAAAxU,KAAA4D,MAAAsC,KAEA,SAAAlG,KAAAgV,OAAArT,MACA6S,GAAA,KAAAxU,KAAA0R,OAAAxL,IAGA,OAAAlG,MAAA8O,OAAA0E,OAAAgB,IAOApF,EAAAlN,UAAA6S,IAAA,WACA,OAAA/U,KAAA4D,MAAAjC,MACA,YACA,QAEA,UACA,MAAA3B,MAAA+O,SAAA6B,IAEA,eACA,MAAA5Q,MAAA+O,SAAA8B,QACA7Q,KAAA8O,OAAA0E,OAAAxT,KAAA4D,MAAAsC,MACAlG,KAAA4D,MAAAqO,MACAjS,KAAA4D,MAAAsC,KAEA,YACA,MAAAlG,MAAA+O,SAAA3J,KAAApF,KAAA4D,MAAAsC,KACAlG,KAAA4D,MAAAxC,KACApB,KAAA4D,MAAA4M,QAEA,aACA,GAEAxM,GACAkR,EACAC,EAEA3O,EANA0L,EAAA,GACAsC,EAAA,EASA,KADAW,EAAA,GACAnR,EAAA,EAAiBA,EAAAhE,KAAA4D,MAAAsO,OAAAzO,OAA8BO,MAC9BkO,QAAA,EAAAE,MAAApS,KAAA4D,MAAAwO,MAAApO,KACjBmR,GAAAnV,KAAA+O,SAAA4F,UACA3U,KAAA8O,OAAA0E,OAAAxT,KAAA4D,MAAAsO,OAAAlO,KACWkO,QAAA,EAAAE,MAAApS,KAAA4D,MAAAwO,MAAApO,IAKX,KAFAkO,GAAAlS,KAAA+O,SAAA2F,SAAAS,GAEAnR,EAAA,EAAiBA,EAAAhE,KAAA4D,MAAAyO,MAAA5O,OAA6BO,IAAA,CAI9C,IAHAkR,EAAAlV,KAAA4D,MAAAyO,MAAArO,GAEAmR,EAAA,GACA3O,EAAA,EAAmBA,EAAA0O,EAAAzR,OAAgB+C,IACnC2O,GAAAnV,KAAA+O,SAAA4F,UACA3U,KAAA8O,OAAA0E,OAAA0B,EAAA1O,KACa0L,QAAA,EAAAE,MAAApS,KAAA4D,MAAAwO,MAAA5L,IAIbgO,IAAAxU,KAAA+O,SAAA2F,SAAAS,GAEA,MAAAnV,MAAA+O,SAAAoC,MAAAe,EAAAsC,EAEA,wBAGA,IAFA,GAAAA,GAAA,GAEA,mBAAAxU,KAAA0R,OAAA/P,MACA6S,GAAAxU,KAAA+U,KAGA,OAAA/U,MAAA+O,SAAAiC,WAAAwD,EAEA,kBAIA,IAHA,GAAAA,GAAA,GACAlC,EAAAtS,KAAA4D,MAAA0O,QAEA,aAAAtS,KAAA0R,OAAA/P,MACA6S,GAAAxU,KAAA+U,KAGA,OAAA/U,MAAA+O,SAAAkC,KAAAuD,EAAAlC,EAEA,uBAGA,IAFA,GAAAkC,GAAA,GAEA,kBAAAxU,KAAA0R,OAAA/P,MACA6S,GAAA,SAAAxU,KAAA4D,MAAAjC,KACA3B,KAAAiV,YACAjV,KAAA+U,KAGA,OAAA/U,MAAA+O,SAAA0F,SAAAD,EAEA,wBAGA,IAFA,GAAAA,GAAA,GAEA,kBAAAxU,KAAA0R,OAAA/P,MACA6S,GAAAxU,KAAA+U,KAGA,OAAA/U,MAAA+O,SAAA0F,SAAAD,EAEA,YACA,GAAAlK,GAAAtK,KAAA4D,MAAA+I,KAAA3M,KAAAoO,QAAAe,SAEAnP,KAAA4D,MAAAsC,KADAlG,KAAA8O,OAAA0E,OAAAxT,KAAA4D,MAAAsC,KAEA,OAAAlG,MAAA+O,SAAAzE,OAEA,iBACA,MAAAtK,MAAA+O,SAAAqC,UAAApR,KAAA8O,OAAA0E,OAAAxT,KAAA4D,MAAAsC,MAEA,YACA,MAAAlG,MAAA+O,SAAAqC,UAAApR,KAAAiV,eA6CAlF,EAAA5I,KAAA4I,EAgHAzB,EAAAF,QACAE,EAAA8G,WAAA,SAAAvF,GAEA,MADAG,GAAA1B,EAAAC,SAAAsB,GACAvB,GAGAA,EAAAC,UACAI,KAAA,EACAC,QAAA,EACAM,QAAA,EACAC,UAAA,EACAsD,UAAA,EACAC,UAAA,KACAe,QAAA,EACAjB,YAAA,EACA/B,QAAA,EACAxK,UAAA,KACAiO,WAAA,QACAL,aAAA,EACAS,aAAA,GACAvF,SAAA,GAAAC,GACAuF,OAAA,GAOAjG,EAAAc,SACAd,EAAA+G,OAAAjG,EAAApG,MAEAsF,EAAAU,WAEAV,EAAAH,QACAG,EAAAgH,MAAAnH,EAAAgC,IAEA7B,EAAAO,cACAP,EAAAiH,YAAA1G,EAAA2E,OAEAlF,EAAAtF,MAAAsF,EAGA1O,EAAAC,QAAAyO,IAOClM,KAAA,WACD,MAAApC,QAAA,mBAAAgB,eAAAD,SN82B6BqB,KAAKvC,EAASe,EAAoB,KAIzD4U,GACA,SAAU5V,EAAQC,GOvnExBsB,MAAA2B,UAAA2S,QACAC,wBACAjP,QAAA,gCACA5E,MAAA,UAEA+H,SACAnD,QAAA,eACAC,YAAA,GAEAmE,QACApE,QAAA,iCACAE,QAAA,GAEAqE,UACAvE,QAAA,mDACAC,YAAA,GAEA0E,cACA3E,QAAA,0BACAC,YAAA,GAEA2E,QAAA,+KACAC,QAAA,qBACAC,OAAA,4EACAC,SAAA,uEACAxB,YAAA,kBP+nEM2L,GACA,SAAU/V,EAAQgW,EAAqBhV,GAE7C,YACAqB,QAAOO,eAAeoT,EAAqB,cAAgBnT,OAAO,GAC7C,IAAIoT,GAAuCjV,EAAoB,IAC3DkV,EAA+ClV,EAAoB2O,EAAEsG,GACrEE,EAAwCnV,EAAoB,IAC5DoV,EAAgDpV,EAAoB2O,EAAEwG,GACtEE,EAA6DrV,EAAoB,IAEjFsV,GADqEtV,EAAoB2O,EAAE0G,GAC1CrV,EAAoB,KAErEuV,GADyDvV,EAAoB2O,EAAE2G,GACZtV,EAAoB,KQv2DhHwV,GRw2DoGxV,EAAoB2O,EAAE4G,GQx2D1H,gzjBAAAvV,EAIA,uBAAAA,EAIA,uBAAAA,EAIA,uBAAAA,EAuJA,ktKAAAA,EAoBA,0nBAAAA,EAwVA,y+aACAgV,GAAA,SRo2CIvN,KQl2CJ,URm2CIzC,KAAM,WACF,OACInF,QAASqV,IQj2CrBM,KRo2CIC,QAAS,WACLL,EAA8CM,EQl2CtDpS,kBRw2CMqS,GACA,SAAU3W,EAAQC,EAASe,GShsEjCf,EAAAD,EAAAC,QAAAe,EAAA,QAKAf,EAAAiI,MAAAlI,EAAAoE,EAAA,OAAkCyJ,QAAA,EAAAC,WAAAC,SAAAC,SAAA,GAAAC,KAAA,cAAAE,WAAA,OTysE5ByI,GACA,SAAU5W,EAAQC,EAASe,GU5sEjC,GAAAgB,GAAAhB,EAAA,GACA,iBAAAgB,SAAAhC,EAAAoE,EAAApC,EAAA,MACAA,EAAAqM,SAAArO,EAAAC,QAAA+B,EAAAqM,OAEArN,GAAA,eAAAgB,GAAA,IVqtEM6U,GACA,SAAU7W,EAAQC,EAASe,GW7tEjChB,EAAAC,QAAAe,EAAA6G,EAAA,sCXmuEMiP,GACA,SAAU9W,EAAQC,EAASe,GYpuEjChB,EAAAC,QAAAe,EAAA6G,EAAA,sCZ0uEMkP,GACA,SAAU/W,EAAQC,EAASe,Ga3uEjChB,EAAAC,QAAAe,EAAA6G,EAAA,sCbivEMmP,GACA,SAAUhX,EAAQC,EAASe,GclvEjChB,EAAAC,QAAAe,EAAA6G,EAAA,sCdwvEMoP,GACA,SAAUjX,EAAQC,EAASe,GezvEjChB,EAAAC,QAAAe,EAAA6G,EAAA,sCf+vEMqP,GACA,SAAUlX,EAAQC,GgBhwExBD,EAAAC,QAAA","file":"static/js/1.c694b068eb9bba984b39.js","sourcesContent":["webpackJsonp([1],{\n\n/***/ 115:\n/***/ (function(module, exports) {\n\nmodule.exports={render:function (){var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;\n  return _c('div', {\n    staticClass: \"research-article\"\n  }, [_c('div', {\n    domProps: {\n      \"innerHTML\": _vm._s(_vm.rawHtml)\n    }\n  })])\n},staticRenderFns: []}\n\n/***/ }),\n\n/***/ 15:\n/***/ (function(module, exports, __webpack_require__) {\n\n\n/* styles */\n__webpack_require__(53)\n\nvar Component = __webpack_require__(4)(\n  /* script */\n  __webpack_require__(40),\n  /* template */\n  __webpack_require__(115),\n  /* scopeId */\n  \"data-v-fc76b68e\",\n  /* cssModules */\n  null\n)\n\nmodule.exports = Component.exports\n\n\n/***/ }),\n\n/***/ 19:\n/***/ (function(module, exports, __webpack_require__) {\n\n/* WEBPACK VAR INJECTION */(function(global) {\n/* **********************************************\n     Begin prism-core.js\n********************************************** */\n\nvar _self = (typeof window !== 'undefined')\n\t? window   // if in browser\n\t: (\n\t\t(typeof WorkerGlobalScope !== 'undefined' && self instanceof WorkerGlobalScope)\n\t\t? self // if in worker\n\t\t: {}   // if in node js\n\t);\n\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n * MIT license http://www.opensource.org/licenses/mit-license.php/\n * @author Lea Verou http://lea.verou.me\n */\n\nvar Prism = (function(){\n\n// Private helper vars\nvar lang = /\\blang(?:uage)?-(\\w+)\\b/i;\nvar uniqueId = 0;\n\nvar _ = _self.Prism = {\n\tutil: {\n\t\tencode: function (tokens) {\n\t\t\tif (tokens instanceof Token) {\n\t\t\t\treturn new Token(tokens.type, _.util.encode(tokens.content), tokens.alias);\n\t\t\t} else if (_.util.type(tokens) === 'Array') {\n\t\t\t\treturn tokens.map(_.util.encode);\n\t\t\t} else {\n\t\t\t\treturn tokens.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/\\u00a0/g, ' ');\n\t\t\t}\n\t\t},\n\n\t\ttype: function (o) {\n\t\t\treturn Object.prototype.toString.call(o).match(/\\[object (\\w+)\\]/)[1];\n\t\t},\n\n\t\tobjId: function (obj) {\n\t\t\tif (!obj['__id']) {\n\t\t\t\tObject.defineProperty(obj, '__id', { value: ++uniqueId });\n\t\t\t}\n\t\t\treturn obj['__id'];\n\t\t},\n\n\t\t// Deep clone a language definition (e.g. to extend it)\n\t\tclone: function (o) {\n\t\t\tvar type = _.util.type(o);\n\n\t\t\tswitch (type) {\n\t\t\t\tcase 'Object':\n\t\t\t\t\tvar clone = {};\n\n\t\t\t\t\tfor (var key in o) {\n\t\t\t\t\t\tif (o.hasOwnProperty(key)) {\n\t\t\t\t\t\t\tclone[key] = _.util.clone(o[key]);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\treturn clone;\n\n\t\t\t\tcase 'Array':\n\t\t\t\t\t// Check for existence for IE8\n\t\t\t\t\treturn o.map && o.map(function(v) { return _.util.clone(v); });\n\t\t\t}\n\n\t\t\treturn o;\n\t\t}\n\t},\n\n\tlanguages: {\n\t\textend: function (id, redef) {\n\t\t\tvar lang = _.util.clone(_.languages[id]);\n\n\t\t\tfor (var key in redef) {\n\t\t\t\tlang[key] = redef[key];\n\t\t\t}\n\n\t\t\treturn lang;\n\t\t},\n\n\t\t/**\n\t\t * Insert a token before another token in a language literal\n\t\t * As this needs to recreate the object (we cannot actually insert before keys in object literals),\n\t\t * we cannot just provide an object, we need anobject and a key.\n\t\t * @param inside The key (or language id) of the parent\n\t\t * @param before The key to insert before. If not provided, the function appends instead.\n\t\t * @param insert Object with the key/value pairs to insert\n\t\t * @param root The object that contains `inside`. If equal to Prism.languages, it can be omitted.\n\t\t */\n\t\tinsertBefore: function (inside, before, insert, root) {\n\t\t\troot = root || _.languages;\n\t\t\tvar grammar = root[inside];\n\n\t\t\tif (arguments.length == 2) {\n\t\t\t\tinsert = arguments[1];\n\n\t\t\t\tfor (var newToken in insert) {\n\t\t\t\t\tif (insert.hasOwnProperty(newToken)) {\n\t\t\t\t\t\tgrammar[newToken] = insert[newToken];\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\treturn grammar;\n\t\t\t}\n\n\t\t\tvar ret = {};\n\n\t\t\tfor (var token in grammar) {\n\n\t\t\t\tif (grammar.hasOwnProperty(token)) {\n\n\t\t\t\t\tif (token == before) {\n\n\t\t\t\t\t\tfor (var newToken in insert) {\n\n\t\t\t\t\t\t\tif (insert.hasOwnProperty(newToken)) {\n\t\t\t\t\t\t\t\tret[newToken] = insert[newToken];\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tret[token] = grammar[token];\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Update references in other language definitions\n\t\t\t_.languages.DFS(_.languages, function(key, value) {\n\t\t\t\tif (value === root[inside] && key != inside) {\n\t\t\t\t\tthis[key] = ret;\n\t\t\t\t}\n\t\t\t});\n\n\t\t\treturn root[inside] = ret;\n\t\t},\n\n\t\t// Traverse a language definition with Depth First Search\n\t\tDFS: function(o, callback, type, visited) {\n\t\t\tvisited = visited || {};\n\t\t\tfor (var i in o) {\n\t\t\t\tif (o.hasOwnProperty(i)) {\n\t\t\t\t\tcallback.call(o, i, o[i], type || i);\n\n\t\t\t\t\tif (_.util.type(o[i]) === 'Object' && !visited[_.util.objId(o[i])]) {\n\t\t\t\t\t\tvisited[_.util.objId(o[i])] = true;\n\t\t\t\t\t\t_.languages.DFS(o[i], callback, null, visited);\n\t\t\t\t\t}\n\t\t\t\t\telse if (_.util.type(o[i]) === 'Array' && !visited[_.util.objId(o[i])]) {\n\t\t\t\t\t\tvisited[_.util.objId(o[i])] = true;\n\t\t\t\t\t\t_.languages.DFS(o[i], callback, i, visited);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t},\n\tplugins: {},\n\n\thighlightAll: function(async, callback) {\n\t\tvar env = {\n\t\t\tcallback: callback,\n\t\t\tselector: 'code[class*=\"language-\"], [class*=\"language-\"] code, code[class*=\"lang-\"], [class*=\"lang-\"] code'\n\t\t};\n\n\t\t_.hooks.run(\"before-highlightall\", env);\n\n\t\tvar elements = env.elements || document.querySelectorAll(env.selector);\n\n\t\tfor (var i=0, element; element = elements[i++];) {\n\t\t\t_.highlightElement(element, async === true, env.callback);\n\t\t}\n\t},\n\n\thighlightElement: function(element, async, callback) {\n\t\t// Find language\n\t\tvar language, grammar, parent = element;\n\n\t\twhile (parent && !lang.test(parent.className)) {\n\t\t\tparent = parent.parentNode;\n\t\t}\n\n\t\tif (parent) {\n\t\t\tlanguage = (parent.className.match(lang) || [,''])[1].toLowerCase();\n\t\t\tgrammar = _.languages[language];\n\t\t}\n\n\t\t// Set language on the element, if not present\n\t\telement.className = element.className.replace(lang, '').replace(/\\s+/g, ' ') + ' language-' + language;\n\n\t\t// Set language on the parent, for styling\n\t\tparent = element.parentNode;\n\n\t\tif (/pre/i.test(parent.nodeName)) {\n\t\t\tparent.className = parent.className.replace(lang, '').replace(/\\s+/g, ' ') + ' language-' + language;\n\t\t}\n\n\t\tvar code = element.textContent;\n\n\t\tvar env = {\n\t\t\telement: element,\n\t\t\tlanguage: language,\n\t\t\tgrammar: grammar,\n\t\t\tcode: code\n\t\t};\n\n\t\t_.hooks.run('before-sanity-check', env);\n\n\t\tif (!env.code || !env.grammar) {\n\t\t\tif (env.code) {\n\t\t\t\tenv.element.textContent = env.code;\n\t\t\t}\n\t\t\t_.hooks.run('complete', env);\n\t\t\treturn;\n\t\t}\n\n\t\t_.hooks.run('before-highlight', env);\n\n\t\tif (async && _self.Worker) {\n\t\t\tvar worker = new Worker(_.filename);\n\n\t\t\tworker.onmessage = function(evt) {\n\t\t\t\tenv.highlightedCode = evt.data;\n\n\t\t\t\t_.hooks.run('before-insert', env);\n\n\t\t\t\tenv.element.innerHTML = env.highlightedCode;\n\n\t\t\t\tcallback && callback.call(env.element);\n\t\t\t\t_.hooks.run('after-highlight', env);\n\t\t\t\t_.hooks.run('complete', env);\n\t\t\t};\n\n\t\t\tworker.postMessage(JSON.stringify({\n\t\t\t\tlanguage: env.language,\n\t\t\t\tcode: env.code,\n\t\t\t\timmediateClose: true\n\t\t\t}));\n\t\t}\n\t\telse {\n\t\t\tenv.highlightedCode = _.highlight(env.code, env.grammar, env.language);\n\n\t\t\t_.hooks.run('before-insert', env);\n\n\t\t\tenv.element.innerHTML = env.highlightedCode;\n\n\t\t\tcallback && callback.call(element);\n\n\t\t\t_.hooks.run('after-highlight', env);\n\t\t\t_.hooks.run('complete', env);\n\t\t}\n\t},\n\n\thighlight: function (text, grammar, language) {\n\t\tvar tokens = _.tokenize(text, grammar);\n\t\treturn Token.stringify(_.util.encode(tokens), language);\n\t},\n\n\ttokenize: function(text, grammar, language) {\n\t\tvar Token = _.Token;\n\n\t\tvar strarr = [text];\n\n\t\tvar rest = grammar.rest;\n\n\t\tif (rest) {\n\t\t\tfor (var token in rest) {\n\t\t\t\tgrammar[token] = rest[token];\n\t\t\t}\n\n\t\t\tdelete grammar.rest;\n\t\t}\n\n\t\ttokenloop: for (var token in grammar) {\n\t\t\tif(!grammar.hasOwnProperty(token) || !grammar[token]) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tvar patterns = grammar[token];\n\t\t\tpatterns = (_.util.type(patterns) === \"Array\") ? patterns : [patterns];\n\n\t\t\tfor (var j = 0; j < patterns.length; ++j) {\n\t\t\t\tvar pattern = patterns[j],\n\t\t\t\t\tinside = pattern.inside,\n\t\t\t\t\tlookbehind = !!pattern.lookbehind,\n\t\t\t\t\tgreedy = !!pattern.greedy,\n\t\t\t\t\tlookbehindLength = 0,\n\t\t\t\t\talias = pattern.alias;\n\n\t\t\t\tif (greedy && !pattern.pattern.global) {\n\t\t\t\t\t// Without the global flag, lastIndex won't work\n\t\t\t\t\tvar flags = pattern.pattern.toString().match(/[imuy]*$/)[0];\n\t\t\t\t\tpattern.pattern = RegExp(pattern.pattern.source, flags + \"g\");\n\t\t\t\t}\n\n\t\t\t\tpattern = pattern.pattern || pattern;\n\n\t\t\t\t// Don’t cache length as it changes during the loop\n\t\t\t\tfor (var i=0, pos = 0; i<strarr.length; pos += strarr[i].length, ++i) {\n\n\t\t\t\t\tvar str = strarr[i];\n\n\t\t\t\t\tif (strarr.length > text.length) {\n\t\t\t\t\t\t// Something went terribly wrong, ABORT, ABORT!\n\t\t\t\t\t\tbreak tokenloop;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (str instanceof Token) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\n\t\t\t\t\tpattern.lastIndex = 0;\n\n\t\t\t\t\tvar match = pattern.exec(str),\n\t\t\t\t\t    delNum = 1;\n\n\t\t\t\t\t// Greedy patterns can override/remove up to two previously matched tokens\n\t\t\t\t\tif (!match && greedy && i != strarr.length - 1) {\n\t\t\t\t\t\tpattern.lastIndex = pos;\n\t\t\t\t\t\tmatch = pattern.exec(text);\n\t\t\t\t\t\tif (!match) {\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tvar from = match.index + (lookbehind ? match[1].length : 0),\n\t\t\t\t\t\t    to = match.index + match[0].length,\n\t\t\t\t\t\t    k = i,\n\t\t\t\t\t\t    p = pos;\n\n\t\t\t\t\t\tfor (var len = strarr.length; k < len && p < to; ++k) {\n\t\t\t\t\t\t\tp += strarr[k].length;\n\t\t\t\t\t\t\t// Move the index i to the element in strarr that is closest to from\n\t\t\t\t\t\t\tif (from >= p) {\n\t\t\t\t\t\t\t\t++i;\n\t\t\t\t\t\t\t\tpos = p;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * If strarr[i] is a Token, then the match starts inside another Token, which is invalid\n\t\t\t\t\t\t * If strarr[k - 1] is greedy we are in conflict with another greedy pattern\n\t\t\t\t\t\t */\n\t\t\t\t\t\tif (strarr[i] instanceof Token || strarr[k - 1].greedy) {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// Number of tokens to delete and replace with the new match\n\t\t\t\t\t\tdelNum = k - i;\n\t\t\t\t\t\tstr = text.slice(pos, p);\n\t\t\t\t\t\tmatch.index -= pos;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (!match) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\n\t\t\t\t\tif(lookbehind) {\n\t\t\t\t\t\tlookbehindLength = match[1].length;\n\t\t\t\t\t}\n\n\t\t\t\t\tvar from = match.index + lookbehindLength,\n\t\t\t\t\t    match = match[0].slice(lookbehindLength),\n\t\t\t\t\t    to = from + match.length,\n\t\t\t\t\t    before = str.slice(0, from),\n\t\t\t\t\t    after = str.slice(to);\n\n\t\t\t\t\tvar args = [i, delNum];\n\n\t\t\t\t\tif (before) {\n\t\t\t\t\t\targs.push(before);\n\t\t\t\t\t}\n\n\t\t\t\t\tvar wrapped = new Token(token, inside? _.tokenize(match, inside) : match, alias, match, greedy);\n\n\t\t\t\t\targs.push(wrapped);\n\n\t\t\t\t\tif (after) {\n\t\t\t\t\t\targs.push(after);\n\t\t\t\t\t}\n\n\t\t\t\t\tArray.prototype.splice.apply(strarr, args);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn strarr;\n\t},\n\n\thooks: {\n\t\tall: {},\n\n\t\tadd: function (name, callback) {\n\t\t\tvar hooks = _.hooks.all;\n\n\t\t\thooks[name] = hooks[name] || [];\n\n\t\t\thooks[name].push(callback);\n\t\t},\n\n\t\trun: function (name, env) {\n\t\t\tvar callbacks = _.hooks.all[name];\n\n\t\t\tif (!callbacks || !callbacks.length) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tfor (var i=0, callback; callback = callbacks[i++];) {\n\t\t\t\tcallback(env);\n\t\t\t}\n\t\t}\n\t}\n};\n\nvar Token = _.Token = function(type, content, alias, matchedStr, greedy) {\n\tthis.type = type;\n\tthis.content = content;\n\tthis.alias = alias;\n\t// Copy of the full string this token was created from\n\tthis.length = (matchedStr || \"\").length|0;\n\tthis.greedy = !!greedy;\n};\n\nToken.stringify = function(o, language, parent) {\n\tif (typeof o == 'string') {\n\t\treturn o;\n\t}\n\n\tif (_.util.type(o) === 'Array') {\n\t\treturn o.map(function(element) {\n\t\t\treturn Token.stringify(element, language, o);\n\t\t}).join('');\n\t}\n\n\tvar env = {\n\t\ttype: o.type,\n\t\tcontent: Token.stringify(o.content, language, parent),\n\t\ttag: 'span',\n\t\tclasses: ['token', o.type],\n\t\tattributes: {},\n\t\tlanguage: language,\n\t\tparent: parent\n\t};\n\n\tif (env.type == 'comment') {\n\t\tenv.attributes['spellcheck'] = 'true';\n\t}\n\n\tif (o.alias) {\n\t\tvar aliases = _.util.type(o.alias) === 'Array' ? o.alias : [o.alias];\n\t\tArray.prototype.push.apply(env.classes, aliases);\n\t}\n\n\t_.hooks.run('wrap', env);\n\n\tvar attributes = Object.keys(env.attributes).map(function(name) {\n\t\treturn name + '=\"' + (env.attributes[name] || '').replace(/\"/g, '&quot;') + '\"';\n\t}).join(' ');\n\n\treturn '<' + env.tag + ' class=\"' + env.classes.join(' ') + '\"' + (attributes ? ' ' + attributes : '') + '>' + env.content + '</' + env.tag + '>';\n\n};\n\nif (!_self.document) {\n\tif (!_self.addEventListener) {\n\t\t// in Node.js\n\t\treturn _self.Prism;\n\t}\n \t// In worker\n\t_self.addEventListener('message', function(evt) {\n\t\tvar message = JSON.parse(evt.data),\n\t\t    lang = message.language,\n\t\t    code = message.code,\n\t\t    immediateClose = message.immediateClose;\n\n\t\t_self.postMessage(_.highlight(code, _.languages[lang], lang));\n\t\tif (immediateClose) {\n\t\t\t_self.close();\n\t\t}\n\t}, false);\n\n\treturn _self.Prism;\n}\n\n//Get current script and highlight\nvar script = document.currentScript || [].slice.call(document.getElementsByTagName(\"script\")).pop();\n\nif (script) {\n\t_.filename = script.src;\n\n\tif (document.addEventListener && !script.hasAttribute('data-manual')) {\n\t\tif(document.readyState !== \"loading\") {\n\t\t\tif (window.requestAnimationFrame) {\n\t\t\t\twindow.requestAnimationFrame(_.highlightAll);\n\t\t\t} else {\n\t\t\t\twindow.setTimeout(_.highlightAll, 16);\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tdocument.addEventListener('DOMContentLoaded', _.highlightAll);\n\t\t}\n\t}\n}\n\nreturn _self.Prism;\n\n})();\n\nif (typeof module !== 'undefined' && module.exports) {\n\tmodule.exports = Prism;\n}\n\n// hack for components to work correctly in node.js\nif (typeof global !== 'undefined') {\n\tglobal.Prism = Prism;\n}\n\n\n/* **********************************************\n     Begin prism-markup.js\n********************************************** */\n\nPrism.languages.markup = {\n\t'comment': /<!--[\\w\\W]*?-->/,\n\t'prolog': /<\\?[\\w\\W]+?\\?>/,\n\t'doctype': /<!DOCTYPE[\\w\\W]+?>/i,\n\t'cdata': /<!\\[CDATA\\[[\\w\\W]*?]]>/i,\n\t'tag': {\n\t\tpattern: /<\\/?(?!\\d)[^\\s>\\/=$<]+(?:\\s+[^\\s>\\/=]+(?:=(?:(\"|')(?:\\\\\\1|\\\\?(?!\\1)[\\w\\W])*\\1|[^\\s'\">=]+))?)*\\s*\\/?>/i,\n\t\tinside: {\n\t\t\t'tag': {\n\t\t\t\tpattern: /^<\\/?[^\\s>\\/]+/i,\n\t\t\t\tinside: {\n\t\t\t\t\t'punctuation': /^<\\/?/,\n\t\t\t\t\t'namespace': /^[^\\s>\\/:]+:/\n\t\t\t\t}\n\t\t\t},\n\t\t\t'attr-value': {\n\t\t\t\tpattern: /=(?:('|\")[\\w\\W]*?(\\1)|[^\\s>]+)/i,\n\t\t\t\tinside: {\n\t\t\t\t\t'punctuation': /[=>\"']/\n\t\t\t\t}\n\t\t\t},\n\t\t\t'punctuation': /\\/?>/,\n\t\t\t'attr-name': {\n\t\t\t\tpattern: /[^\\s>\\/]+/,\n\t\t\t\tinside: {\n\t\t\t\t\t'namespace': /^[^\\s>\\/:]+:/\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n\t},\n\t'entity': /&#?[\\da-z]{1,8};/i\n};\n\n// Plugin to make entity title show the real entity, idea by Roman Komarov\nPrism.hooks.add('wrap', function(env) {\n\n\tif (env.type === 'entity') {\n\t\tenv.attributes['title'] = env.content.replace(/&amp;/, '&');\n\t}\n});\n\nPrism.languages.xml = Prism.languages.markup;\nPrism.languages.html = Prism.languages.markup;\nPrism.languages.mathml = Prism.languages.markup;\nPrism.languages.svg = Prism.languages.markup;\n\n\n/* **********************************************\n     Begin prism-css.js\n********************************************** */\n\nPrism.languages.css = {\n\t'comment': /\\/\\*[\\w\\W]*?\\*\\//,\n\t'atrule': {\n\t\tpattern: /@[\\w-]+?.*?(;|(?=\\s*\\{))/i,\n\t\tinside: {\n\t\t\t'rule': /@[\\w-]+/\n\t\t\t// See rest below\n\t\t}\n\t},\n\t'url': /url\\((?:([\"'])(\\\\(?:\\r\\n|[\\w\\W])|(?!\\1)[^\\\\\\r\\n])*\\1|.*?)\\)/i,\n\t'selector': /[^\\{\\}\\s][^\\{\\};]*?(?=\\s*\\{)/,\n\t'string': {\n\t\tpattern: /(\"|')(\\\\(?:\\r\\n|[\\w\\W])|(?!\\1)[^\\\\\\r\\n])*\\1/,\n\t\tgreedy: true\n\t},\n\t'property': /(\\b|\\B)[\\w-]+(?=\\s*:)/i,\n\t'important': /\\B!important\\b/i,\n\t'function': /[-a-z0-9]+(?=\\()/i,\n\t'punctuation': /[(){};:]/\n};\n\nPrism.languages.css['atrule'].inside.rest = Prism.util.clone(Prism.languages.css);\n\nif (Prism.languages.markup) {\n\tPrism.languages.insertBefore('markup', 'tag', {\n\t\t'style': {\n\t\t\tpattern: /(<style[\\w\\W]*?>)[\\w\\W]*?(?=<\\/style>)/i,\n\t\t\tlookbehind: true,\n\t\t\tinside: Prism.languages.css,\n\t\t\talias: 'language-css'\n\t\t}\n\t});\n\t\n\tPrism.languages.insertBefore('inside', 'attr-value', {\n\t\t'style-attr': {\n\t\t\tpattern: /\\s*style=(\"|').*?\\1/i,\n\t\t\tinside: {\n\t\t\t\t'attr-name': {\n\t\t\t\t\tpattern: /^\\s*style/i,\n\t\t\t\t\tinside: Prism.languages.markup.tag.inside\n\t\t\t\t},\n\t\t\t\t'punctuation': /^\\s*=\\s*['\"]|['\"]\\s*$/,\n\t\t\t\t'attr-value': {\n\t\t\t\t\tpattern: /.+/i,\n\t\t\t\t\tinside: Prism.languages.css\n\t\t\t\t}\n\t\t\t},\n\t\t\talias: 'language-css'\n\t\t}\n\t}, Prism.languages.markup.tag);\n}\n\n/* **********************************************\n     Begin prism-clike.js\n********************************************** */\n\nPrism.languages.clike = {\n\t'comment': [\n\t\t{\n\t\t\tpattern: /(^|[^\\\\])\\/\\*[\\w\\W]*?\\*\\//,\n\t\t\tlookbehind: true\n\t\t},\n\t\t{\n\t\t\tpattern: /(^|[^\\\\:])\\/\\/.*/,\n\t\t\tlookbehind: true\n\t\t}\n\t],\n\t'string': {\n\t\tpattern: /([\"'])(\\\\(?:\\r\\n|[\\s\\S])|(?!\\1)[^\\\\\\r\\n])*\\1/,\n\t\tgreedy: true\n\t},\n\t'class-name': {\n\t\tpattern: /((?:\\b(?:class|interface|extends|implements|trait|instanceof|new)\\s+)|(?:catch\\s+\\())[a-z0-9_\\.\\\\]+/i,\n\t\tlookbehind: true,\n\t\tinside: {\n\t\t\tpunctuation: /(\\.|\\\\)/\n\t\t}\n\t},\n\t'keyword': /\\b(if|else|while|do|for|return|in|instanceof|function|new|try|throw|catch|finally|null|break|continue)\\b/,\n\t'boolean': /\\b(true|false)\\b/,\n\t'function': /[a-z0-9_]+(?=\\()/i,\n\t'number': /\\b-?(?:0x[\\da-f]+|\\d*\\.?\\d+(?:e[+-]?\\d+)?)\\b/i,\n\t'operator': /--?|\\+\\+?|!=?=?|<=?|>=?|==?=?|&&?|\\|\\|?|\\?|\\*|\\/|~|\\^|%/,\n\t'punctuation': /[{}[\\];(),.:]/\n};\n\n\n/* **********************************************\n     Begin prism-javascript.js\n********************************************** */\n\nPrism.languages.javascript = Prism.languages.extend('clike', {\n\t'keyword': /\\b(as|async|await|break|case|catch|class|const|continue|debugger|default|delete|do|else|enum|export|extends|finally|for|from|function|get|if|implements|import|in|instanceof|interface|let|new|null|of|package|private|protected|public|return|set|static|super|switch|this|throw|try|typeof|var|void|while|with|yield)\\b/,\n\t'number': /\\b-?(0x[\\dA-Fa-f]+|0b[01]+|0o[0-7]+|\\d*\\.?\\d+([Ee][+-]?\\d+)?|NaN|Infinity)\\b/,\n\t// Allow for all non-ASCII characters (See http://stackoverflow.com/a/2008444)\n\t'function': /[_$a-zA-Z\\xA0-\\uFFFF][_$a-zA-Z0-9\\xA0-\\uFFFF]*(?=\\()/i,\n\t'operator': /--?|\\+\\+?|!=?=?|<=?|>=?|==?=?|&&?|\\|\\|?|\\?|\\*\\*?|\\/|~|\\^|%|\\.{3}/\n});\n\nPrism.languages.insertBefore('javascript', 'keyword', {\n\t'regex': {\n\t\tpattern: /(^|[^/])\\/(?!\\/)(\\[.+?]|\\\\.|[^/\\\\\\r\\n])+\\/[gimyu]{0,5}(?=\\s*($|[\\r\\n,.;})]))/,\n\t\tlookbehind: true,\n\t\tgreedy: true\n\t}\n});\n\nPrism.languages.insertBefore('javascript', 'string', {\n\t'template-string': {\n\t\tpattern: /`(?:\\\\\\\\|\\\\?[^\\\\])*?`/,\n\t\tgreedy: true,\n\t\tinside: {\n\t\t\t'interpolation': {\n\t\t\t\tpattern: /\\$\\{[^}]+\\}/,\n\t\t\t\tinside: {\n\t\t\t\t\t'interpolation-punctuation': {\n\t\t\t\t\t\tpattern: /^\\$\\{|\\}$/,\n\t\t\t\t\t\talias: 'punctuation'\n\t\t\t\t\t},\n\t\t\t\t\trest: Prism.languages.javascript\n\t\t\t\t}\n\t\t\t},\n\t\t\t'string': /[\\s\\S]+/\n\t\t}\n\t}\n});\n\nif (Prism.languages.markup) {\n\tPrism.languages.insertBefore('markup', 'tag', {\n\t\t'script': {\n\t\t\tpattern: /(<script[\\w\\W]*?>)[\\w\\W]*?(?=<\\/script>)/i,\n\t\t\tlookbehind: true,\n\t\t\tinside: Prism.languages.javascript,\n\t\t\talias: 'language-javascript'\n\t\t}\n\t});\n}\n\nPrism.languages.js = Prism.languages.javascript;\n\n/* **********************************************\n     Begin prism-file-highlight.js\n********************************************** */\n\n(function () {\n\tif (typeof self === 'undefined' || !self.Prism || !self.document || !document.querySelector) {\n\t\treturn;\n\t}\n\n\tself.Prism.fileHighlight = function() {\n\n\t\tvar Extensions = {\n\t\t\t'js': 'javascript',\n\t\t\t'py': 'python',\n\t\t\t'rb': 'ruby',\n\t\t\t'ps1': 'powershell',\n\t\t\t'psm1': 'powershell',\n\t\t\t'sh': 'bash',\n\t\t\t'bat': 'batch',\n\t\t\t'h': 'c',\n\t\t\t'tex': 'latex'\n\t\t};\n\n\t\tif(Array.prototype.forEach) { // Check to prevent error in IE8\n\t\t\tArray.prototype.slice.call(document.querySelectorAll('pre[data-src]')).forEach(function (pre) {\n\t\t\t\tvar src = pre.getAttribute('data-src');\n\n\t\t\t\tvar language, parent = pre;\n\t\t\t\tvar lang = /\\blang(?:uage)?-(?!\\*)(\\w+)\\b/i;\n\t\t\t\twhile (parent && !lang.test(parent.className)) {\n\t\t\t\t\tparent = parent.parentNode;\n\t\t\t\t}\n\n\t\t\t\tif (parent) {\n\t\t\t\t\tlanguage = (pre.className.match(lang) || [, ''])[1];\n\t\t\t\t}\n\n\t\t\t\tif (!language) {\n\t\t\t\t\tvar extension = (src.match(/\\.(\\w+)$/) || [, ''])[1];\n\t\t\t\t\tlanguage = Extensions[extension] || extension;\n\t\t\t\t}\n\n\t\t\t\tvar code = document.createElement('code');\n\t\t\t\tcode.className = 'language-' + language;\n\n\t\t\t\tpre.textContent = '';\n\n\t\t\t\tcode.textContent = 'Loading…';\n\n\t\t\t\tpre.appendChild(code);\n\n\t\t\t\tvar xhr = new XMLHttpRequest();\n\n\t\t\t\txhr.open('GET', src, true);\n\n\t\t\t\txhr.onreadystatechange = function () {\n\t\t\t\t\tif (xhr.readyState == 4) {\n\n\t\t\t\t\t\tif (xhr.status < 400 && xhr.responseText) {\n\t\t\t\t\t\t\tcode.textContent = xhr.responseText;\n\n\t\t\t\t\t\t\tPrism.highlightElement(code);\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse if (xhr.status >= 400) {\n\t\t\t\t\t\t\tcode.textContent = '✖ Error ' + xhr.status + ' while fetching file: ' + xhr.statusText;\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\tcode.textContent = '✖ Error: File does not exist or is empty';\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t};\n\n\t\t\t\txhr.send(null);\n\t\t\t});\n\t\t}\n\n\t};\n\n\tdocument.addEventListener('DOMContentLoaded', self.Prism.fileHighlight);\n\n})();\n\n/* WEBPACK VAR INJECTION */}.call(exports, __webpack_require__(5)))\n\n/***/ }),\n\n/***/ 20:\n/***/ (function(module, exports, __webpack_require__) {\n\nexports = module.exports = __webpack_require__(13)(true);\n// imports\n\n\n// module\nexports.push([module.i, \"code[class*=language-],pre[class*=language-]{color:#000;background:none;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{position:relative;margin:.5em 0;box-shadow:-1px 0 0 0 #358ccb,0 0 0 1px #dfdfdf;border-left:10px solid #358ccb;background-color:#fdfdfd;background-image:linear-gradient(transparent 50%,rgba(69,142,209,.04) 0);background-size:3em 3em;background-origin:content-box;overflow:visible;padding:0}code[class*=language]{max-height:inherit;height:100%;padding:0 1em;display:block;overflow:auto}:not(pre)>code[class*=language-],pre[class*=language-]{background-color:#fdfdfd;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;margin-bottom:1em}:not(pre)>code[class*=language-]{position:relative;padding:.2em;border-radius:.3em;color:#c92c2c;border:1px solid rgba(0,0,0,.1);display:inline;white-space:normal}pre[class*=language-]:after,pre[class*=language-]:before{content:\\\"\\\";z-index:-2;display:block;position:absolute;bottom:.75em;left:.18em;width:40%;height:20%;max-height:13em;box-shadow:0 13px 8px #979797;-webkit-transform:rotate(-2deg);-moz-transform:rotate(-2deg);-ms-transform:rotate(-2deg);-o-transform:rotate(-2deg);transform:rotate(-2deg)}:not(pre)>code[class*=language-]:after,pre[class*=language-]:after{right:.75em;left:auto;-webkit-transform:rotate(2deg);-moz-transform:rotate(2deg);-ms-transform:rotate(2deg);-o-transform:rotate(2deg);transform:rotate(2deg)}.token.block-comment,.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#7d8b99}.token.punctuation{color:#5f6364}.token.boolean,.token.constant,.token.deleted,.token.function-name,.token.number,.token.property,.token.symbol,.token.tag{color:#c92c2c}.token.attr-name,.token.builtin,.token.char,.token.function,.token.inserted,.token.selector,.token.string{color:#2f9c0a}.token.entity,.token.operator,.token.url,.token.variable{color:#a67f59;background:hsla(0,0%,100%,.5)}.token.atrule,.token.attr-value,.token.class-name,.token.keyword{color:#1990b8}.token.important,.token.regex{color:#e90}.language-css .token.string,.style .token.string{color:#a67f59;background:hsla(0,0%,100%,.5)}.token.important{font-weight:400}.token.bold{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}.namespace{opacity:.7}@media screen and (max-width:767px){pre[class*=language-]:after,pre[class*=language-]:before{bottom:14px;box-shadow:none}}.token.cr:before,.token.lf:before,.token.tab:not(:empty):before{color:#e0d7d1}pre[class*=language-].line-numbers{padding-left:0}pre[class*=language-].line-numbers code{padding-left:3.8em}pre[class*=language-].line-numbers .line-numbers-rows{left:0}pre[class*=language-][data-line]{padding-top:0;padding-bottom:0;padding-left:0}pre[data-line] code{position:relative;padding-left:4em}pre .line-highlight{margin-top:0}\", \"\", {\"version\":3,\"sources\":[\"F:/Github/newvue/node_modules/prismjs/themes/prism-coy.css\"],\"names\":[],\"mappings\":\"AAMA,6CAEC,WAAa,AACb,gBAAiB,AACjB,8DAAuE,AACvE,gBAAiB,AACjB,gBAAiB,AACjB,oBAAqB,AACrB,kBAAmB,AACnB,iBAAkB,AAClB,gBAAiB,AAEjB,gBAAiB,AACjB,cAAe,AACf,WAAY,AAEZ,qBAAsB,AACtB,kBAAmB,AACnB,iBAAkB,AAClB,YAAc,CACd,AAGD,sBACC,kBAAmB,AACnB,cAAe,AACf,gDAA8D,AAC9D,+BAAgC,AAChC,yBAA0B,AAC1B,yEAAiF,AACjF,wBAAyB,AACzB,8BAA+B,AAC/B,iBAAkB,AAClB,SAAW,CACX,AAED,sBACC,mBAAoB,AACpB,YAAa,AACb,cAAe,AACf,cAAe,AACf,aAAe,CACf,AAGD,uDAEC,yBAA0B,AAC1B,8BAA+B,AAC/B,2BAA4B,AAC5B,sBAAuB,AACvB,iBAAmB,CACnB,AAGD,iCACC,kBAAmB,AACnB,aAAc,AACd,mBAAqB,AACrB,cAAe,AACf,gCAAqC,AACrC,eAAgB,AAChB,kBAAoB,CACpB,AAED,yDAEC,WAAY,AACZ,WAAY,AACZ,cAAe,AACf,kBAAmB,AACnB,aAAe,AACf,WAAa,AACb,UAAW,AACX,WAAY,AACZ,gBAAiB,AACjB,8BAAiC,AACjC,gCAAiC,AACjC,6BAA8B,AAC9B,4BAA6B,AAC7B,2BAA4B,AAC5B,uBAAyB,CACzB,AAED,mEAEC,YAAc,AACd,UAAW,AACX,+BAAgC,AAChC,4BAA6B,AAC7B,2BAA4B,AAC5B,0BAA2B,AAC3B,sBAAwB,CACxB,AAED,8EAKC,aAAe,CACf,AAED,mBACC,aAAe,CACf,AAED,0HAQC,aAAe,CACf,AAED,0GAOC,aAAe,CACf,AAED,yDAIC,cAAe,AACf,6BAAqC,CACrC,AAED,iEAIC,aAAe,CACf,AAED,8BAEC,UAAY,CACZ,AAED,iDAEC,cAAe,AACf,6BAAqC,CACrC,AAED,iBACC,eAAoB,CACpB,AAED,YACC,eAAkB,CAClB,AACD,cACC,iBAAmB,CACnB,AAED,cACC,WAAa,CACb,AAED,WACC,UAAY,CACZ,AAED,oCACC,yDAEC,YAAa,AACb,eAAiB,CACjB,CAED,AAGD,gEAGC,aAAe,CACf,AAGD,mCACC,cAAgB,CAChB,AAED,wCACC,kBAAoB,CACpB,AAED,sDACC,MAAQ,CACR,AAGD,iCACC,cAAe,AACf,iBAAkB,AAClB,cAAgB,CAChB,AACD,oBACC,kBAAmB,AACnB,gBAAkB,CAClB,AACD,oBACC,YAAc,CACd\",\"file\":\"prism-coy.css\",\"sourcesContent\":[\"/**\\n * prism.js Coy theme for JavaScript, CoffeeScript, CSS and HTML\\n * Based on https://github.com/tshedor/workshop-wp-theme (Example: http://workshop.kansan.com/category/sessions/basics or http://workshop.timshedor.com/category/sessions/basics);\\n * @author Tim  Shedor\\n */\\n\\ncode[class*=\\\"language-\\\"],\\npre[class*=\\\"language-\\\"] {\\n\\tcolor: black;\\n\\tbackground: none;\\n\\tfont-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;\\n\\ttext-align: left;\\n\\twhite-space: pre;\\n\\tword-spacing: normal;\\n\\tword-break: normal;\\n\\tword-wrap: normal;\\n\\tline-height: 1.5;\\n\\n\\t-moz-tab-size: 4;\\n\\t-o-tab-size: 4;\\n\\ttab-size: 4;\\n\\n\\t-webkit-hyphens: none;\\n\\t-moz-hyphens: none;\\n\\t-ms-hyphens: none;\\n\\thyphens: none;\\n}\\n\\n/* Code blocks */\\npre[class*=\\\"language-\\\"] {\\n\\tposition: relative;\\n\\tmargin: .5em 0;\\n\\tbox-shadow: -1px 0px 0px 0px #358ccb, 0px 0px 0px 1px #dfdfdf;\\n\\tborder-left: 10px solid #358ccb;\\n\\tbackground-color: #fdfdfd;\\n\\tbackground-image: linear-gradient(transparent 50%, rgba(69, 142, 209, 0.04) 50%);\\n\\tbackground-size: 3em 3em;\\n\\tbackground-origin: content-box;\\n\\toverflow: visible;\\n\\tpadding: 0;\\n}\\n\\ncode[class*=\\\"language\\\"] {\\n\\tmax-height: inherit;\\n\\theight: 100%;\\n\\tpadding: 0 1em;\\n\\tdisplay: block;\\n\\toverflow: auto;\\n}\\n\\n/* Margin bottom to accomodate shadow */\\n:not(pre) > code[class*=\\\"language-\\\"],\\npre[class*=\\\"language-\\\"] {\\n\\tbackground-color: #fdfdfd;\\n\\t-webkit-box-sizing: border-box;\\n\\t-moz-box-sizing: border-box;\\n\\tbox-sizing: border-box;\\n\\tmargin-bottom: 1em;\\n}\\n\\n/* Inline code */\\n:not(pre) > code[class*=\\\"language-\\\"] {\\n\\tposition: relative;\\n\\tpadding: .2em;\\n\\tborder-radius: 0.3em;\\n\\tcolor: #c92c2c;\\n\\tborder: 1px solid rgba(0, 0, 0, 0.1);\\n\\tdisplay: inline;\\n\\twhite-space: normal;\\n}\\n\\npre[class*=\\\"language-\\\"]:before,\\npre[class*=\\\"language-\\\"]:after {\\n\\tcontent: '';\\n\\tz-index: -2;\\n\\tdisplay: block;\\n\\tposition: absolute;\\n\\tbottom: 0.75em;\\n\\tleft: 0.18em;\\n\\twidth: 40%;\\n\\theight: 20%;\\n\\tmax-height: 13em;\\n\\tbox-shadow: 0px 13px 8px #979797;\\n\\t-webkit-transform: rotate(-2deg);\\n\\t-moz-transform: rotate(-2deg);\\n\\t-ms-transform: rotate(-2deg);\\n\\t-o-transform: rotate(-2deg);\\n\\ttransform: rotate(-2deg);\\n}\\n\\n:not(pre) > code[class*=\\\"language-\\\"]:after,\\npre[class*=\\\"language-\\\"]:after {\\n\\tright: 0.75em;\\n\\tleft: auto;\\n\\t-webkit-transform: rotate(2deg);\\n\\t-moz-transform: rotate(2deg);\\n\\t-ms-transform: rotate(2deg);\\n\\t-o-transform: rotate(2deg);\\n\\ttransform: rotate(2deg);\\n}\\n\\n.token.comment,\\n.token.block-comment,\\n.token.prolog,\\n.token.doctype,\\n.token.cdata {\\n\\tcolor: #7D8B99;\\n}\\n\\n.token.punctuation {\\n\\tcolor: #5F6364;\\n}\\n\\n.token.property,\\n.token.tag,\\n.token.boolean,\\n.token.number,\\n.token.function-name,\\n.token.constant,\\n.token.symbol,\\n.token.deleted {\\n\\tcolor: #c92c2c;\\n}\\n\\n.token.selector,\\n.token.attr-name,\\n.token.string,\\n.token.char,\\n.token.function,\\n.token.builtin,\\n.token.inserted {\\n\\tcolor: #2f9c0a;\\n}\\n\\n.token.operator,\\n.token.entity,\\n.token.url,\\n.token.variable {\\n\\tcolor: #a67f59;\\n\\tbackground: rgba(255, 255, 255, 0.5);\\n}\\n\\n.token.atrule,\\n.token.attr-value,\\n.token.keyword,\\n.token.class-name {\\n\\tcolor: #1990b8;\\n}\\n\\n.token.regex,\\n.token.important {\\n\\tcolor: #e90;\\n}\\n\\n.language-css .token.string,\\n.style .token.string {\\n\\tcolor: #a67f59;\\n\\tbackground: rgba(255, 255, 255, 0.5);\\n}\\n\\n.token.important {\\n\\tfont-weight: normal;\\n}\\n\\n.token.bold {\\n\\tfont-weight: bold;\\n}\\n.token.italic {\\n\\tfont-style: italic;\\n}\\n\\n.token.entity {\\n\\tcursor: help;\\n}\\n\\n.namespace {\\n\\topacity: .7;\\n}\\n\\n@media screen and (max-width: 767px) {\\n\\tpre[class*=\\\"language-\\\"]:before,\\n\\tpre[class*=\\\"language-\\\"]:after {\\n\\t\\tbottom: 14px;\\n\\t\\tbox-shadow: none;\\n\\t}\\n\\n}\\n\\n/* Plugin styles */\\n.token.tab:not(:empty):before,\\n.token.cr:before,\\n.token.lf:before {\\n\\tcolor: #e0d7d1;\\n}\\n\\n/* Plugin styles: Line Numbers */\\npre[class*=\\\"language-\\\"].line-numbers {\\n\\tpadding-left: 0;\\n}\\n\\npre[class*=\\\"language-\\\"].line-numbers code {\\n\\tpadding-left: 3.8em;\\n}\\n\\npre[class*=\\\"language-\\\"].line-numbers .line-numbers-rows {\\n\\tleft: 0;\\n}\\n\\n/* Plugin styles: Line Highlight */\\npre[class*=\\\"language-\\\"][data-line] {\\n\\tpadding-top: 0;\\n\\tpadding-bottom: 0;\\n\\tpadding-left: 0;\\n}\\npre[data-line] code {\\n\\tposition: relative;\\n\\tpadding-left: 4em;\\n}\\npre .line-highlight {\\n\\tmargin-top: 0;\\n}\\n\"],\"sourceRoot\":\"\"}]);\n\n// exports\n\n\n/***/ }),\n\n/***/ 21:\n/***/ (function(module, exports, __webpack_require__) {\n\n// style-loader: Adds some css to the DOM by adding a <style> tag\n\n// load the styles\nvar content = __webpack_require__(20);\nif(typeof content === 'string') content = [[module.i, content, '']];\nif(content.locals) module.exports = content.locals;\n// add the styles to the DOM\nvar update = __webpack_require__(14)(\"1fcd7648\", content, true);\n\n/***/ }),\n\n/***/ 22:\n/***/ (function(module, exports, __webpack_require__) {\n\n/* WEBPACK VAR INJECTION */(function(global) {/**\n * marked - a markdown parser\n * Copyright (c) 2011-2014, Christopher Jeffrey. (MIT Licensed)\n * https://github.com/chjj/marked\n */\n\n;(function() {\n\n/**\n * Block-Level Grammar\n */\n\nvar block = {\n  newline: /^\\n+/,\n  code: /^( {4}[^\\n]+\\n*)+/,\n  fences: noop,\n  hr: /^( *[-*_]){3,} *(?:\\n+|$)/,\n  heading: /^ *(#{1,6}) *([^\\n]+?) *#* *(?:\\n+|$)/,\n  nptable: noop,\n  lheading: /^([^\\n]+)\\n *(=|-){2,} *(?:\\n+|$)/,\n  blockquote: /^( *>[^\\n]+(\\n(?!def)[^\\n]+)*\\n*)+/,\n  list: /^( *)(bull) [\\s\\S]+?(?:hr|def|\\n{2,}(?! )(?!\\1bull )\\n*|\\s*$)/,\n  html: /^ *(?:comment *(?:\\n|\\s*$)|closed *(?:\\n{2,}|\\s*$)|closing *(?:\\n{2,}|\\s*$))/,\n  def: /^ *\\[([^\\]]+)\\]: *<?([^\\s>]+)>?(?: +[\"(]([^\\n]+)[\")])? *(?:\\n+|$)/,\n  table: noop,\n  paragraph: /^((?:[^\\n]+\\n?(?!hr|heading|lheading|blockquote|tag|def))+)\\n*/,\n  text: /^[^\\n]+/\n};\n\nblock.bullet = /(?:[*+-]|\\d+\\.)/;\nblock.item = /^( *)(bull) [^\\n]*(?:\\n(?!\\1bull )[^\\n]*)*/;\nblock.item = replace(block.item, 'gm')\n  (/bull/g, block.bullet)\n  ();\n\nblock.list = replace(block.list)\n  (/bull/g, block.bullet)\n  ('hr', '\\\\n+(?=\\\\1?(?:[-*_] *){3,}(?:\\\\n+|$))')\n  ('def', '\\\\n+(?=' + block.def.source + ')')\n  ();\n\nblock.blockquote = replace(block.blockquote)\n  ('def', block.def)\n  ();\n\nblock._tag = '(?!(?:'\n  + 'a|em|strong|small|s|cite|q|dfn|abbr|data|time|code'\n  + '|var|samp|kbd|sub|sup|i|b|u|mark|ruby|rt|rp|bdi|bdo'\n  + '|span|br|wbr|ins|del|img)\\\\b)\\\\w+(?!:/|[^\\\\w\\\\s@]*@)\\\\b';\n\nblock.html = replace(block.html)\n  ('comment', /<!--[\\s\\S]*?-->/)\n  ('closed', /<(tag)[\\s\\S]+?<\\/\\1>/)\n  ('closing', /<tag(?:\"[^\"]*\"|'[^']*'|[^'\">])*?>/)\n  (/tag/g, block._tag)\n  ();\n\nblock.paragraph = replace(block.paragraph)\n  ('hr', block.hr)\n  ('heading', block.heading)\n  ('lheading', block.lheading)\n  ('blockquote', block.blockquote)\n  ('tag', '<' + block._tag)\n  ('def', block.def)\n  ();\n\n/**\n * Normal Block Grammar\n */\n\nblock.normal = merge({}, block);\n\n/**\n * GFM Block Grammar\n */\n\nblock.gfm = merge({}, block.normal, {\n  fences: /^ *(`{3,}|~{3,})[ \\.]*(\\S+)? *\\n([\\s\\S]*?)\\s*\\1 *(?:\\n+|$)/,\n  paragraph: /^/,\n  heading: /^ *(#{1,6}) +([^\\n]+?) *#* *(?:\\n+|$)/\n});\n\nblock.gfm.paragraph = replace(block.paragraph)\n  ('(?!', '(?!'\n    + block.gfm.fences.source.replace('\\\\1', '\\\\2') + '|'\n    + block.list.source.replace('\\\\1', '\\\\3') + '|')\n  ();\n\n/**\n * GFM + Tables Block Grammar\n */\n\nblock.tables = merge({}, block.gfm, {\n  nptable: /^ *(\\S.*\\|.*)\\n *([-:]+ *\\|[-| :]*)\\n((?:.*\\|.*(?:\\n|$))*)\\n*/,\n  table: /^ *\\|(.+)\\n *\\|( *[-:]+[-| :]*)\\n((?: *\\|.*(?:\\n|$))*)\\n*/\n});\n\n/**\n * Block Lexer\n */\n\nfunction Lexer(options) {\n  this.tokens = [];\n  this.tokens.links = {};\n  this.options = options || marked.defaults;\n  this.rules = block.normal;\n\n  if (this.options.gfm) {\n    if (this.options.tables) {\n      this.rules = block.tables;\n    } else {\n      this.rules = block.gfm;\n    }\n  }\n}\n\n/**\n * Expose Block Rules\n */\n\nLexer.rules = block;\n\n/**\n * Static Lex Method\n */\n\nLexer.lex = function(src, options) {\n  var lexer = new Lexer(options);\n  return lexer.lex(src);\n};\n\n/**\n * Preprocessing\n */\n\nLexer.prototype.lex = function(src) {\n  src = src\n    .replace(/\\r\\n|\\r/g, '\\n')\n    .replace(/\\t/g, '    ')\n    .replace(/\\u00a0/g, ' ')\n    .replace(/\\u2424/g, '\\n');\n\n  return this.token(src, true);\n};\n\n/**\n * Lexing\n */\n\nLexer.prototype.token = function(src, top, bq) {\n  var src = src.replace(/^ +$/gm, '')\n    , next\n    , loose\n    , cap\n    , bull\n    , b\n    , item\n    , space\n    , i\n    , l;\n\n  while (src) {\n    // newline\n    if (cap = this.rules.newline.exec(src)) {\n      src = src.substring(cap[0].length);\n      if (cap[0].length > 1) {\n        this.tokens.push({\n          type: 'space'\n        });\n      }\n    }\n\n    // code\n    if (cap = this.rules.code.exec(src)) {\n      src = src.substring(cap[0].length);\n      cap = cap[0].replace(/^ {4}/gm, '');\n      this.tokens.push({\n        type: 'code',\n        text: !this.options.pedantic\n          ? cap.replace(/\\n+$/, '')\n          : cap\n      });\n      continue;\n    }\n\n    // fences (gfm)\n    if (cap = this.rules.fences.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'code',\n        lang: cap[2],\n        text: cap[3] || ''\n      });\n      continue;\n    }\n\n    // heading\n    if (cap = this.rules.heading.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'heading',\n        depth: cap[1].length,\n        text: cap[2]\n      });\n      continue;\n    }\n\n    // table no leading pipe (gfm)\n    if (top && (cap = this.rules.nptable.exec(src))) {\n      src = src.substring(cap[0].length);\n\n      item = {\n        type: 'table',\n        header: cap[1].replace(/^ *| *\\| *$/g, '').split(/ *\\| */),\n        align: cap[2].replace(/^ *|\\| *$/g, '').split(/ *\\| */),\n        cells: cap[3].replace(/\\n$/, '').split('\\n')\n      };\n\n      for (i = 0; i < item.align.length; i++) {\n        if (/^ *-+: *$/.test(item.align[i])) {\n          item.align[i] = 'right';\n        } else if (/^ *:-+: *$/.test(item.align[i])) {\n          item.align[i] = 'center';\n        } else if (/^ *:-+ *$/.test(item.align[i])) {\n          item.align[i] = 'left';\n        } else {\n          item.align[i] = null;\n        }\n      }\n\n      for (i = 0; i < item.cells.length; i++) {\n        item.cells[i] = item.cells[i].split(/ *\\| */);\n      }\n\n      this.tokens.push(item);\n\n      continue;\n    }\n\n    // lheading\n    if (cap = this.rules.lheading.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'heading',\n        depth: cap[2] === '=' ? 1 : 2,\n        text: cap[1]\n      });\n      continue;\n    }\n\n    // hr\n    if (cap = this.rules.hr.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'hr'\n      });\n      continue;\n    }\n\n    // blockquote\n    if (cap = this.rules.blockquote.exec(src)) {\n      src = src.substring(cap[0].length);\n\n      this.tokens.push({\n        type: 'blockquote_start'\n      });\n\n      cap = cap[0].replace(/^ *> ?/gm, '');\n\n      // Pass `top` to keep the current\n      // \"toplevel\" state. This is exactly\n      // how markdown.pl works.\n      this.token(cap, top, true);\n\n      this.tokens.push({\n        type: 'blockquote_end'\n      });\n\n      continue;\n    }\n\n    // list\n    if (cap = this.rules.list.exec(src)) {\n      src = src.substring(cap[0].length);\n      bull = cap[2];\n\n      this.tokens.push({\n        type: 'list_start',\n        ordered: bull.length > 1\n      });\n\n      // Get each top-level item.\n      cap = cap[0].match(this.rules.item);\n\n      next = false;\n      l = cap.length;\n      i = 0;\n\n      for (; i < l; i++) {\n        item = cap[i];\n\n        // Remove the list item's bullet\n        // so it is seen as the next token.\n        space = item.length;\n        item = item.replace(/^ *([*+-]|\\d+\\.) +/, '');\n\n        // Outdent whatever the\n        // list item contains. Hacky.\n        if (~item.indexOf('\\n ')) {\n          space -= item.length;\n          item = !this.options.pedantic\n            ? item.replace(new RegExp('^ {1,' + space + '}', 'gm'), '')\n            : item.replace(/^ {1,4}/gm, '');\n        }\n\n        // Determine whether the next list item belongs here.\n        // Backpedal if it does not belong in this list.\n        if (this.options.smartLists && i !== l - 1) {\n          b = block.bullet.exec(cap[i + 1])[0];\n          if (bull !== b && !(bull.length > 1 && b.length > 1)) {\n            src = cap.slice(i + 1).join('\\n') + src;\n            i = l - 1;\n          }\n        }\n\n        // Determine whether item is loose or not.\n        // Use: /(^|\\n)(?! )[^\\n]+\\n\\n(?!\\s*$)/\n        // for discount behavior.\n        loose = next || /\\n\\n(?!\\s*$)/.test(item);\n        if (i !== l - 1) {\n          next = item.charAt(item.length - 1) === '\\n';\n          if (!loose) loose = next;\n        }\n\n        this.tokens.push({\n          type: loose\n            ? 'loose_item_start'\n            : 'list_item_start'\n        });\n\n        // Recurse.\n        this.token(item, false, bq);\n\n        this.tokens.push({\n          type: 'list_item_end'\n        });\n      }\n\n      this.tokens.push({\n        type: 'list_end'\n      });\n\n      continue;\n    }\n\n    // html\n    if (cap = this.rules.html.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: this.options.sanitize\n          ? 'paragraph'\n          : 'html',\n        pre: !this.options.sanitizer\n          && (cap[1] === 'pre' || cap[1] === 'script' || cap[1] === 'style'),\n        text: cap[0]\n      });\n      continue;\n    }\n\n    // def\n    if ((!bq && top) && (cap = this.rules.def.exec(src))) {\n      src = src.substring(cap[0].length);\n      this.tokens.links[cap[1].toLowerCase()] = {\n        href: cap[2],\n        title: cap[3]\n      };\n      continue;\n    }\n\n    // table (gfm)\n    if (top && (cap = this.rules.table.exec(src))) {\n      src = src.substring(cap[0].length);\n\n      item = {\n        type: 'table',\n        header: cap[1].replace(/^ *| *\\| *$/g, '').split(/ *\\| */),\n        align: cap[2].replace(/^ *|\\| *$/g, '').split(/ *\\| */),\n        cells: cap[3].replace(/(?: *\\| *)?\\n$/, '').split('\\n')\n      };\n\n      for (i = 0; i < item.align.length; i++) {\n        if (/^ *-+: *$/.test(item.align[i])) {\n          item.align[i] = 'right';\n        } else if (/^ *:-+: *$/.test(item.align[i])) {\n          item.align[i] = 'center';\n        } else if (/^ *:-+ *$/.test(item.align[i])) {\n          item.align[i] = 'left';\n        } else {\n          item.align[i] = null;\n        }\n      }\n\n      for (i = 0; i < item.cells.length; i++) {\n        item.cells[i] = item.cells[i]\n          .replace(/^ *\\| *| *\\| *$/g, '')\n          .split(/ *\\| */);\n      }\n\n      this.tokens.push(item);\n\n      continue;\n    }\n\n    // top-level paragraph\n    if (top && (cap = this.rules.paragraph.exec(src))) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'paragraph',\n        text: cap[1].charAt(cap[1].length - 1) === '\\n'\n          ? cap[1].slice(0, -1)\n          : cap[1]\n      });\n      continue;\n    }\n\n    // text\n    if (cap = this.rules.text.exec(src)) {\n      // Top-level should never reach here.\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'text',\n        text: cap[0]\n      });\n      continue;\n    }\n\n    if (src) {\n      throw new\n        Error('Infinite loop on byte: ' + src.charCodeAt(0));\n    }\n  }\n\n  return this.tokens;\n};\n\n/**\n * Inline-Level Grammar\n */\n\nvar inline = {\n  escape: /^\\\\([\\\\`*{}\\[\\]()#+\\-.!_>])/,\n  autolink: /^<([^ >]+(@|:\\/)[^ >]+)>/,\n  url: noop,\n  tag: /^<!--[\\s\\S]*?-->|^<\\/?\\w+(?:\"[^\"]*\"|'[^']*'|[^'\">])*?>/,\n  link: /^!?\\[(inside)\\]\\(href\\)/,\n  reflink: /^!?\\[(inside)\\]\\s*\\[([^\\]]*)\\]/,\n  nolink: /^!?\\[((?:\\[[^\\]]*\\]|[^\\[\\]])*)\\]/,\n  strong: /^__([\\s\\S]+?)__(?!_)|^\\*\\*([\\s\\S]+?)\\*\\*(?!\\*)/,\n  em: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n  code: /^(`+)\\s*([\\s\\S]*?[^`])\\s*\\1(?!`)/,\n  br: /^ {2,}\\n(?!\\s*$)/,\n  del: noop,\n  text: /^[\\s\\S]+?(?=[\\\\<!\\[_*`]| {2,}\\n|$)/\n};\n\ninline._inside = /(?:\\[[^\\]]*\\]|[^\\[\\]]|\\](?=[^\\[]*\\]))*/;\ninline._href = /\\s*<?([\\s\\S]*?)>?(?:\\s+['\"]([\\s\\S]*?)['\"])?\\s*/;\n\ninline.link = replace(inline.link)\n  ('inside', inline._inside)\n  ('href', inline._href)\n  ();\n\ninline.reflink = replace(inline.reflink)\n  ('inside', inline._inside)\n  ();\n\n/**\n * Normal Inline Grammar\n */\n\ninline.normal = merge({}, inline);\n\n/**\n * Pedantic Inline Grammar\n */\n\ninline.pedantic = merge({}, inline.normal, {\n  strong: /^__(?=\\S)([\\s\\S]*?\\S)__(?!_)|^\\*\\*(?=\\S)([\\s\\S]*?\\S)\\*\\*(?!\\*)/,\n  em: /^_(?=\\S)([\\s\\S]*?\\S)_(?!_)|^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/\n});\n\n/**\n * GFM Inline Grammar\n */\n\ninline.gfm = merge({}, inline.normal, {\n  escape: replace(inline.escape)('])', '~|])')(),\n  url: /^(https?:\\/\\/[^\\s<]+[^<.,:;\"')\\]\\s])/,\n  del: /^~~(?=\\S)([\\s\\S]*?\\S)~~/,\n  text: replace(inline.text)\n    (']|', '~]|')\n    ('|', '|https?://|')\n    ()\n});\n\n/**\n * GFM + Line Breaks Inline Grammar\n */\n\ninline.breaks = merge({}, inline.gfm, {\n  br: replace(inline.br)('{2,}', '*')(),\n  text: replace(inline.gfm.text)('{2,}', '*')()\n});\n\n/**\n * Inline Lexer & Compiler\n */\n\nfunction InlineLexer(links, options) {\n  this.options = options || marked.defaults;\n  this.links = links;\n  this.rules = inline.normal;\n  this.renderer = this.options.renderer || new Renderer;\n  this.renderer.options = this.options;\n\n  if (!this.links) {\n    throw new\n      Error('Tokens array requires a `links` property.');\n  }\n\n  if (this.options.gfm) {\n    if (this.options.breaks) {\n      this.rules = inline.breaks;\n    } else {\n      this.rules = inline.gfm;\n    }\n  } else if (this.options.pedantic) {\n    this.rules = inline.pedantic;\n  }\n}\n\n/**\n * Expose Inline Rules\n */\n\nInlineLexer.rules = inline;\n\n/**\n * Static Lexing/Compiling Method\n */\n\nInlineLexer.output = function(src, links, options) {\n  var inline = new InlineLexer(links, options);\n  return inline.output(src);\n};\n\n/**\n * Lexing/Compiling\n */\n\nInlineLexer.prototype.output = function(src) {\n  var out = ''\n    , link\n    , text\n    , href\n    , cap;\n\n  while (src) {\n    // escape\n    if (cap = this.rules.escape.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += cap[1];\n      continue;\n    }\n\n    // autolink\n    if (cap = this.rules.autolink.exec(src)) {\n      src = src.substring(cap[0].length);\n      if (cap[2] === '@') {\n        text = cap[1].charAt(6) === ':'\n          ? this.mangle(cap[1].substring(7))\n          : this.mangle(cap[1]);\n        href = this.mangle('mailto:') + text;\n      } else {\n        text = escape(cap[1]);\n        href = text;\n      }\n      out += this.renderer.link(href, null, text);\n      continue;\n    }\n\n    // url (gfm)\n    if (!this.inLink && (cap = this.rules.url.exec(src))) {\n      src = src.substring(cap[0].length);\n      text = escape(cap[1]);\n      href = text;\n      out += this.renderer.link(href, null, text);\n      continue;\n    }\n\n    // tag\n    if (cap = this.rules.tag.exec(src)) {\n      if (!this.inLink && /^<a /i.test(cap[0])) {\n        this.inLink = true;\n      } else if (this.inLink && /^<\\/a>/i.test(cap[0])) {\n        this.inLink = false;\n      }\n      src = src.substring(cap[0].length);\n      out += this.options.sanitize\n        ? this.options.sanitizer\n          ? this.options.sanitizer(cap[0])\n          : escape(cap[0])\n        : cap[0]\n      continue;\n    }\n\n    // link\n    if (cap = this.rules.link.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.inLink = true;\n      out += this.outputLink(cap, {\n        href: cap[2],\n        title: cap[3]\n      });\n      this.inLink = false;\n      continue;\n    }\n\n    // reflink, nolink\n    if ((cap = this.rules.reflink.exec(src))\n        || (cap = this.rules.nolink.exec(src))) {\n      src = src.substring(cap[0].length);\n      link = (cap[2] || cap[1]).replace(/\\s+/g, ' ');\n      link = this.links[link.toLowerCase()];\n      if (!link || !link.href) {\n        out += cap[0].charAt(0);\n        src = cap[0].substring(1) + src;\n        continue;\n      }\n      this.inLink = true;\n      out += this.outputLink(cap, link);\n      this.inLink = false;\n      continue;\n    }\n\n    // strong\n    if (cap = this.rules.strong.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.strong(this.output(cap[2] || cap[1]));\n      continue;\n    }\n\n    // em\n    if (cap = this.rules.em.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.em(this.output(cap[2] || cap[1]));\n      continue;\n    }\n\n    // code\n    if (cap = this.rules.code.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.codespan(escape(cap[2], true));\n      continue;\n    }\n\n    // br\n    if (cap = this.rules.br.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.br();\n      continue;\n    }\n\n    // del (gfm)\n    if (cap = this.rules.del.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.del(this.output(cap[1]));\n      continue;\n    }\n\n    // text\n    if (cap = this.rules.text.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.text(escape(this.smartypants(cap[0])));\n      continue;\n    }\n\n    if (src) {\n      throw new\n        Error('Infinite loop on byte: ' + src.charCodeAt(0));\n    }\n  }\n\n  return out;\n};\n\n/**\n * Compile Link\n */\n\nInlineLexer.prototype.outputLink = function(cap, link) {\n  var href = escape(link.href)\n    , title = link.title ? escape(link.title) : null;\n\n  return cap[0].charAt(0) !== '!'\n    ? this.renderer.link(href, title, this.output(cap[1]))\n    : this.renderer.image(href, title, escape(cap[1]));\n};\n\n/**\n * Smartypants Transformations\n */\n\nInlineLexer.prototype.smartypants = function(text) {\n  if (!this.options.smartypants) return text;\n  return text\n    // em-dashes\n    .replace(/---/g, '\\u2014')\n    // en-dashes\n    .replace(/--/g, '\\u2013')\n    // opening singles\n    .replace(/(^|[-\\u2014/(\\[{\"\\s])'/g, '$1\\u2018')\n    // closing singles & apostrophes\n    .replace(/'/g, '\\u2019')\n    // opening doubles\n    .replace(/(^|[-\\u2014/(\\[{\\u2018\\s])\"/g, '$1\\u201c')\n    // closing doubles\n    .replace(/\"/g, '\\u201d')\n    // ellipses\n    .replace(/\\.{3}/g, '\\u2026');\n};\n\n/**\n * Mangle Links\n */\n\nInlineLexer.prototype.mangle = function(text) {\n  if (!this.options.mangle) return text;\n  var out = ''\n    , l = text.length\n    , i = 0\n    , ch;\n\n  for (; i < l; i++) {\n    ch = text.charCodeAt(i);\n    if (Math.random() > 0.5) {\n      ch = 'x' + ch.toString(16);\n    }\n    out += '&#' + ch + ';';\n  }\n\n  return out;\n};\n\n/**\n * Renderer\n */\n\nfunction Renderer(options) {\n  this.options = options || {};\n}\n\nRenderer.prototype.code = function(code, lang, escaped) {\n  if (this.options.highlight) {\n    var out = this.options.highlight(code, lang);\n    if (out != null && out !== code) {\n      escaped = true;\n      code = out;\n    }\n  }\n\n  if (!lang) {\n    return '<pre><code>'\n      + (escaped ? code : escape(code, true))\n      + '\\n</code></pre>';\n  }\n\n  return '<pre><code class=\"'\n    + this.options.langPrefix\n    + escape(lang, true)\n    + '\">'\n    + (escaped ? code : escape(code, true))\n    + '\\n</code></pre>\\n';\n};\n\nRenderer.prototype.blockquote = function(quote) {\n  return '<blockquote>\\n' + quote + '</blockquote>\\n';\n};\n\nRenderer.prototype.html = function(html) {\n  return html;\n};\n\nRenderer.prototype.heading = function(text, level, raw) {\n  return '<h'\n    + level\n    + ' id=\"'\n    + this.options.headerPrefix\n    + raw.toLowerCase().replace(/[^\\w]+/g, '-')\n    + '\">'\n    + text\n    + '</h'\n    + level\n    + '>\\n';\n};\n\nRenderer.prototype.hr = function() {\n  return this.options.xhtml ? '<hr/>\\n' : '<hr>\\n';\n};\n\nRenderer.prototype.list = function(body, ordered) {\n  var type = ordered ? 'ol' : 'ul';\n  return '<' + type + '>\\n' + body + '</' + type + '>\\n';\n};\n\nRenderer.prototype.listitem = function(text) {\n  return '<li>' + text + '</li>\\n';\n};\n\nRenderer.prototype.paragraph = function(text) {\n  return '<p>' + text + '</p>\\n';\n};\n\nRenderer.prototype.table = function(header, body) {\n  return '<table>\\n'\n    + '<thead>\\n'\n    + header\n    + '</thead>\\n'\n    + '<tbody>\\n'\n    + body\n    + '</tbody>\\n'\n    + '</table>\\n';\n};\n\nRenderer.prototype.tablerow = function(content) {\n  return '<tr>\\n' + content + '</tr>\\n';\n};\n\nRenderer.prototype.tablecell = function(content, flags) {\n  var type = flags.header ? 'th' : 'td';\n  var tag = flags.align\n    ? '<' + type + ' style=\"text-align:' + flags.align + '\">'\n    : '<' + type + '>';\n  return tag + content + '</' + type + '>\\n';\n};\n\n// span level renderer\nRenderer.prototype.strong = function(text) {\n  return '<strong>' + text + '</strong>';\n};\n\nRenderer.prototype.em = function(text) {\n  return '<em>' + text + '</em>';\n};\n\nRenderer.prototype.codespan = function(text) {\n  return '<code>' + text + '</code>';\n};\n\nRenderer.prototype.br = function() {\n  return this.options.xhtml ? '<br/>' : '<br>';\n};\n\nRenderer.prototype.del = function(text) {\n  return '<del>' + text + '</del>';\n};\n\nRenderer.prototype.link = function(href, title, text) {\n  if (this.options.sanitize) {\n    try {\n      var prot = decodeURIComponent(unescape(href))\n        .replace(/[^\\w:]/g, '')\n        .toLowerCase();\n    } catch (e) {\n      return '';\n    }\n    if (prot.indexOf('javascript:') === 0 || prot.indexOf('vbscript:') === 0) {\n      return '';\n    }\n  }\n  var out = '<a href=\"' + href + '\"';\n  if (title) {\n    out += ' title=\"' + title + '\"';\n  }\n  out += '>' + text + '</a>';\n  return out;\n};\n\nRenderer.prototype.image = function(href, title, text) {\n  var out = '<img src=\"' + href + '\" alt=\"' + text + '\"';\n  if (title) {\n    out += ' title=\"' + title + '\"';\n  }\n  out += this.options.xhtml ? '/>' : '>';\n  return out;\n};\n\nRenderer.prototype.text = function(text) {\n  return text;\n};\n\n/**\n * Parsing & Compiling\n */\n\nfunction Parser(options) {\n  this.tokens = [];\n  this.token = null;\n  this.options = options || marked.defaults;\n  this.options.renderer = this.options.renderer || new Renderer;\n  this.renderer = this.options.renderer;\n  this.renderer.options = this.options;\n}\n\n/**\n * Static Parse Method\n */\n\nParser.parse = function(src, options, renderer) {\n  var parser = new Parser(options, renderer);\n  return parser.parse(src);\n};\n\n/**\n * Parse Loop\n */\n\nParser.prototype.parse = function(src) {\n  this.inline = new InlineLexer(src.links, this.options, this.renderer);\n  this.tokens = src.reverse();\n\n  var out = '';\n  while (this.next()) {\n    out += this.tok();\n  }\n\n  return out;\n};\n\n/**\n * Next Token\n */\n\nParser.prototype.next = function() {\n  return this.token = this.tokens.pop();\n};\n\n/**\n * Preview Next Token\n */\n\nParser.prototype.peek = function() {\n  return this.tokens[this.tokens.length - 1] || 0;\n};\n\n/**\n * Parse Text Tokens\n */\n\nParser.prototype.parseText = function() {\n  var body = this.token.text;\n\n  while (this.peek().type === 'text') {\n    body += '\\n' + this.next().text;\n  }\n\n  return this.inline.output(body);\n};\n\n/**\n * Parse Current Token\n */\n\nParser.prototype.tok = function() {\n  switch (this.token.type) {\n    case 'space': {\n      return '';\n    }\n    case 'hr': {\n      return this.renderer.hr();\n    }\n    case 'heading': {\n      return this.renderer.heading(\n        this.inline.output(this.token.text),\n        this.token.depth,\n        this.token.text);\n    }\n    case 'code': {\n      return this.renderer.code(this.token.text,\n        this.token.lang,\n        this.token.escaped);\n    }\n    case 'table': {\n      var header = ''\n        , body = ''\n        , i\n        , row\n        , cell\n        , flags\n        , j;\n\n      // header\n      cell = '';\n      for (i = 0; i < this.token.header.length; i++) {\n        flags = { header: true, align: this.token.align[i] };\n        cell += this.renderer.tablecell(\n          this.inline.output(this.token.header[i]),\n          { header: true, align: this.token.align[i] }\n        );\n      }\n      header += this.renderer.tablerow(cell);\n\n      for (i = 0; i < this.token.cells.length; i++) {\n        row = this.token.cells[i];\n\n        cell = '';\n        for (j = 0; j < row.length; j++) {\n          cell += this.renderer.tablecell(\n            this.inline.output(row[j]),\n            { header: false, align: this.token.align[j] }\n          );\n        }\n\n        body += this.renderer.tablerow(cell);\n      }\n      return this.renderer.table(header, body);\n    }\n    case 'blockquote_start': {\n      var body = '';\n\n      while (this.next().type !== 'blockquote_end') {\n        body += this.tok();\n      }\n\n      return this.renderer.blockquote(body);\n    }\n    case 'list_start': {\n      var body = ''\n        , ordered = this.token.ordered;\n\n      while (this.next().type !== 'list_end') {\n        body += this.tok();\n      }\n\n      return this.renderer.list(body, ordered);\n    }\n    case 'list_item_start': {\n      var body = '';\n\n      while (this.next().type !== 'list_item_end') {\n        body += this.token.type === 'text'\n          ? this.parseText()\n          : this.tok();\n      }\n\n      return this.renderer.listitem(body);\n    }\n    case 'loose_item_start': {\n      var body = '';\n\n      while (this.next().type !== 'list_item_end') {\n        body += this.tok();\n      }\n\n      return this.renderer.listitem(body);\n    }\n    case 'html': {\n      var html = !this.token.pre && !this.options.pedantic\n        ? this.inline.output(this.token.text)\n        : this.token.text;\n      return this.renderer.html(html);\n    }\n    case 'paragraph': {\n      return this.renderer.paragraph(this.inline.output(this.token.text));\n    }\n    case 'text': {\n      return this.renderer.paragraph(this.parseText());\n    }\n  }\n};\n\n/**\n * Helpers\n */\n\nfunction escape(html, encode) {\n  return html\n    .replace(!encode ? /&(?!#?\\w+;)/g : /&/g, '&amp;')\n    .replace(/</g, '&lt;')\n    .replace(/>/g, '&gt;')\n    .replace(/\"/g, '&quot;')\n    .replace(/'/g, '&#39;');\n}\n\nfunction unescape(html) {\n\t// explicitly match decimal, hex, and named HTML entities \n  return html.replace(/&(#(?:\\d+)|(?:#x[0-9A-Fa-f]+)|(?:\\w+));?/g, function(_, n) {\n    n = n.toLowerCase();\n    if (n === 'colon') return ':';\n    if (n.charAt(0) === '#') {\n      return n.charAt(1) === 'x'\n        ? String.fromCharCode(parseInt(n.substring(2), 16))\n        : String.fromCharCode(+n.substring(1));\n    }\n    return '';\n  });\n}\n\nfunction replace(regex, opt) {\n  regex = regex.source;\n  opt = opt || '';\n  return function self(name, val) {\n    if (!name) return new RegExp(regex, opt);\n    val = val.source || val;\n    val = val.replace(/(^|[^\\[])\\^/g, '$1');\n    regex = regex.replace(name, val);\n    return self;\n  };\n}\n\nfunction noop() {}\nnoop.exec = noop;\n\nfunction merge(obj) {\n  var i = 1\n    , target\n    , key;\n\n  for (; i < arguments.length; i++) {\n    target = arguments[i];\n    for (key in target) {\n      if (Object.prototype.hasOwnProperty.call(target, key)) {\n        obj[key] = target[key];\n      }\n    }\n  }\n\n  return obj;\n}\n\n\n/**\n * Marked\n */\n\nfunction marked(src, opt, callback) {\n  if (callback || typeof opt === 'function') {\n    if (!callback) {\n      callback = opt;\n      opt = null;\n    }\n\n    opt = merge({}, marked.defaults, opt || {});\n\n    var highlight = opt.highlight\n      , tokens\n      , pending\n      , i = 0;\n\n    try {\n      tokens = Lexer.lex(src, opt)\n    } catch (e) {\n      return callback(e);\n    }\n\n    pending = tokens.length;\n\n    var done = function(err) {\n      if (err) {\n        opt.highlight = highlight;\n        return callback(err);\n      }\n\n      var out;\n\n      try {\n        out = Parser.parse(tokens, opt);\n      } catch (e) {\n        err = e;\n      }\n\n      opt.highlight = highlight;\n\n      return err\n        ? callback(err)\n        : callback(null, out);\n    };\n\n    if (!highlight || highlight.length < 3) {\n      return done();\n    }\n\n    delete opt.highlight;\n\n    if (!pending) return done();\n\n    for (; i < tokens.length; i++) {\n      (function(token) {\n        if (token.type !== 'code') {\n          return --pending || done();\n        }\n        return highlight(token.text, token.lang, function(err, code) {\n          if (err) return done(err);\n          if (code == null || code === token.text) {\n            return --pending || done();\n          }\n          token.text = code;\n          token.escaped = true;\n          --pending || done();\n        });\n      })(tokens[i]);\n    }\n\n    return;\n  }\n  try {\n    if (opt) opt = merge({}, marked.defaults, opt);\n    return Parser.parse(Lexer.lex(src, opt), opt);\n  } catch (e) {\n    e.message += '\\nPlease report this to https://github.com/chjj/marked.';\n    if ((opt || marked.defaults).silent) {\n      return '<p>An error occured:</p><pre>'\n        + escape(e.message + '', true)\n        + '</pre>';\n    }\n    throw e;\n  }\n}\n\n/**\n * Options\n */\n\nmarked.options =\nmarked.setOptions = function(opt) {\n  merge(marked.defaults, opt);\n  return marked;\n};\n\nmarked.defaults = {\n  gfm: true,\n  tables: true,\n  breaks: false,\n  pedantic: false,\n  sanitize: false,\n  sanitizer: null,\n  mangle: true,\n  smartLists: false,\n  silent: false,\n  highlight: null,\n  langPrefix: 'lang-',\n  smartypants: false,\n  headerPrefix: '',\n  renderer: new Renderer,\n  xhtml: false\n};\n\n/**\n * Expose\n */\n\nmarked.Parser = Parser;\nmarked.parser = Parser.parse;\n\nmarked.Renderer = Renderer;\n\nmarked.Lexer = Lexer;\nmarked.lexer = Lexer.lex;\n\nmarked.InlineLexer = InlineLexer;\nmarked.inlineLexer = InlineLexer.output;\n\nmarked.parse = marked;\n\nif (true) {\n  module.exports = marked;\n} else if (typeof define === 'function' && define.amd) {\n  define(function() { return marked; });\n} else {\n  this.marked = marked;\n}\n\n}).call(function() {\n  return this || (typeof window !== 'undefined' ? window : global);\n}());\n\n/* WEBPACK VAR INJECTION */}.call(exports, __webpack_require__(5)))\n\n/***/ }),\n\n/***/ 23:\n/***/ (function(module, exports) {\n\nPrism.languages.python= {\n\t'triple-quoted-string': {\n\t\tpattern: /\"\"\"[\\s\\S]+?\"\"\"|'''[\\s\\S]+?'''/,\n\t\talias: 'string'\n\t},\n\t'comment': {\n\t\tpattern: /(^|[^\\\\])#.*/,\n\t\tlookbehind: true\n\t},\n\t'string': {\n\t\tpattern: /(\"|')(?:\\\\\\\\|\\\\?[^\\\\\\r\\n])*?\\1/,\n\t\tgreedy: true\n\t},\n\t'function' : {\n\t\tpattern: /((?:^|\\s)def[ \\t]+)[a-zA-Z_][a-zA-Z0-9_]*(?=\\()/g,\n\t\tlookbehind: true\n\t},\n\t'class-name': {\n\t\tpattern: /(\\bclass\\s+)[a-z0-9_]+/i,\n\t\tlookbehind: true\n\t},\n\t'keyword' : /\\b(?:as|assert|async|await|break|class|continue|def|del|elif|else|except|exec|finally|for|from|global|if|import|in|is|lambda|pass|print|raise|return|try|while|with|yield)\\b/,\n\t'boolean' : /\\b(?:True|False)\\b/,\n\t'number' : /\\b-?(?:0[bo])?(?:(?:\\d|0x[\\da-f])[\\da-f]*\\.?\\d*|\\.\\d+)(?:e[+-]?\\d+)?j?\\b/i,\n\t'operator' : /[-+%=]=?|!=|\\*\\*?=?|\\/\\/?=?|<[<=>]?|>[=>]?|[&|^~]|\\b(?:or|and|not)\\b/,\n\t'punctuation' : /[{}[\\];(),.:]/\n};\n\n\n/***/ }),\n\n/***/ 40:\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\nObject.defineProperty(__webpack_exports__, \"__esModule\", { value: true });\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0_marked__ = __webpack_require__(22);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0_marked___default = __webpack_require__.n(__WEBPACK_IMPORTED_MODULE_0_marked__);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_1_prismjs__ = __webpack_require__(19);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_1_prismjs___default = __webpack_require__.n(__WEBPACK_IMPORTED_MODULE_1_prismjs__);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_2_prismjs_themes_prism_coy_css__ = __webpack_require__(21);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_2_prismjs_themes_prism_coy_css___default = __webpack_require__.n(__WEBPACK_IMPORTED_MODULE_2_prismjs_themes_prism_coy_css__);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_3_prismjs_prism_js__ = __webpack_require__(19);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_3_prismjs_prism_js___default = __webpack_require__.n(__WEBPACK_IMPORTED_MODULE_3_prismjs_prism_js__);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_4_prismjs_components_prism_python_js__ = __webpack_require__(23);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_4_prismjs_components_prism_python_js___default = __webpack_require__.n(__WEBPACK_IMPORTED_MODULE_4_prismjs_components_prism_python_js__);\n\n\n\n\n\n\n\n\nvar article = '\\n# LSTM-RNN For Sentiment Analysis\\nAuthor: Trilby Hren and Costa Huang\\n\\nSource code vailable at https://github.com/costahuang/Sentiment-Analysis-LSTM\\n\\n**Our project has three main parts:**\\n1. Reproduce and understand Keras(a python machine learning library) official demo code on LSTM-RNN for sentiment analysis \\n * [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py) Main program that rains a LSTM on the IMDB sentiment classification task [1]\\n * [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py) Preprocessing script of IMDB movie review dataset [2]\\n* Improve the program\\'s accuracy by exploring different techniques:\\n * Preprocessing techniques\\n * Activation functions\\n * Optimizer choices\\n* Futher application\\n * Visualization of sentiment analysis\\n * Applied our result to Amazon Review Datas\\n\\n\\n\\n## Baseline algorithm (Demo code from Keras)\\nFirstly we execute [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py)[1] and yields following results where a test accuracy of 82.35% is acquired.\\n\\n\\n```python\\n\\'\\'\\'Trains a LSTM on the IMDB sentiment classification task.\\nThe dataset is actually too small for LSTM to be of any advantage\\ncompared to simpler, much faster methods such as TF-IDF + LogReg.\\nNotes:\\n- RNNs are tricky. Choice of batch size is important,\\nchoice of loss and optimizer is critical, etc.\\nSome configurations won\\'t converge.\\n- LSTM loss decrease patterns during training can be quite different\\nfrom what you see with CNNs/MLPs/etc.\\n\\'\\'\\'\\nfrom __future__ import print_function\\nimport numpy as np\\nnp.random.seed(1337)  # for reproducibility\\n\\nfrom keras.preprocessing import sequence\\nfrom keras.utils import np_utils\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Dropout, Activation, Embedding\\nfrom keras.layers import LSTM, SimpleRNN, GRU\\nfrom keras.datasets import imdb\\n\\nmax_features = 20000\\nmaxlen = 80  # cut texts after this number of words (among top max_features most common words)\\nbatch_size = 32\\n\\nprint(\\'Loading data...\\')\\n(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\\nprint(len(X_train), \\'train sequences\\')\\nprint(len(X_test), \\'test sequences\\')\\n\\nprint(\\'Pad sequences (samples x time)\\')\\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\\nprint(\\'X_train shape:\\', X_train.shape)\\nprint(\\'X_test shape:\\', X_test.shape)\\n\\nprint(\\'Build model...\\')\\nmodel = Sequential()\\nmodel.add(Embedding(max_features, 128, dropout=0.2))\\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\\nmodel.add(Dense(1))\\nmodel.add(Activation(\\'sigmoid\\'))\\n\\n# try using different optimizers and different optimizer configs\\nmodel.compile(loss=\\'binary_crossentropy\\',\\n              optimizer=\\'adam\\',\\n              metrics=[\\'accuracy\\'])\\n\\nprint(\\'Train...\\')\\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\\n          validation_data=(X_test, y_test))\\nscore, acc = model.evaluate(X_test, y_test,\\n                            batch_size=batch_size)\\nprint(\\'Test score:\\', score)\\nprint(\\'Test accuracy:\\', acc)\\n```\\n\\n    Using Theano backend.\\n    \\n\\n    Loading data...\\n    25000 train sequences\\n    25000 test sequences\\n    Pad sequences (samples x time)\\n    X_train shape: (25000, 80)\\n    X_test shape: (25000, 80)\\n    Build model...\\n    Train...\\n    Train on 25000 samples, validate on 25000 samples\\n    Epoch 1/10\\n    25000/25000 [==============================] - 138s - loss: 0.5333 - acc: 0.7316 - val_loss: 0.4631 - val_acc: 0.7898\\n    Epoch 2/10\\n    25000/25000 [==============================] - 144s - loss: 0.3812 - acc: 0.8358 - val_loss: 0.3776 - val_acc: 0.8370\\n    Epoch 3/10\\n    25000/25000 [==============================] - 147s - loss: 0.3077 - acc: 0.8737 - val_loss: 0.3687 - val_acc: 0.8360\\n    Epoch 4/10\\n    25000/25000 [==============================] - 146s - loss: 0.2497 - acc: 0.8979 - val_loss: 0.4161 - val_acc: 0.8322\\n    Epoch 5/10\\n    25000/25000 [==============================] - 149s - loss: 0.2100 - acc: 0.9180 - val_loss: 0.4277 - val_acc: 0.8330\\n    Epoch 6/10\\n    25000/25000 [==============================] - 151s - loss: 0.1801 - acc: 0.9298 - val_loss: 0.4318 - val_acc: 0.8362\\n    Epoch 7/10\\n    25000/25000 [==============================] - 148s - loss: 0.1503 - acc: 0.9423 - val_loss: 0.4964 - val_acc: 0.8280\\n    Epoch 8/10\\n    25000/25000 [==============================] - 150s - loss: 0.1328 - acc: 0.9499 - val_loss: 0.5169 - val_acc: 0.8266\\n    Epoch 9/10\\n    25000/25000 [==============================] - 146s - loss: 0.1163 - acc: 0.9548 - val_loss: 0.5354 - val_acc: 0.8227\\n    Epoch 10/10\\n    25000/25000 [==============================] - 153s - loss: 0.1047 - acc: 0.9618 - val_loss: 0.5375 - val_acc: 0.8236\\n    25000/25000 [==============================] - 34s    \\n    Test score: 0.537465095153\\n    Test accuracy: 0.82356\\n    \\n\\n## Understanding the source code\\n** 1. Preprocessing**\\n * Original Dataset is provided by a [Stanford research group](http://ai.stanford.edu/~amaas//data/sentiment/) [3], which provided a set of 25,000 preclassified IMDB movie reviews for training, and 25,000 for testing.\\n * [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py)[2] performs the following tasks:\\n     * Download tokenized the IMDB movie review pre-supplied by Keras, where each review is encoded as word indices. An example training review would look like [3, 213, 43, 324, 12, 4, ...] where 3 represents the third most popular word in the dataset.\\n     * Shuffle the training and testing dataset.\\n     * Include only *nb_words* most frequent word for ouput\\n     * Add a [1] to each review to mark the start of the sequence\\n     * Add a [2] to replace the words that are cut off because of *nb_words*\\n     * Index actual words with *\"index_from = 3*\" and higher. (We think this part is **very problematic**, which is mentioned in the next code cell)\\n\\n\\n```python\\n# From imdb.py, it has a preprocessing code like this, where they use index_from = 3 as the default parameter\\n# if start_char is not None:\\n#     X = [[start_char] + [w + index_from for w in x] for x in X]\\nindex_from = 3\\nexample_review = [6, 499, 6, 99, 221, 9, 22, 501, 2, 3]\\nexample_review = [[1] + [w + index_from for w in example_review]]\\nprint(example_review)\\n```\\n\\n    [[1, 9, 502, 9, 102, 224, 12, 25, 504, 5, 6]]\\n    \\n\\n> As we demoed, this particular parameter \"shifted\" each word index of the review up by *\"index_from = 3\"*, which is totally nonsense in our opinion because it almost changes everything about that review. We also couldn\\'t find any documentation or papers to support such parameter. As a result, in our experiment, we just simply set *\"index_from = 0\"*.\\n\\n**2. Model Building**\\n * [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py) builds the model:\\n     * Limit top most frequent words: Each review only has *\"max_features = nb_words = 20000\"* most frequent word index\\n     * Pad Sequence: Each review is padded with a maximum length of *\"maxlen = 80\"*\\n     * Embedding layer: Map each word index into *\"p = 128\"* dimentional vector space\\n       * The original idea came from this landmark paper [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)[6] where they show a good vector representation for text sequences is achievable.\\n     * LSTM layer\\n        * Hochreiter & Schmidhuber [7] first introduced this special kind of RNN to solve the long-term dependency problem. Unlike traditional RNN, LSTM has a memory cell that it uses to determine what information is important by using \"forget gate layer\" and \"input gate layer\". [8]\\n     * Dense(1): fully connect LSTM to create one demensional output\\n     * Sigmoid\\n     * Adam optimizer for training.\\n\\n## Improving the algorithm\\n**_Activation functions_** by [Keras.layers.core.Activation](https://keras.io/activations/)\\n1. Sigmoid  \\n   1. \\u03C3(x) = 1/(1+e\\u2212x)  \\n   2. The sigmoid function is a squashing function. We discovered that this activation function jumps up in test accuracy and converges to about 82.6% after 8 generations.     \\n   \\n2. ReLU  \\n  1. f(x) = max(0,x)  \\n  1. For the ReLU activation function our results show dramatic improvement in later generations. It jumps from very low results to much higher accuracy in both testing and training. The final accuracy is not as good as the results from the sigmoid function, it converges to 73.8% in the last generation.      \\n  \\n3. Tanh  \\n  1. tanh(x) = 2\\u03C3(2x)\\u22121tanh(x)=2\\u03C3(2x)\\u22121  \\n  1. The tanh function was slightly better than ReLU with a final accuracy of 77.6%. During our 8 generations, this function seemed to decrease in overall test accuracy perhaps due to overfitting.      \\n  \\n4. Hard sigmoid    \\n  1. The hard sigmoid function performed only slightly better than the sigmoid with a final test accuracy of 83%. From our understanding, it should have converged faster due to increased speed since the precision isn\\'t as important. Perhaps the lower test accuracy is because of the loss in precision rather than the speed.      \\n\\n[3]\\nCheck out our [Activation functions data](https://docs.google.com/document/d/1uQ49jhYYOmwzSPxYYamTXYCq-07Dhhu75eleew3LBPU/edit?usp=sharing)\\n\\n\\n**_Optimizer choices_** by [keras.optimizers](https://keras.io/optimizers/)\\n1. SGD (Stochastic Gradient Decent)\\n    1. This type of optimizer uses only a few training examples rather than the entire training set. It also can lead to fast convergence since it doesn\\'t have to run back propagation over the full training set. \\n    1. Sometimes a meaningful order leads to a bias for the gradient and causes poor convergence. Since `our data was randomly shuffled before each training`, this shouldn\\'t have been an issue.\\n        2. _Adam_\\n            1. Adaptive Moment Estimation\\n            1. This optimizer uses momentum and past gradients to help with learning by combining the two advantages of RMSprop and AdaGrad. \\n            1. While we expected this optimizer to perfom the best since it combines the better parts of RMSprop and AdaGrad, it was actually one of our lowest perfoming tests. It showed very slow convergence to 82.6% during our 8 generations. Perhaps it would have converged to a higher accuracy than the others, given a higher number of generations. \\n        3. _RMSprop_\\n            1. Root Mean Square Propagation\\n            1. This method adapts the learning rate for each parameter by using tha magnitude of recent gradients to normalize the gradients.\\n            1. From out tests, RSMprop performed the best with a final testing accuracy converging to 84.8%. This optimizer had steady performance through the 8 generations with the lowest accuracy being 82.8%.\\n        4. _AdaGrad_\\n            1. Adaptive Gradient Algorithm\\n            1. AdaGrad adapts to the data in order to chose a learning rate for each feature.\\n            1. The performance of the AdaGrad optimizer was mediocre, with a final convergence at 83.4%.\\n        5. _Nadam_\\n            1. Nesterov Adaptive Moment Estimation\\n            1. Nadam combines Adam with Nesterov\\'s accelerated gradient.\\n            1. Our results from this test were one of the lowest with Adam, converging to 82.3% and increasing by a whopping 0.3% over the course of 8 generations.\\n\\n[4]\\nCheck out our [Optimizer choices data](https://docs.google.com/document/d/1YI5wluhh3rqHs8LBqqL2vOxqNwZdYiPHstWkS2_1s4s/edit?usp=sharing)\\n\\n** Preprocessing techniques**\\n1. Change the padding(*\"maxlen = 80\"*) for each reviews\\n    1. It is reasonable to assume when people write reviews, they would express their opinion at the start more often than at the end. So we can limit each review to some *\"maxlen\"* to eliminate noisy data. \\n2. Change the top most frequent words(*\"max_features = nb_words = 20000\"*) for training and testing dataset to keep \\n    1. Some words are essential than other words for expressing sentiment. For example, \"love\" would express more emotion than \"keyboard\". We therefore limit *\"max_features\"* to eliminate less sentimentally expresed words.\\n3. Set *\"index_from = 0\"* as we explained previously\\n    \\nCheck out our [Preprocessing techniques data](https://docs.google.com/document/d/1pjZbvzbbHWRUvybd--uTJUhPKvMCjbjxacE3j_zQCM4/edit?usp=sharing)\\n\\n\\n\\n```python\\nimport pandas as pd\\nimport re\\nimport matplotlib.pyplot as plt\\n\\n\"\"\" get data from model generated text; we further used this script to get datas below. \"\"\"\\nmodel_output = \"Epoch 1/8 - acc: 0.7090 - val_acc: 0.8205 Epoch 2/8 - acc: 0.8402 - val_acc: 0.8370 , ....\"\\ndata_list = re.findall(r\\'acc: (d+.d+)\\', model_output)\\ndata_list = list(map(float, data_list))\\nprint(data_list[::2])\\nprint(data_list[1::2])\\n\\n\"\"\" activation function data \"\"\"\\ntrain_hard_sigmoid = pd.DataFrame({\\'hard_sigmoid\\': [0.72,0.8137,0.8403,0.8659,0.8892,0.8974,0.9116,0.9184]})\\ntrain_sigmoid = pd.DataFrame({\\'sigmoid\\': [0.7316,0.8358,0.8737,0.8979,0.9180,0.9298,0.9423,0.9499]})\\ntrain_tanh = pd.DataFrame({\\'tanh\\': [0.6574,0.7830,0.8335,0.8710,0.8882,0.8954,0.9052,0.8746]})\\ntrain_relu = pd.DataFrame({\\'relu\\': [0.6524,0.7396,0.7273,0.5278,0.5663,0.6519,0.8128,0.8407]})\\nactivation_train = pd.concat([train_hard_sigmoid,train_sigmoid,train_tanh,train_relu], axis = 1)\\n\\ntest_hard_sigmoid = pd.DataFrame({\\'hard_sigmoid\\': [0.7417,0.8195,0.8336,0.8351,0.8280,0.8241,0.8282,0.8302]})\\ntest_sigmoid = pd.DataFrame({\\'sigmoid\\': [0.7898,0.8370,0.8360,0.8322,0.8330,0.8362,0.8280,0.8266]})\\ntest_tanh = pd.DataFrame({\\'tanh\\': [0.7883,0.7926,0.8153,0.8193,0.7427,0.7679,0.7967,0.7760]})\\ntest_relu = pd.DataFrame({\\'relu\\': [0.7634,0.7986,0.5040,0.4519,0.5000,0.7477,0.7482,0.7388]})\\nactivation_test = pd.concat([test_hard_sigmoid,test_sigmoid, test_tanh, test_relu], axis = 1)\\n\\n\\n\"\"\" optimizer data \"\"\"\\ntrain_adam = pd.DataFrame({\\'adam\\': [0.7316, 0.8358, 0.8737, 0.8979, 0.918, 0.9298, 0.9423, 0.9499]})\\ntrain_RMSprop = pd.DataFrame({\\'RMSprop\\': [0.7357, 0.8212, 0.8461, 0.8664, 0.8764, 0.8844, 0.8927, 0.8998]})\\ntrain_Adagrad = pd.DataFrame({\\'Adagrad\\': [0.7349, 0.8347, 0.8654, 0.8828, 0.8963, 0.902, 0.9118, 0.9164]})\\ntrain_Nadam = pd.DataFrame({\\'Nadam\\': [0.709, 0.8402, 0.8871, 0.9174, 0.9358, 0.947, 0.957, 0.9625]})\\noptimizer_train = pd.concat([train_adam,train_RMSprop,train_Adagrad,train_Nadam], axis = 1)\\n\\n\\ntest_adam = pd.DataFrame({\\'adam\\': [0.7898, 0.837, 0.836, 0.8322, 0.833, 0.8362, 0.828, 0.8266]})\\ntest_RMSprop = pd.DataFrame({\\'RMSprop\\': [0.8288, 0.831, 0.8512, 0.8317, 0.8529, 0.8401, 0.847, 0.849]})\\ntest_Adagrad = pd.DataFrame({\\'Adagrad\\': [0.8264, 0.7822, 0.8432, 0.8435, 0.838, 0.8372, 0.8384, 0.835]})\\ntest_Nadam = pd.DataFrame({\\'Nadam\\': [0.8205, 0.837, 0.833, 0.8366, 0.8325, 0.8289, 0.8276, 0.8238]})\\noptimizer_test = pd.concat([test_adam,test_RMSprop,test_Adagrad,test_Nadam], axis = 1)\\n\\n\\n\"\"\" preprocessing data \"\"\"\\ntrain_max_len_120 = pd.DataFrame({\\'max_len_120\\': [0.7041, 0.8279, 0.8577, 0.8767, 0.889, 0.8983, 0.9042, 0.9142, 0.9198, 0.9247]})\\ntrain_max_len_160 = pd.DataFrame({\\'max_len_160\\': [0.722, 0.8315, 0.863, 0.8832, 0.8937, 0.9032, 0.9124, 0.9202, 0.9259, 0.9305]})\\ntrain_max_len_200 = pd.DataFrame({\\'max_len_200\\': [0.7176, 0.8294, 0.8623, 0.885, 0.8975, 0.9068, 0.9131, 0.9214, 0.9265, 0.9327]})\\ntrain_max_len = pd.concat([train_max_len_120,train_max_len_160,train_max_len_200], axis = 1)\\n\\ntest_max_len_120 = pd.DataFrame({\\'max_len_120\\': [0.8318, 0.8538, 0.8657, 0.8227, 0.8681, 0.8693, 0.8731, 0.8708, 0.8718, 0.8584]})\\ntest_max_len_160 = pd.DataFrame({\\'max_len_160\\': [0.8481, 0.694, 0.8731, 0.8752, 0.8768, 0.8794, 0.8779, 0.88, 0.8834, 0.8792]})\\ntest_max_len_200 = pd.DataFrame({\\'max_len_200\\': [0.8254, 0.8498, 0.877, 0.8671, 0.8828, 0.8854, 0.8891, 0.8862, 0.8877, 0.8769]})\\ntest_max_len = pd.concat([test_max_len_120,test_max_len_160,test_max_len_200], axis = 1)\\n\\ntrain_max_features_4000 = pd.DataFrame({\\'max_features_4000\\': [0.7287, 0.8087, 0.8272, 0.8416, 0.8522, 0.857, 0.8608, 0.8672, 0.8729, 0.8763]})\\ntrain_max_features_3000 = pd.DataFrame({\\'max_features_3000\\': [0.7218, 0.8039, 0.823, 0.8349, 0.8422, 0.8492, 0.8562, 0.8607, 0.8638, 0.8696]})\\ntrain_max_features_2000 = pd.DataFrame({\\'max_features_2000\\': [0.7016, 0.7971, 0.8116, 0.8214, 0.8314, 0.8364, 0.8417, 0.8476, 0.8466, 0.8499]})\\ntrain_max_features = pd.concat([train_max_features_4000,train_max_features_3000,train_max_features_2000], axis = 1)\\n\\ntest_max_features_4000 = pd.DataFrame({\\'max_features_4000\\': [0.7834, 0.821, 0.8296, 0.8473, 0.8521, 0.8435, 0.8461, 0.85, 0.8545, 0.8486]})\\ntest_max_features_3000 = pd.DataFrame({\\'max_features_3000\\': [0.7859, 0.824, 0.8231, 0.8448, 0.8465, 0.84, 0.8501, 0.8412, 0.8488, 0.8536]})\\ntest_max_features_2000 = pd.DataFrame({\\'max_features_2000\\': [0.8152, 0.8274, 0.83, 0.8392, 0.8268, 0.8376, 0.8452, 0.8457, 0.8424, 0.8413]})\\ntest_max_features = pd.concat([test_max_features_4000,test_max_features_3000,test_max_features_2000], axis = 1)\\n\\n\"\"\" plotting \"\"\"\\n%matplotlib inline\\nf1, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\\nf1.set_figwidth(15)\\nactivation_train.plot(ax = ax1, title = \"activation_train\")\\nactivation_test.plot(ax = ax2, title = \"activation_test\")\\n\\nf2, (ax3, ax4) = plt.subplots(1, 2, sharey=True)\\nf2.set_figwidth(15)\\noptimizer_train.plot(ax = ax3,title = \"optimizer_train\")\\noptimizer_test.plot(ax = ax4,title = \"optimizer_test\")\\n\\nf3, (ax5, ax6) = plt.subplots(1, 2, sharey=True)\\nf3.set_figwidth(15)\\ntrain_max_len.plot(ax = ax5,title = \"train_max_len\")\\ntest_max_len.plot(ax = ax6,title = \"test_max_len\")\\n\\nf4, (ax7, ax8) = plt.subplots(1, 2, sharey=True)\\nf4.set_figwidth(15)\\ntrain_max_features.plot(ax = ax7,title = \"train_max_features\")\\ntest_max_features.plot(ax = ax8,title = \"test_max_features\")\\n```\\n\\n    [0.709, 0.8402]\\n    [0.8205, 0.837]\\n\\n    <matplotlib.axes._subplots.AxesSubplot at 0x1d192d00f28>\\n\\n![png](' + __webpack_require__(57) + ')\\n\\n\\n\\n![png](' + __webpack_require__(58) + ')\\n\\n\\n\\n![png](' + __webpack_require__(59) + ')\\n\\n\\n\\n![png](' + __webpack_require__(60) + ')\\n\\n\\n## Building New Model \\nAfter such comparision, we decided to build model using *RMSprop* optimizer, *sigmoid* activation function, *max_features* = 3000, and *max_len*= 200.\\n\\n\\n```python\\nfrom __future__ import print_function\\nimport numpy as np\\nnp.random.seed(1337)  # for reproducibility\\n\\nfrom keras.preprocessing import sequence\\nfrom keras.utils import np_utils\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Dropout, Activation, Embedding\\nfrom keras.layers import LSTM, SimpleRNN, GRU\\nfrom keras.datasets import imdb\\nfrom keras.layers.wrappers import TimeDistributed\\nimport pickle\\nfrom six.moves import cPickle\\nimport pandas as pd\\nimport math\\nimport matplotlib.pyplot as plt\\n\\n\\nmax_features = 3000\\nmaxlen = 200  # cut texts after this number of words (among top max_features most common words)\\nbatch_size = 32\\n\\nprint(\\'Loading data...\\')\\n(XX_train, y_train), (XX_test, y_test) = imdb.load_data(path=\\'imdb_full.pkl\\', nb_words=max_features, skip_top=10,\\n              maxlen=None, seed=113,\\n              start_char=1, oov_char=2, index_from=0)\\nprint(len(XX_train), \\'train sequences\\')\\nprint(len(XX_test), \\'test sequences\\')\\n\\nprint(\\'Pad sequences (samples x time)\\')\\nX_train = sequence.pad_sequences(XX_train, maxlen=maxlen)\\nX_test = sequence.pad_sequences(XX_test, maxlen=maxlen)\\nprint(\\'X_train shape:\\', X_train.shape)\\nprint(\\'X_test shape:\\', X_test.shape)\\n\\nprint(\\'Build model...\\')\\nmodel = Sequential()\\nmodel.add(Embedding(max_features, 128, dropout=0.2))\\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\\nmodel.add(Dense(1))\\nmodel.add(Activation(\\'sigmoid\\'))\\n\\n# try using different optimizers and different optimizer configs\\nmodel.compile(loss=\\'binary_crossentropy\\',\\n              optimizer=\\'rmsprop\\',\\n              metrics=[\\'accuracy\\'])\\n\\nprint(\\'Train...\\')\\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\\n          validation_data=(X_test, y_test))\\n```\\n\\n    Loading data...\\n    25000 train sequences\\n    25000 test sequences\\n    Pad sequences (samples x time)\\n    X_train shape: (25000, 200)\\n    X_test shape: (25000, 200)\\n    Build model...\\n    Train...\\n    Train on 25000 samples, validate on 25000 samples\\n    Epoch 1/10\\n    25000/25000 [==============================] - 288s - loss: 0.5716 - acc: 0.6874 - val_loss: 0.3979 - val_acc: 0.8376\\n    Epoch 2/10\\n    25000/25000 [==============================] - 289s - loss: 0.4373 - acc: 0.8071 - val_loss: 0.3447 - val_acc: 0.8603\\n    Epoch 3/10\\n    25000/25000 [==============================] - 297s - loss: 0.3914 - acc: 0.8336 - val_loss: 0.3410 - val_acc: 0.8577\\n    Epoch 4/10\\n    25000/25000 [==============================] - 301s - loss: 0.3526 - acc: 0.8508 - val_loss: 0.3228 - val_acc: 0.8604\\n    Epoch 5/10\\n    25000/25000 [==============================] - 305s - loss: 0.3224 - acc: 0.8631 - val_loss: 0.2915 - val_acc: 0.8782\\n    Epoch 6/10\\n    25000/25000 [==============================] - 302s - loss: 0.3031 - acc: 0.8756 - val_loss: 0.3119 - val_acc: 0.8671\\n    Epoch 7/10\\n    25000/25000 [==============================] - 305s - loss: 0.2868 - acc: 0.8830 - val_loss: 0.2771 - val_acc: 0.8833\\n    Epoch 8/10\\n    25000/25000 [==============================] - 290s - loss: 0.2745 - acc: 0.8867 - val_loss: 0.2739 - val_acc: 0.8842\\n    Epoch 9/10\\n    25000/25000 [==============================] - 275s - loss: 0.2649 - acc: 0.8904 - val_loss: 0.2715 - val_acc: 0.8844\\n    Epoch 10/10\\n    25000/25000 [==============================] - 284s - loss: 0.2587 - acc: 0.8937 - val_loss: 0.2718 - val_acc: 0.8875\\n    \\n\\n\\n\\n\\n    <keras.callbacks.History at 0x1d1950c3588>\\n\\n\\n\\nAs we can see, our new model has a better prediction of 88.75% accuracy versus the original model.\\n\\n## Further Applications\\nVisualization of sentiment analysis ,inspired by [Taylor Arnold\\'s jupyter notebook](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/notebook21.html) [9]\\n\\n\\n```python\\n%matplotlib inline\\nfrom keras.layers.wrappers import TimeDistributed\\nimport pickle\\nfrom six.moves import cPickle\\nimport pandas as pd\\nimport math\\nimport matplotlib.pyplot as plt\\n\\ndef reconstruct_text(index, index_to_word):\\n    text = []\\n    for ind in index:\\n        if ind != 0:\\n            text += [index_to_word[ind]]\\n        else:\\n            text += [\"\"]\\n    return text\\n\\nword_to_index = imdb.get_word_index()\\nindex_to_word = {k:v for v,k in word_to_index.items()}\\nf = open(\\'imdb_full.pkl\\', \\'rb\\')\\n(x_train, labels_train), (x_test, labels_test) = cPickle.load(f)\\nf.close()\\ndf = pd.DataFrame(x_train)\\n\\nmodel2 = Sequential()\\nmodel2.add(Embedding(max_features, 128, dropout=0.2))\\nmodel2.add(LSTM(128, dropout_W=0.2, dropout_U=0.2, return_sequences=True))  # try using a GRU instead, for fun\\nmodel2.add(TimeDistributed(Dense(1)))\\nmodel2.add(Activation(\\'sigmoid\\'))\\nmodel2.compile(loss=\\'binary_crossentropy\\',\\n              optimizer=\\'rmsprop\\',\\n              metrics=[\\'accuracy\\'])\\n\\nmodel2.set_weights(model.get_weights())\\ny_hat2 = model2.predict(X_train)\\n\\nind = 100\\ntokens = reconstruct_text(X_train[ind], index_to_word)\\n\\nplt.figure(figsize=(16, 10))\\nplt.plot(y_hat2[ind],alpha=0.5)\\nfor i in range(len(tokens)):\\n    plt.text(i,0.5,tokens[i],rotation=90)\\n```\\n\\n\\n![png](' + __webpack_require__(61) + ')\\n\\n\\n## Apply Current Model to Amazon Review Data\\nNow that we have successfully improved the model, we further apply our new model to Amazon Review data provided by Dr. Allen.\\nAmazon Review Dataset has preclassified sentiment value on 6 categories: books, camera, dvd, health, music, software. We need to complie data in a specific way so that our current model can directly use it.\\n\\n### 1. Change File Structure\\nWe will split the reviews into training set and testing set by changing the file structure\\n\\n\\n```python\\nfrom IPython.display import Image\\ni = Image(filename=\\'file_structure.png\\')\\ni\\n```\\n\\n\\n\\n\\n![png](' + __webpack_require__(62) + ')\\n\\n\\n\\n### 2. Preprocess Amazon Reviews\\n**Load reviews:**\\n\\n\\n```python\\nfrom six.moves import cPickle\\nimport numpy\\nimport os\\nimport pandas as pd\\nimport string\\nfrom collections import Counter\\nfrom keras.preprocessing.text import Tokenizer\\n\\npath = [\"books/\", \"camera/\", \"dvd/\", \"health/\", \"music/\", \"software/\"]  # camera, has missing data\\nff = []\\ninput_label = []\\nfor i in range(6):\\n    ff += [path[i] + \"train/pos/\" + x for x in os.listdir(path[i] + \"train/pos\")] +          [path[i] + \"train/neg/\" + x for x in os.listdir(path[i] + \"train/neg\")] +          [path[i] + \"test/pos/\" + x for x in os.listdir(path[i] + \"test/pos\")] +          [path[i] + \"test/neg/\" + x for x in os.listdir(path[i] + \"test/neg\")]\\n    \\n    # Because of missing data, we need to measure how many reviews are there in each folder in order to label them correctly.\\n    train_pos = len(os.listdir(path[i] + \"train/pos\"))\\n    train_neg = len(os.listdir(path[i] + \"train/neg\"))\\n    test_pos = len(os.listdir(path[i] + \"test/pos\"))\\n    test_neg = len(os.listdir(path[i] + \"test/neg\"))\\n    input_label += [1] * train_pos + [0] * train_neg + [1] * test_pos + [0] * test_neg\\n    \\n      \\ninput_text  = []\\nfor f in ff:\\n    with open(f, \\'rb\\') as fin:\\n        temp = fin.read().splitlines()\\n        x = \" \".join([x.decode(\"utf-8\", errors = \\'ignore\\') for x in temp])\\n        input_text += [x]\\n        \\nprint(input_text[0])\\n```\\n\\n    I wish I could give this book four and a half stars instead of four; I can\\'t quite justify five stars in my mind.  Two of the stories were definitely good: the first, \"masked Riders\" by Parhelion, and the third, \"Ricochet\" by BA Tortuga.  I enjoyed both, but was not particularly snowed by the intensity of the conflict/plot line or the main characters and their relationships.  Not so the second story: \"Hung Up\" by Cat Kane was a powerful, intense and moving story about two delightful yet flawed characters who had secrets that tore their relationship up until they were able to work their ways (separately) around the issues.  That story deserved a five stars plus rating and I wish Kane would turn it into a full length novel and develop the characters and their backgrounds more.  This book is an excellent read both for the plot line and the erotic substance.  Enjoy\\n    \\n\\n**Tokenize reviews:**\\n\\n\\n```python\\ninput_text  = []\\nfor f in ff:\\n    with open(f, \\'rb\\') as fin:\\n        temp = fin.read().splitlines()\\n        x = \" \".join([x.decode(\"utf-8\", errors = \\'ignore\\') for x in temp])\\n        input_text += [x]\\n\\ncut_index = int(len(input_text)/2)\\ntok = Tokenizer()\\ntok.fit_on_texts(input_text[:cut_index])\\n\\nX_train = tok.texts_to_sequences(input_text[:cut_index])\\nX_test  = tok.texts_to_sequences(input_text[cut_index:])\\ny_train = input_label[:cut_index]\\ny_test  = input_label[cut_index:]\\n\\nprint(\"As you can see, the first review has been encoded as their word index:\")\\nprint(X_train[0][:10])\\n```\\n\\n    As you can see, the first review has been encoded as their word index:\\n    [6, 499, 6, 99, 221, 9, 22, 501, 2, 3]\\n    \\n\\n**Reconstruct reviews:**\\n\\n\\n```python\\nwords = {k:v for v,k in tok.word_index.items()}\\ndef reconstruct_text(index, words):\\n    text = []\\n    for ind in index:\\n        if ind != 0:\\n            text += [words[ind]]\\n        else:\\n            text += [\"\"]\\n    return text\\n    \\nprint(input_text[100])\\nprint(reconstruct_text(X_train[100], words))\\n```\\n\\n    Peck relates growing up in rural/small town Vermont with a best friend who gets him into lots of trouble. Humor and pranks abound in between lessons learned\\n    [\\'peck\\', \\'relates\\', \\'growing\\', \\'up\\', \\'in\\', \\'rural\\', \\'small\\', \\'town\\', \\'vermont\\', \\'with\\', \\'a\\', \\'best\\', \\'friend\\', \\'who\\', \\'gets\\', \\'him\\', \\'into\\', \\'lots\\', \\'of\\', \\'trouble\\', \\'humor\\', \\'and\\', \\'pranks\\', \\'abound\\', \\'in\\', \\'between\\', \\'lessons\\', \\'learned\\']\\n    \\n\\n**Store reviews: **\\n\\n\\n```python\\nf = open(\\'amzn_full.pkl\\', \\'wb\\')\\ntrain_tuple = (X_train, y_train)\\ntest_tuple = (X_test, y_test)\\ncombine = (train_tuple, test_tuple)\\ncPickle.dump(combine, f)\\nf.close()\\n```\\n\\n### 3. Apply Current Model to Amazon Reviews:\\nFirstly we need to make a copy of [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py)(the script that loads imdb_full.pkl data) and make it loads our amzn_full.pkl instead. Name the new file amzn.py. Then we apply our model:\\n\\n\\n```python\\nfrom __future__ import print_function\\nimport numpy as np\\nnp.random.seed(1337)  # for reproducibility\\n\\nfrom keras.preprocessing import sequence\\nfrom keras.utils import np_utils\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Dropout, Activation, Embedding\\nfrom keras.layers import LSTM, SimpleRNN, GRU\\nimport amzn\\nfrom keras.layers.wrappers import TimeDistributed\\nimport pickle\\nfrom six.moves import cPickle\\nimport pandas as pd\\nimport math\\nimport matplotlib.pyplot as plt\\n\\n\\nmax_features = 3000\\nmaxlen = 200  # cut texts after this number of words (among top max_features most common words)\\nbatch_size = 32\\n\\nprint(\\'Loading data...\\')\\n(XX_train, y_train), (XX_test, y_test) = amzn.load_data(path=\\'amzn_full.pkl\\', nb_words=max_features)\\nprint(len(XX_train), \\'train sequences\\')\\nprint(len(XX_test), \\'test sequences\\')\\n\\nprint(\\'Pad sequences (samples x time)\\')\\nX_train = sequence.pad_sequences(XX_train, maxlen=maxlen)\\nX_test = sequence.pad_sequences(XX_test, maxlen=maxlen)\\nprint(\\'X_train shape:\\', X_train.shape)\\nprint(\\'X_test shape:\\', X_test.shape)\\n\\nprint(\\'Build model...\\')\\nmodel = Sequential()\\nmodel.add(Embedding(max_features, 128, dropout=0.2))\\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\\nmodel.add(Dense(1))\\nmodel.add(Activation(\\'sigmoid\\'))\\n\\n# try using different optimizers and different optimizer configs\\nmodel.compile(loss=\\'binary_crossentropy\\',\\n              optimizer=\\'RMSprop\\',\\n              metrics=[\\'accuracy\\'])\\n\\nprint(\\'Train...\\')\\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\\n          validation_data=(X_test, y_test))\\n```\\n\\n    Loading data...\\n    5957 train sequences\\n    5957 test sequences\\n    Pad sequences (samples x time)\\n    X_train shape: (5957, 200)\\n    X_test shape: (5957, 200)\\n    Build model...\\n    Train...\\n    Train on 5957 samples, validate on 5957 samples\\n    Epoch 1/10\\n    5957/5957 [==============================] - 63s - loss: 0.6823 - acc: 0.5587 - val_loss: 0.6493 - val_acc: 0.6325\\n    Epoch 2/10\\n    5957/5957 [==============================] - 63s - loss: 0.5696 - acc: 0.7082 - val_loss: 0.5350 - val_acc: 0.7386\\n    Epoch 3/10\\n    5957/5957 [==============================] - 68s - loss: 0.4870 - acc: 0.7692 - val_loss: 0.5081 - val_acc: 0.7480\\n    Epoch 4/10\\n    5957/5957 [==============================] - 70s - loss: 0.4290 - acc: 0.8054 - val_loss: 0.8713 - val_acc: 0.6157\\n    Epoch 5/10\\n    5957/5957 [==============================] - 60s - loss: 0.3910 - acc: 0.8348 - val_loss: 0.5099 - val_acc: 0.7646\\n    Epoch 6/10\\n    5957/5957 [==============================] - 60s - loss: 0.3738 - acc: 0.8385 - val_loss: 0.5034 - val_acc: 0.7702\\n    Epoch 7/10\\n    5957/5957 [==============================] - 59s - loss: 0.3465 - acc: 0.8531 - val_loss: 0.4990 - val_acc: 0.7875\\n    Epoch 8/10\\n    5957/5957 [==============================] - 62s - loss: 0.3143 - acc: 0.8724 - val_loss: 0.5216 - val_acc: 0.7747\\n    Epoch 9/10\\n    5957/5957 [==============================] - 64s - loss: 0.2992 - acc: 0.8795 - val_loss: 0.4989 - val_acc: 0.7900\\n    Epoch 10/10\\n    5957/5957 [==============================] - 64s - loss: 0.2846 - acc: 0.8815 - val_loss: 0.4971 - val_acc: 0.7890\\n    \\n\\n\\n\\n\\n    <keras.callbacks.History at 0x1d1f921a668>\\n\\n\\n\\n## Compare Result to Original Model\\n\\n\\n```python\\nfrom __future__ import print_function\\nimport numpy as np\\nnp.random.seed(1337)  # for reproducibility\\n\\nfrom keras.preprocessing import sequence\\nfrom keras.utils import np_utils\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Dropout, Activation, Embedding\\nfrom keras.layers import LSTM, SimpleRNN, GRU\\nimport amzn\\n\\nmax_features = 20000\\nmaxlen = 80  # cut texts after this number of words (among top max_features most common words)\\nbatch_size = 32\\n\\nprint(\\'Loading data...\\')\\n(X_train, y_train), (X_test, y_test) = amzn.load_data(path=\\'amzn_full.pkl\\', nb_words=max_features)\\nprint(len(X_train), \\'train sequences\\')\\nprint(len(X_test), \\'test sequences\\')\\n\\nprint(\\'Pad sequences (samples x time)\\')\\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\\nprint(\\'X_train shape:\\', X_train.shape)\\nprint(\\'X_test shape:\\', X_test.shape)\\n\\nprint(\\'Build model...\\')\\nmodel = Sequential()\\nmodel.add(Embedding(max_features, 128, dropout=0.2))\\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\\nmodel.add(Dense(1))\\nmodel.add(Activation(\\'sigmoid\\'))\\n\\n# try using different optimizers and different optimizer configs\\nmodel.compile(loss=\\'binary_crossentropy\\',\\n              optimizer=\\'adam\\',\\n              metrics=[\\'accuracy\\'])\\n\\nprint(\\'Train...\\')\\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\\n          validation_data=(X_test, y_test))\\n```\\n\\n    Loading data...\\n    5957 train sequences\\n    5957 test sequences\\n    Pad sequences (samples x time)\\n    X_train shape: (5957, 80)\\n    X_test shape: (5957, 80)\\n    Build model...\\n    Train...\\n    Train on 5957 samples, validate on 5957 samples\\n    Epoch 1/10\\n    5957/5957 [==============================] - 31s - loss: 0.6641 - acc: 0.5949 - val_loss: 0.5708 - val_acc: 0.7077\\n    Epoch 2/10\\n    5957/5957 [==============================] - 31s - loss: 0.4679 - acc: 0.7823 - val_loss: 0.5437 - val_acc: 0.7326\\n    Epoch 3/10\\n    5957/5957 [==============================] - 31s - loss: 0.3293 - acc: 0.8687 - val_loss: 0.5521 - val_acc: 0.7521\\n    Epoch 4/10\\n    5957/5957 [==============================] - 31s - loss: 0.2428 - acc: 0.9077 - val_loss: 0.5821 - val_acc: 0.7601\\n    Epoch 5/10\\n    5957/5957 [==============================] - 31s - loss: 0.1790 - acc: 0.9347 - val_loss: 0.7185 - val_acc: 0.7408\\n    Epoch 6/10\\n    5957/5957 [==============================] - 31s - loss: 0.1270 - acc: 0.9542 - val_loss: 0.7469 - val_acc: 0.7519\\n    Epoch 7/10\\n    5957/5957 [==============================] - 32s - loss: 0.1136 - acc: 0.9599 - val_loss: 0.8070 - val_acc: 0.7430\\n    Epoch 8/10\\n    5957/5957 [==============================] - 31s - loss: 0.0890 - acc: 0.9694 - val_loss: 0.9022 - val_acc: 0.7521\\n    Epoch 9/10\\n    5957/5957 [==============================] - 31s - loss: 0.0721 - acc: 0.9758 - val_loss: 0.9214 - val_acc: 0.7275\\n    Epoch 10/10\\n    5957/5957 [==============================] - 32s - loss: 0.0583 - acc: 0.9802 - val_loss: 0.9235 - val_acc: 0.7334\\n    \\n\\n\\n\\n\\n    <keras.callbacks.History at 0x1d20002c588>\\n\\n\\n\\n\\n```python\\n%%html\\n<style>\\ntable {float:left}\\n</style>\\n```\\n\\n\\n<style>\\ntable {float:left}\\n</style>\\n\\n\\n# Conclusion\\nOur new model is based on two main hypothesis:\\n 1. It is reasonable to assume when people write reviews, they would express their opinion at the start more often than at the end. So we can limit each review to some *\"maxlen\"* to eliminate noisy data. \\n 2. Some words are essential than other words for expressing sentiment. For example, \"love\" would express more emotion than \"keyboard\". We therefore limit *\"max_features\"* to eliminate less sentimentally expresed words.\\n\\n\\nTherefore by experimenting different *\"maxlen\"*, *\"max_features\"* , and optimizers, our new model improves the prediction accuracy on both IMDB and Amazon Review dataset, hence validating our hypothesis.\\n\\n\\n\\n|                        | Original Model Provided by Keras| Our New Model\\n| :------                | :-----------                    | ----       |\\n| **Amazon Review **     | 73.34%                          | 78.90%     |\\n| **IMDB Movie Review ** | 82.35%                          | 88.75%     |\\n\\n\\n\\n<br/>\\n<br/>\\n<br/>\\n\\n# Reference\\n\\n\\n[1] Fran\\xB8cois Chollet. Keras. [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py), 2015 \\n\\n[2] Fran\\xB8cois Chollet. Keras. [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py), 2015 \\n\\n[3] \"Convolutional Neural Networks for Visual Recognition.\" Web. 06 Dec. 2016. (http://cs231n.github.io/neural-networks-1).\\n\\n[4] Ruder, Sebastian. \"An Overview of Gradient Descent Optimization Algorithms.\" 30 Sept. 2016. Web. (http://sebastianruder.com/optimizing-gradient-descent/).\\n\\n[5] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas//papers/wvSent_acl2011.pdf). *The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)*, 2011\\n\\n[6] T Mikolov, I Sutskever, K Chen, GS Corrado, J Dean - Advances in neural information processing systems, [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), 2013\\n\\n[7] Hochreiter, S., & Schmidhuber, J. [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf). Neural computation, 9(8), 1735-1780. 1997\\n\\n[8] Olah C [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), 2015\\n\\n[9] Arnold T, *STAT 365/665: Data Mining and Machine Learning*, [Recurrent neural networks](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/notebook21.html), 2016\\n';\n\n/* harmony default export */ __webpack_exports__[\"default\"] = ({\n    name: 'article',\n    data: function data() {\n        return {\n            rawHtml: __WEBPACK_IMPORTED_MODULE_0_marked___default()(article)\n        };\n    },\n    mounted: function mounted() {\n        __WEBPACK_IMPORTED_MODULE_1_prismjs___default.a.highlightAll();\n    }\n});\n\n/***/ }),\n\n/***/ 48:\n/***/ (function(module, exports, __webpack_require__) {\n\nexports = module.exports = __webpack_require__(13)(true);\n// imports\n\n\n// module\nexports.push([module.i, \"\", \"\", {\"version\":3,\"sources\":[],\"names\":[],\"mappings\":\"\",\"file\":\"article.vue\",\"sourceRoot\":\"\"}]);\n\n// exports\n\n\n/***/ }),\n\n/***/ 53:\n/***/ (function(module, exports, __webpack_require__) {\n\n// style-loader: Adds some css to the DOM by adding a <style> tag\n\n// load the styles\nvar content = __webpack_require__(48);\nif(typeof content === 'string') content = [[module.i, content, '']];\nif(content.locals) module.exports = content.locals;\n// add the styles to the DOM\nvar update = __webpack_require__(14)(\"4eb8fe15\", content, true);\n\n/***/ }),\n\n/***/ 57:\n/***/ (function(module, exports, __webpack_require__) {\n\nmodule.exports = __webpack_require__.p + \"static/img/output_11_2.0408346.png\";\n\n/***/ }),\n\n/***/ 58:\n/***/ (function(module, exports, __webpack_require__) {\n\nmodule.exports = __webpack_require__.p + \"static/img/output_11_3.eda9622.png\";\n\n/***/ }),\n\n/***/ 59:\n/***/ (function(module, exports, __webpack_require__) {\n\nmodule.exports = __webpack_require__.p + \"static/img/output_11_4.f5d8933.png\";\n\n/***/ }),\n\n/***/ 60:\n/***/ (function(module, exports, __webpack_require__) {\n\nmodule.exports = __webpack_require__.p + \"static/img/output_11_5.827a616.png\";\n\n/***/ }),\n\n/***/ 61:\n/***/ (function(module, exports, __webpack_require__) {\n\nmodule.exports = __webpack_require__.p + \"static/img/output_16_0.a4a67e4.png\";\n\n/***/ }),\n\n/***/ 62:\n/***/ (function(module, exports) {\n\nmodule.exports = \"data:image/png;base64,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\"\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// static/js/1.c694b068eb9bba984b39.js","module.exports={render:function (){var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;\n  return _c('div', {\n    staticClass: \"research-article\"\n  }, [_c('div', {\n    domProps: {\n      \"innerHTML\": _vm._s(_vm.rawHtml)\n    }\n  })])\n},staticRenderFns: []}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/vue-loader/lib/template-compiler?{\"id\":\"data-v-fc76b68e\"}!./~/vue-loader/lib/selector.js?type=template&index=0!./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/article.vue\n// module id = 115\n// module chunks = 1","\n/* styles */\nrequire(\"!!../../../../node_modules/extract-text-webpack-plugin/loader.js?{\\\"omit\\\":1,\\\"remove\\\":true}!vue-style-loader!css-loader?{\\\"minimize\\\":true,\\\"sourceMap\\\":true}!../../../../node_modules/vue-loader/lib/style-compiler/index?{\\\"id\\\":\\\"data-v-fc76b68e\\\",\\\"scoped\\\":true,\\\"hasInlineConfig\\\":false}!../../../../node_modules/vue-loader/lib/selector?type=styles&index=0!./article.vue\")\n\nvar Component = require(\"!../../../../node_modules/vue-loader/lib/component-normalizer\")(\n  /* script */\n  require(\"!!babel-loader!../../../../node_modules/vue-loader/lib/selector?type=script&index=0!./article.vue\"),\n  /* template */\n  require(\"!!../../../../node_modules/vue-loader/lib/template-compiler/index?{\\\"id\\\":\\\"data-v-fc76b68e\\\"}!../../../../node_modules/vue-loader/lib/selector?type=template&index=0!./article.vue\"),\n  /* scopeId */\n  \"data-v-fc76b68e\",\n  /* cssModules */\n  null\n)\n\nmodule.exports = Component.exports\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/article.vue\n// module id = 15\n// module chunks = 1","\n/* **********************************************\n     Begin prism-core.js\n********************************************** */\n\nvar _self = (typeof window !== 'undefined')\n\t? window   // if in browser\n\t: (\n\t\t(typeof WorkerGlobalScope !== 'undefined' && self instanceof WorkerGlobalScope)\n\t\t? self // if in worker\n\t\t: {}   // if in node js\n\t);\n\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n * MIT license http://www.opensource.org/licenses/mit-license.php/\n * @author Lea Verou http://lea.verou.me\n */\n\nvar Prism = (function(){\n\n// Private helper vars\nvar lang = /\\blang(?:uage)?-(\\w+)\\b/i;\nvar uniqueId = 0;\n\nvar _ = _self.Prism = {\n\tutil: {\n\t\tencode: function (tokens) {\n\t\t\tif (tokens instanceof Token) {\n\t\t\t\treturn new Token(tokens.type, _.util.encode(tokens.content), tokens.alias);\n\t\t\t} else if (_.util.type(tokens) === 'Array') {\n\t\t\t\treturn tokens.map(_.util.encode);\n\t\t\t} else {\n\t\t\t\treturn tokens.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/\\u00a0/g, ' ');\n\t\t\t}\n\t\t},\n\n\t\ttype: function (o) {\n\t\t\treturn Object.prototype.toString.call(o).match(/\\[object (\\w+)\\]/)[1];\n\t\t},\n\n\t\tobjId: function (obj) {\n\t\t\tif (!obj['__id']) {\n\t\t\t\tObject.defineProperty(obj, '__id', { value: ++uniqueId });\n\t\t\t}\n\t\t\treturn obj['__id'];\n\t\t},\n\n\t\t// Deep clone a language definition (e.g. to extend it)\n\t\tclone: function (o) {\n\t\t\tvar type = _.util.type(o);\n\n\t\t\tswitch (type) {\n\t\t\t\tcase 'Object':\n\t\t\t\t\tvar clone = {};\n\n\t\t\t\t\tfor (var key in o) {\n\t\t\t\t\t\tif (o.hasOwnProperty(key)) {\n\t\t\t\t\t\t\tclone[key] = _.util.clone(o[key]);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\treturn clone;\n\n\t\t\t\tcase 'Array':\n\t\t\t\t\t// Check for existence for IE8\n\t\t\t\t\treturn o.map && o.map(function(v) { return _.util.clone(v); });\n\t\t\t}\n\n\t\t\treturn o;\n\t\t}\n\t},\n\n\tlanguages: {\n\t\textend: function (id, redef) {\n\t\t\tvar lang = _.util.clone(_.languages[id]);\n\n\t\t\tfor (var key in redef) {\n\t\t\t\tlang[key] = redef[key];\n\t\t\t}\n\n\t\t\treturn lang;\n\t\t},\n\n\t\t/**\n\t\t * Insert a token before another token in a language literal\n\t\t * As this needs to recreate the object (we cannot actually insert before keys in object literals),\n\t\t * we cannot just provide an object, we need anobject and a key.\n\t\t * @param inside The key (or language id) of the parent\n\t\t * @param before The key to insert before. If not provided, the function appends instead.\n\t\t * @param insert Object with the key/value pairs to insert\n\t\t * @param root The object that contains `inside`. If equal to Prism.languages, it can be omitted.\n\t\t */\n\t\tinsertBefore: function (inside, before, insert, root) {\n\t\t\troot = root || _.languages;\n\t\t\tvar grammar = root[inside];\n\n\t\t\tif (arguments.length == 2) {\n\t\t\t\tinsert = arguments[1];\n\n\t\t\t\tfor (var newToken in insert) {\n\t\t\t\t\tif (insert.hasOwnProperty(newToken)) {\n\t\t\t\t\t\tgrammar[newToken] = insert[newToken];\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\treturn grammar;\n\t\t\t}\n\n\t\t\tvar ret = {};\n\n\t\t\tfor (var token in grammar) {\n\n\t\t\t\tif (grammar.hasOwnProperty(token)) {\n\n\t\t\t\t\tif (token == before) {\n\n\t\t\t\t\t\tfor (var newToken in insert) {\n\n\t\t\t\t\t\t\tif (insert.hasOwnProperty(newToken)) {\n\t\t\t\t\t\t\t\tret[newToken] = insert[newToken];\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tret[token] = grammar[token];\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Update references in other language definitions\n\t\t\t_.languages.DFS(_.languages, function(key, value) {\n\t\t\t\tif (value === root[inside] && key != inside) {\n\t\t\t\t\tthis[key] = ret;\n\t\t\t\t}\n\t\t\t});\n\n\t\t\treturn root[inside] = ret;\n\t\t},\n\n\t\t// Traverse a language definition with Depth First Search\n\t\tDFS: function(o, callback, type, visited) {\n\t\t\tvisited = visited || {};\n\t\t\tfor (var i in o) {\n\t\t\t\tif (o.hasOwnProperty(i)) {\n\t\t\t\t\tcallback.call(o, i, o[i], type || i);\n\n\t\t\t\t\tif (_.util.type(o[i]) === 'Object' && !visited[_.util.objId(o[i])]) {\n\t\t\t\t\t\tvisited[_.util.objId(o[i])] = true;\n\t\t\t\t\t\t_.languages.DFS(o[i], callback, null, visited);\n\t\t\t\t\t}\n\t\t\t\t\telse if (_.util.type(o[i]) === 'Array' && !visited[_.util.objId(o[i])]) {\n\t\t\t\t\t\tvisited[_.util.objId(o[i])] = true;\n\t\t\t\t\t\t_.languages.DFS(o[i], callback, i, visited);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t},\n\tplugins: {},\n\n\thighlightAll: function(async, callback) {\n\t\tvar env = {\n\t\t\tcallback: callback,\n\t\t\tselector: 'code[class*=\"language-\"], [class*=\"language-\"] code, code[class*=\"lang-\"], [class*=\"lang-\"] code'\n\t\t};\n\n\t\t_.hooks.run(\"before-highlightall\", env);\n\n\t\tvar elements = env.elements || document.querySelectorAll(env.selector);\n\n\t\tfor (var i=0, element; element = elements[i++];) {\n\t\t\t_.highlightElement(element, async === true, env.callback);\n\t\t}\n\t},\n\n\thighlightElement: function(element, async, callback) {\n\t\t// Find language\n\t\tvar language, grammar, parent = element;\n\n\t\twhile (parent && !lang.test(parent.className)) {\n\t\t\tparent = parent.parentNode;\n\t\t}\n\n\t\tif (parent) {\n\t\t\tlanguage = (parent.className.match(lang) || [,''])[1].toLowerCase();\n\t\t\tgrammar = _.languages[language];\n\t\t}\n\n\t\t// Set language on the element, if not present\n\t\telement.className = element.className.replace(lang, '').replace(/\\s+/g, ' ') + ' language-' + language;\n\n\t\t// Set language on the parent, for styling\n\t\tparent = element.parentNode;\n\n\t\tif (/pre/i.test(parent.nodeName)) {\n\t\t\tparent.className = parent.className.replace(lang, '').replace(/\\s+/g, ' ') + ' language-' + language;\n\t\t}\n\n\t\tvar code = element.textContent;\n\n\t\tvar env = {\n\t\t\telement: element,\n\t\t\tlanguage: language,\n\t\t\tgrammar: grammar,\n\t\t\tcode: code\n\t\t};\n\n\t\t_.hooks.run('before-sanity-check', env);\n\n\t\tif (!env.code || !env.grammar) {\n\t\t\tif (env.code) {\n\t\t\t\tenv.element.textContent = env.code;\n\t\t\t}\n\t\t\t_.hooks.run('complete', env);\n\t\t\treturn;\n\t\t}\n\n\t\t_.hooks.run('before-highlight', env);\n\n\t\tif (async && _self.Worker) {\n\t\t\tvar worker = new Worker(_.filename);\n\n\t\t\tworker.onmessage = function(evt) {\n\t\t\t\tenv.highlightedCode = evt.data;\n\n\t\t\t\t_.hooks.run('before-insert', env);\n\n\t\t\t\tenv.element.innerHTML = env.highlightedCode;\n\n\t\t\t\tcallback && callback.call(env.element);\n\t\t\t\t_.hooks.run('after-highlight', env);\n\t\t\t\t_.hooks.run('complete', env);\n\t\t\t};\n\n\t\t\tworker.postMessage(JSON.stringify({\n\t\t\t\tlanguage: env.language,\n\t\t\t\tcode: env.code,\n\t\t\t\timmediateClose: true\n\t\t\t}));\n\t\t}\n\t\telse {\n\t\t\tenv.highlightedCode = _.highlight(env.code, env.grammar, env.language);\n\n\t\t\t_.hooks.run('before-insert', env);\n\n\t\t\tenv.element.innerHTML = env.highlightedCode;\n\n\t\t\tcallback && callback.call(element);\n\n\t\t\t_.hooks.run('after-highlight', env);\n\t\t\t_.hooks.run('complete', env);\n\t\t}\n\t},\n\n\thighlight: function (text, grammar, language) {\n\t\tvar tokens = _.tokenize(text, grammar);\n\t\treturn Token.stringify(_.util.encode(tokens), language);\n\t},\n\n\ttokenize: function(text, grammar, language) {\n\t\tvar Token = _.Token;\n\n\t\tvar strarr = [text];\n\n\t\tvar rest = grammar.rest;\n\n\t\tif (rest) {\n\t\t\tfor (var token in rest) {\n\t\t\t\tgrammar[token] = rest[token];\n\t\t\t}\n\n\t\t\tdelete grammar.rest;\n\t\t}\n\n\t\ttokenloop: for (var token in grammar) {\n\t\t\tif(!grammar.hasOwnProperty(token) || !grammar[token]) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tvar patterns = grammar[token];\n\t\t\tpatterns = (_.util.type(patterns) === \"Array\") ? patterns : [patterns];\n\n\t\t\tfor (var j = 0; j < patterns.length; ++j) {\n\t\t\t\tvar pattern = patterns[j],\n\t\t\t\t\tinside = pattern.inside,\n\t\t\t\t\tlookbehind = !!pattern.lookbehind,\n\t\t\t\t\tgreedy = !!pattern.greedy,\n\t\t\t\t\tlookbehindLength = 0,\n\t\t\t\t\talias = pattern.alias;\n\n\t\t\t\tif (greedy && !pattern.pattern.global) {\n\t\t\t\t\t// Without the global flag, lastIndex won't work\n\t\t\t\t\tvar flags = pattern.pattern.toString().match(/[imuy]*$/)[0];\n\t\t\t\t\tpattern.pattern = RegExp(pattern.pattern.source, flags + \"g\");\n\t\t\t\t}\n\n\t\t\t\tpattern = pattern.pattern || pattern;\n\n\t\t\t\t// Don’t cache length as it changes during the loop\n\t\t\t\tfor (var i=0, pos = 0; i<strarr.length; pos += strarr[i].length, ++i) {\n\n\t\t\t\t\tvar str = strarr[i];\n\n\t\t\t\t\tif (strarr.length > text.length) {\n\t\t\t\t\t\t// Something went terribly wrong, ABORT, ABORT!\n\t\t\t\t\t\tbreak tokenloop;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (str instanceof Token) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\n\t\t\t\t\tpattern.lastIndex = 0;\n\n\t\t\t\t\tvar match = pattern.exec(str),\n\t\t\t\t\t    delNum = 1;\n\n\t\t\t\t\t// Greedy patterns can override/remove up to two previously matched tokens\n\t\t\t\t\tif (!match && greedy && i != strarr.length - 1) {\n\t\t\t\t\t\tpattern.lastIndex = pos;\n\t\t\t\t\t\tmatch = pattern.exec(text);\n\t\t\t\t\t\tif (!match) {\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tvar from = match.index + (lookbehind ? match[1].length : 0),\n\t\t\t\t\t\t    to = match.index + match[0].length,\n\t\t\t\t\t\t    k = i,\n\t\t\t\t\t\t    p = pos;\n\n\t\t\t\t\t\tfor (var len = strarr.length; k < len && p < to; ++k) {\n\t\t\t\t\t\t\tp += strarr[k].length;\n\t\t\t\t\t\t\t// Move the index i to the element in strarr that is closest to from\n\t\t\t\t\t\t\tif (from >= p) {\n\t\t\t\t\t\t\t\t++i;\n\t\t\t\t\t\t\t\tpos = p;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * If strarr[i] is a Token, then the match starts inside another Token, which is invalid\n\t\t\t\t\t\t * If strarr[k - 1] is greedy we are in conflict with another greedy pattern\n\t\t\t\t\t\t */\n\t\t\t\t\t\tif (strarr[i] instanceof Token || strarr[k - 1].greedy) {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// Number of tokens to delete and replace with the new match\n\t\t\t\t\t\tdelNum = k - i;\n\t\t\t\t\t\tstr = text.slice(pos, p);\n\t\t\t\t\t\tmatch.index -= pos;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (!match) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\n\t\t\t\t\tif(lookbehind) {\n\t\t\t\t\t\tlookbehindLength = match[1].length;\n\t\t\t\t\t}\n\n\t\t\t\t\tvar from = match.index + lookbehindLength,\n\t\t\t\t\t    match = match[0].slice(lookbehindLength),\n\t\t\t\t\t    to = from + match.length,\n\t\t\t\t\t    before = str.slice(0, from),\n\t\t\t\t\t    after = str.slice(to);\n\n\t\t\t\t\tvar args = [i, delNum];\n\n\t\t\t\t\tif (before) {\n\t\t\t\t\t\targs.push(before);\n\t\t\t\t\t}\n\n\t\t\t\t\tvar wrapped = new Token(token, inside? _.tokenize(match, inside) : match, alias, match, greedy);\n\n\t\t\t\t\targs.push(wrapped);\n\n\t\t\t\t\tif (after) {\n\t\t\t\t\t\targs.push(after);\n\t\t\t\t\t}\n\n\t\t\t\t\tArray.prototype.splice.apply(strarr, args);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn strarr;\n\t},\n\n\thooks: {\n\t\tall: {},\n\n\t\tadd: function (name, callback) {\n\t\t\tvar hooks = _.hooks.all;\n\n\t\t\thooks[name] = hooks[name] || [];\n\n\t\t\thooks[name].push(callback);\n\t\t},\n\n\t\trun: function (name, env) {\n\t\t\tvar callbacks = _.hooks.all[name];\n\n\t\t\tif (!callbacks || !callbacks.length) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tfor (var i=0, callback; callback = callbacks[i++];) {\n\t\t\t\tcallback(env);\n\t\t\t}\n\t\t}\n\t}\n};\n\nvar Token = _.Token = function(type, content, alias, matchedStr, greedy) {\n\tthis.type = type;\n\tthis.content = content;\n\tthis.alias = alias;\n\t// Copy of the full string this token was created from\n\tthis.length = (matchedStr || \"\").length|0;\n\tthis.greedy = !!greedy;\n};\n\nToken.stringify = function(o, language, parent) {\n\tif (typeof o == 'string') {\n\t\treturn o;\n\t}\n\n\tif (_.util.type(o) === 'Array') {\n\t\treturn o.map(function(element) {\n\t\t\treturn Token.stringify(element, language, o);\n\t\t}).join('');\n\t}\n\n\tvar env = {\n\t\ttype: o.type,\n\t\tcontent: Token.stringify(o.content, language, parent),\n\t\ttag: 'span',\n\t\tclasses: ['token', o.type],\n\t\tattributes: {},\n\t\tlanguage: language,\n\t\tparent: parent\n\t};\n\n\tif (env.type == 'comment') {\n\t\tenv.attributes['spellcheck'] = 'true';\n\t}\n\n\tif (o.alias) {\n\t\tvar aliases = _.util.type(o.alias) === 'Array' ? o.alias : [o.alias];\n\t\tArray.prototype.push.apply(env.classes, aliases);\n\t}\n\n\t_.hooks.run('wrap', env);\n\n\tvar attributes = Object.keys(env.attributes).map(function(name) {\n\t\treturn name + '=\"' + (env.attributes[name] || '').replace(/\"/g, '&quot;') + '\"';\n\t}).join(' ');\n\n\treturn '<' + env.tag + ' class=\"' + env.classes.join(' ') + '\"' + (attributes ? ' ' + attributes : '') + '>' + env.content + '</' + env.tag + '>';\n\n};\n\nif (!_self.document) {\n\tif (!_self.addEventListener) {\n\t\t// in Node.js\n\t\treturn _self.Prism;\n\t}\n \t// In worker\n\t_self.addEventListener('message', function(evt) {\n\t\tvar message = JSON.parse(evt.data),\n\t\t    lang = message.language,\n\t\t    code = message.code,\n\t\t    immediateClose = message.immediateClose;\n\n\t\t_self.postMessage(_.highlight(code, _.languages[lang], lang));\n\t\tif (immediateClose) {\n\t\t\t_self.close();\n\t\t}\n\t}, false);\n\n\treturn _self.Prism;\n}\n\n//Get current script and highlight\nvar script = document.currentScript || [].slice.call(document.getElementsByTagName(\"script\")).pop();\n\nif (script) {\n\t_.filename = script.src;\n\n\tif (document.addEventListener && !script.hasAttribute('data-manual')) {\n\t\tif(document.readyState !== \"loading\") {\n\t\t\tif (window.requestAnimationFrame) {\n\t\t\t\twindow.requestAnimationFrame(_.highlightAll);\n\t\t\t} else {\n\t\t\t\twindow.setTimeout(_.highlightAll, 16);\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tdocument.addEventListener('DOMContentLoaded', _.highlightAll);\n\t\t}\n\t}\n}\n\nreturn _self.Prism;\n\n})();\n\nif (typeof module !== 'undefined' && module.exports) {\n\tmodule.exports = Prism;\n}\n\n// hack for components to work correctly in node.js\nif (typeof global !== 'undefined') {\n\tglobal.Prism = Prism;\n}\n\n\n/* **********************************************\n     Begin prism-markup.js\n********************************************** */\n\nPrism.languages.markup = {\n\t'comment': /<!--[\\w\\W]*?-->/,\n\t'prolog': /<\\?[\\w\\W]+?\\?>/,\n\t'doctype': /<!DOCTYPE[\\w\\W]+?>/i,\n\t'cdata': /<!\\[CDATA\\[[\\w\\W]*?]]>/i,\n\t'tag': {\n\t\tpattern: /<\\/?(?!\\d)[^\\s>\\/=$<]+(?:\\s+[^\\s>\\/=]+(?:=(?:(\"|')(?:\\\\\\1|\\\\?(?!\\1)[\\w\\W])*\\1|[^\\s'\">=]+))?)*\\s*\\/?>/i,\n\t\tinside: {\n\t\t\t'tag': {\n\t\t\t\tpattern: /^<\\/?[^\\s>\\/]+/i,\n\t\t\t\tinside: {\n\t\t\t\t\t'punctuation': /^<\\/?/,\n\t\t\t\t\t'namespace': /^[^\\s>\\/:]+:/\n\t\t\t\t}\n\t\t\t},\n\t\t\t'attr-value': {\n\t\t\t\tpattern: /=(?:('|\")[\\w\\W]*?(\\1)|[^\\s>]+)/i,\n\t\t\t\tinside: {\n\t\t\t\t\t'punctuation': /[=>\"']/\n\t\t\t\t}\n\t\t\t},\n\t\t\t'punctuation': /\\/?>/,\n\t\t\t'attr-name': {\n\t\t\t\tpattern: /[^\\s>\\/]+/,\n\t\t\t\tinside: {\n\t\t\t\t\t'namespace': /^[^\\s>\\/:]+:/\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n\t},\n\t'entity': /&#?[\\da-z]{1,8};/i\n};\n\n// Plugin to make entity title show the real entity, idea by Roman Komarov\nPrism.hooks.add('wrap', function(env) {\n\n\tif (env.type === 'entity') {\n\t\tenv.attributes['title'] = env.content.replace(/&amp;/, '&');\n\t}\n});\n\nPrism.languages.xml = Prism.languages.markup;\nPrism.languages.html = Prism.languages.markup;\nPrism.languages.mathml = Prism.languages.markup;\nPrism.languages.svg = Prism.languages.markup;\n\n\n/* **********************************************\n     Begin prism-css.js\n********************************************** */\n\nPrism.languages.css = {\n\t'comment': /\\/\\*[\\w\\W]*?\\*\\//,\n\t'atrule': {\n\t\tpattern: /@[\\w-]+?.*?(;|(?=\\s*\\{))/i,\n\t\tinside: {\n\t\t\t'rule': /@[\\w-]+/\n\t\t\t// See rest below\n\t\t}\n\t},\n\t'url': /url\\((?:([\"'])(\\\\(?:\\r\\n|[\\w\\W])|(?!\\1)[^\\\\\\r\\n])*\\1|.*?)\\)/i,\n\t'selector': /[^\\{\\}\\s][^\\{\\};]*?(?=\\s*\\{)/,\n\t'string': {\n\t\tpattern: /(\"|')(\\\\(?:\\r\\n|[\\w\\W])|(?!\\1)[^\\\\\\r\\n])*\\1/,\n\t\tgreedy: true\n\t},\n\t'property': /(\\b|\\B)[\\w-]+(?=\\s*:)/i,\n\t'important': /\\B!important\\b/i,\n\t'function': /[-a-z0-9]+(?=\\()/i,\n\t'punctuation': /[(){};:]/\n};\n\nPrism.languages.css['atrule'].inside.rest = Prism.util.clone(Prism.languages.css);\n\nif (Prism.languages.markup) {\n\tPrism.languages.insertBefore('markup', 'tag', {\n\t\t'style': {\n\t\t\tpattern: /(<style[\\w\\W]*?>)[\\w\\W]*?(?=<\\/style>)/i,\n\t\t\tlookbehind: true,\n\t\t\tinside: Prism.languages.css,\n\t\t\talias: 'language-css'\n\t\t}\n\t});\n\t\n\tPrism.languages.insertBefore('inside', 'attr-value', {\n\t\t'style-attr': {\n\t\t\tpattern: /\\s*style=(\"|').*?\\1/i,\n\t\t\tinside: {\n\t\t\t\t'attr-name': {\n\t\t\t\t\tpattern: /^\\s*style/i,\n\t\t\t\t\tinside: Prism.languages.markup.tag.inside\n\t\t\t\t},\n\t\t\t\t'punctuation': /^\\s*=\\s*['\"]|['\"]\\s*$/,\n\t\t\t\t'attr-value': {\n\t\t\t\t\tpattern: /.+/i,\n\t\t\t\t\tinside: Prism.languages.css\n\t\t\t\t}\n\t\t\t},\n\t\t\talias: 'language-css'\n\t\t}\n\t}, Prism.languages.markup.tag);\n}\n\n/* **********************************************\n     Begin prism-clike.js\n********************************************** */\n\nPrism.languages.clike = {\n\t'comment': [\n\t\t{\n\t\t\tpattern: /(^|[^\\\\])\\/\\*[\\w\\W]*?\\*\\//,\n\t\t\tlookbehind: true\n\t\t},\n\t\t{\n\t\t\tpattern: /(^|[^\\\\:])\\/\\/.*/,\n\t\t\tlookbehind: true\n\t\t}\n\t],\n\t'string': {\n\t\tpattern: /([\"'])(\\\\(?:\\r\\n|[\\s\\S])|(?!\\1)[^\\\\\\r\\n])*\\1/,\n\t\tgreedy: true\n\t},\n\t'class-name': {\n\t\tpattern: /((?:\\b(?:class|interface|extends|implements|trait|instanceof|new)\\s+)|(?:catch\\s+\\())[a-z0-9_\\.\\\\]+/i,\n\t\tlookbehind: true,\n\t\tinside: {\n\t\t\tpunctuation: /(\\.|\\\\)/\n\t\t}\n\t},\n\t'keyword': /\\b(if|else|while|do|for|return|in|instanceof|function|new|try|throw|catch|finally|null|break|continue)\\b/,\n\t'boolean': /\\b(true|false)\\b/,\n\t'function': /[a-z0-9_]+(?=\\()/i,\n\t'number': /\\b-?(?:0x[\\da-f]+|\\d*\\.?\\d+(?:e[+-]?\\d+)?)\\b/i,\n\t'operator': /--?|\\+\\+?|!=?=?|<=?|>=?|==?=?|&&?|\\|\\|?|\\?|\\*|\\/|~|\\^|%/,\n\t'punctuation': /[{}[\\];(),.:]/\n};\n\n\n/* **********************************************\n     Begin prism-javascript.js\n********************************************** */\n\nPrism.languages.javascript = Prism.languages.extend('clike', {\n\t'keyword': /\\b(as|async|await|break|case|catch|class|const|continue|debugger|default|delete|do|else|enum|export|extends|finally|for|from|function|get|if|implements|import|in|instanceof|interface|let|new|null|of|package|private|protected|public|return|set|static|super|switch|this|throw|try|typeof|var|void|while|with|yield)\\b/,\n\t'number': /\\b-?(0x[\\dA-Fa-f]+|0b[01]+|0o[0-7]+|\\d*\\.?\\d+([Ee][+-]?\\d+)?|NaN|Infinity)\\b/,\n\t// Allow for all non-ASCII characters (See http://stackoverflow.com/a/2008444)\n\t'function': /[_$a-zA-Z\\xA0-\\uFFFF][_$a-zA-Z0-9\\xA0-\\uFFFF]*(?=\\()/i,\n\t'operator': /--?|\\+\\+?|!=?=?|<=?|>=?|==?=?|&&?|\\|\\|?|\\?|\\*\\*?|\\/|~|\\^|%|\\.{3}/\n});\n\nPrism.languages.insertBefore('javascript', 'keyword', {\n\t'regex': {\n\t\tpattern: /(^|[^/])\\/(?!\\/)(\\[.+?]|\\\\.|[^/\\\\\\r\\n])+\\/[gimyu]{0,5}(?=\\s*($|[\\r\\n,.;})]))/,\n\t\tlookbehind: true,\n\t\tgreedy: true\n\t}\n});\n\nPrism.languages.insertBefore('javascript', 'string', {\n\t'template-string': {\n\t\tpattern: /`(?:\\\\\\\\|\\\\?[^\\\\])*?`/,\n\t\tgreedy: true,\n\t\tinside: {\n\t\t\t'interpolation': {\n\t\t\t\tpattern: /\\$\\{[^}]+\\}/,\n\t\t\t\tinside: {\n\t\t\t\t\t'interpolation-punctuation': {\n\t\t\t\t\t\tpattern: /^\\$\\{|\\}$/,\n\t\t\t\t\t\talias: 'punctuation'\n\t\t\t\t\t},\n\t\t\t\t\trest: Prism.languages.javascript\n\t\t\t\t}\n\t\t\t},\n\t\t\t'string': /[\\s\\S]+/\n\t\t}\n\t}\n});\n\nif (Prism.languages.markup) {\n\tPrism.languages.insertBefore('markup', 'tag', {\n\t\t'script': {\n\t\t\tpattern: /(<script[\\w\\W]*?>)[\\w\\W]*?(?=<\\/script>)/i,\n\t\t\tlookbehind: true,\n\t\t\tinside: Prism.languages.javascript,\n\t\t\talias: 'language-javascript'\n\t\t}\n\t});\n}\n\nPrism.languages.js = Prism.languages.javascript;\n\n/* **********************************************\n     Begin prism-file-highlight.js\n********************************************** */\n\n(function () {\n\tif (typeof self === 'undefined' || !self.Prism || !self.document || !document.querySelector) {\n\t\treturn;\n\t}\n\n\tself.Prism.fileHighlight = function() {\n\n\t\tvar Extensions = {\n\t\t\t'js': 'javascript',\n\t\t\t'py': 'python',\n\t\t\t'rb': 'ruby',\n\t\t\t'ps1': 'powershell',\n\t\t\t'psm1': 'powershell',\n\t\t\t'sh': 'bash',\n\t\t\t'bat': 'batch',\n\t\t\t'h': 'c',\n\t\t\t'tex': 'latex'\n\t\t};\n\n\t\tif(Array.prototype.forEach) { // Check to prevent error in IE8\n\t\t\tArray.prototype.slice.call(document.querySelectorAll('pre[data-src]')).forEach(function (pre) {\n\t\t\t\tvar src = pre.getAttribute('data-src');\n\n\t\t\t\tvar language, parent = pre;\n\t\t\t\tvar lang = /\\blang(?:uage)?-(?!\\*)(\\w+)\\b/i;\n\t\t\t\twhile (parent && !lang.test(parent.className)) {\n\t\t\t\t\tparent = parent.parentNode;\n\t\t\t\t}\n\n\t\t\t\tif (parent) {\n\t\t\t\t\tlanguage = (pre.className.match(lang) || [, ''])[1];\n\t\t\t\t}\n\n\t\t\t\tif (!language) {\n\t\t\t\t\tvar extension = (src.match(/\\.(\\w+)$/) || [, ''])[1];\n\t\t\t\t\tlanguage = Extensions[extension] || extension;\n\t\t\t\t}\n\n\t\t\t\tvar code = document.createElement('code');\n\t\t\t\tcode.className = 'language-' + language;\n\n\t\t\t\tpre.textContent = '';\n\n\t\t\t\tcode.textContent = 'Loading…';\n\n\t\t\t\tpre.appendChild(code);\n\n\t\t\t\tvar xhr = new XMLHttpRequest();\n\n\t\t\t\txhr.open('GET', src, true);\n\n\t\t\t\txhr.onreadystatechange = function () {\n\t\t\t\t\tif (xhr.readyState == 4) {\n\n\t\t\t\t\t\tif (xhr.status < 400 && xhr.responseText) {\n\t\t\t\t\t\t\tcode.textContent = xhr.responseText;\n\n\t\t\t\t\t\t\tPrism.highlightElement(code);\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse if (xhr.status >= 400) {\n\t\t\t\t\t\t\tcode.textContent = '✖ Error ' + xhr.status + ' while fetching file: ' + xhr.statusText;\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\tcode.textContent = '✖ Error: File does not exist or is empty';\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t};\n\n\t\t\t\txhr.send(null);\n\t\t\t});\n\t\t}\n\n\t};\n\n\tdocument.addEventListener('DOMContentLoaded', self.Prism.fileHighlight);\n\n})();\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/prismjs/prism.js\n// module id = 19\n// module chunks = 0 1","exports = module.exports = require(\"../../css-loader/lib/css-base.js\")(true);\n// imports\n\n\n// module\nexports.push([module.id, \"code[class*=language-],pre[class*=language-]{color:#000;background:none;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{position:relative;margin:.5em 0;box-shadow:-1px 0 0 0 #358ccb,0 0 0 1px #dfdfdf;border-left:10px solid #358ccb;background-color:#fdfdfd;background-image:linear-gradient(transparent 50%,rgba(69,142,209,.04) 0);background-size:3em 3em;background-origin:content-box;overflow:visible;padding:0}code[class*=language]{max-height:inherit;height:100%;padding:0 1em;display:block;overflow:auto}:not(pre)>code[class*=language-],pre[class*=language-]{background-color:#fdfdfd;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;margin-bottom:1em}:not(pre)>code[class*=language-]{position:relative;padding:.2em;border-radius:.3em;color:#c92c2c;border:1px solid rgba(0,0,0,.1);display:inline;white-space:normal}pre[class*=language-]:after,pre[class*=language-]:before{content:\\\"\\\";z-index:-2;display:block;position:absolute;bottom:.75em;left:.18em;width:40%;height:20%;max-height:13em;box-shadow:0 13px 8px #979797;-webkit-transform:rotate(-2deg);-moz-transform:rotate(-2deg);-ms-transform:rotate(-2deg);-o-transform:rotate(-2deg);transform:rotate(-2deg)}:not(pre)>code[class*=language-]:after,pre[class*=language-]:after{right:.75em;left:auto;-webkit-transform:rotate(2deg);-moz-transform:rotate(2deg);-ms-transform:rotate(2deg);-o-transform:rotate(2deg);transform:rotate(2deg)}.token.block-comment,.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#7d8b99}.token.punctuation{color:#5f6364}.token.boolean,.token.constant,.token.deleted,.token.function-name,.token.number,.token.property,.token.symbol,.token.tag{color:#c92c2c}.token.attr-name,.token.builtin,.token.char,.token.function,.token.inserted,.token.selector,.token.string{color:#2f9c0a}.token.entity,.token.operator,.token.url,.token.variable{color:#a67f59;background:hsla(0,0%,100%,.5)}.token.atrule,.token.attr-value,.token.class-name,.token.keyword{color:#1990b8}.token.important,.token.regex{color:#e90}.language-css .token.string,.style .token.string{color:#a67f59;background:hsla(0,0%,100%,.5)}.token.important{font-weight:400}.token.bold{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}.namespace{opacity:.7}@media screen and (max-width:767px){pre[class*=language-]:after,pre[class*=language-]:before{bottom:14px;box-shadow:none}}.token.cr:before,.token.lf:before,.token.tab:not(:empty):before{color:#e0d7d1}pre[class*=language-].line-numbers{padding-left:0}pre[class*=language-].line-numbers code{padding-left:3.8em}pre[class*=language-].line-numbers .line-numbers-rows{left:0}pre[class*=language-][data-line]{padding-top:0;padding-bottom:0;padding-left:0}pre[data-line] code{position:relative;padding-left:4em}pre .line-highlight{margin-top:0}\", \"\", {\"version\":3,\"sources\":[\"F:/Github/newvue/node_modules/prismjs/themes/prism-coy.css\"],\"names\":[],\"mappings\":\"AAMA,6CAEC,WAAa,AACb,gBAAiB,AACjB,8DAAuE,AACvE,gBAAiB,AACjB,gBAAiB,AACjB,oBAAqB,AACrB,kBAAmB,AACnB,iBAAkB,AAClB,gBAAiB,AAEjB,gBAAiB,AACjB,cAAe,AACf,WAAY,AAEZ,qBAAsB,AACtB,kBAAmB,AACnB,iBAAkB,AAClB,YAAc,CACd,AAGD,sBACC,kBAAmB,AACnB,cAAe,AACf,gDAA8D,AAC9D,+BAAgC,AAChC,yBAA0B,AAC1B,yEAAiF,AACjF,wBAAyB,AACzB,8BAA+B,AAC/B,iBAAkB,AAClB,SAAW,CACX,AAED,sBACC,mBAAoB,AACpB,YAAa,AACb,cAAe,AACf,cAAe,AACf,aAAe,CACf,AAGD,uDAEC,yBAA0B,AAC1B,8BAA+B,AAC/B,2BAA4B,AAC5B,sBAAuB,AACvB,iBAAmB,CACnB,AAGD,iCACC,kBAAmB,AACnB,aAAc,AACd,mBAAqB,AACrB,cAAe,AACf,gCAAqC,AACrC,eAAgB,AAChB,kBAAoB,CACpB,AAED,yDAEC,WAAY,AACZ,WAAY,AACZ,cAAe,AACf,kBAAmB,AACnB,aAAe,AACf,WAAa,AACb,UAAW,AACX,WAAY,AACZ,gBAAiB,AACjB,8BAAiC,AACjC,gCAAiC,AACjC,6BAA8B,AAC9B,4BAA6B,AAC7B,2BAA4B,AAC5B,uBAAyB,CACzB,AAED,mEAEC,YAAc,AACd,UAAW,AACX,+BAAgC,AAChC,4BAA6B,AAC7B,2BAA4B,AAC5B,0BAA2B,AAC3B,sBAAwB,CACxB,AAED,8EAKC,aAAe,CACf,AAED,mBACC,aAAe,CACf,AAED,0HAQC,aAAe,CACf,AAED,0GAOC,aAAe,CACf,AAED,yDAIC,cAAe,AACf,6BAAqC,CACrC,AAED,iEAIC,aAAe,CACf,AAED,8BAEC,UAAY,CACZ,AAED,iDAEC,cAAe,AACf,6BAAqC,CACrC,AAED,iBACC,eAAoB,CACpB,AAED,YACC,eAAkB,CAClB,AACD,cACC,iBAAmB,CACnB,AAED,cACC,WAAa,CACb,AAED,WACC,UAAY,CACZ,AAED,oCACC,yDAEC,YAAa,AACb,eAAiB,CACjB,CAED,AAGD,gEAGC,aAAe,CACf,AAGD,mCACC,cAAgB,CAChB,AAED,wCACC,kBAAoB,CACpB,AAED,sDACC,MAAQ,CACR,AAGD,iCACC,cAAe,AACf,iBAAkB,AAClB,cAAgB,CAChB,AACD,oBACC,kBAAmB,AACnB,gBAAkB,CAClB,AACD,oBACC,YAAc,CACd\",\"file\":\"prism-coy.css\",\"sourcesContent\":[\"/**\\n * prism.js Coy theme for JavaScript, CoffeeScript, CSS and HTML\\n * Based on https://github.com/tshedor/workshop-wp-theme (Example: http://workshop.kansan.com/category/sessions/basics or http://workshop.timshedor.com/category/sessions/basics);\\n * @author Tim  Shedor\\n */\\n\\ncode[class*=\\\"language-\\\"],\\npre[class*=\\\"language-\\\"] {\\n\\tcolor: black;\\n\\tbackground: none;\\n\\tfont-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;\\n\\ttext-align: left;\\n\\twhite-space: pre;\\n\\tword-spacing: normal;\\n\\tword-break: normal;\\n\\tword-wrap: normal;\\n\\tline-height: 1.5;\\n\\n\\t-moz-tab-size: 4;\\n\\t-o-tab-size: 4;\\n\\ttab-size: 4;\\n\\n\\t-webkit-hyphens: none;\\n\\t-moz-hyphens: none;\\n\\t-ms-hyphens: none;\\n\\thyphens: none;\\n}\\n\\n/* Code blocks */\\npre[class*=\\\"language-\\\"] {\\n\\tposition: relative;\\n\\tmargin: .5em 0;\\n\\tbox-shadow: -1px 0px 0px 0px #358ccb, 0px 0px 0px 1px #dfdfdf;\\n\\tborder-left: 10px solid #358ccb;\\n\\tbackground-color: #fdfdfd;\\n\\tbackground-image: linear-gradient(transparent 50%, rgba(69, 142, 209, 0.04) 50%);\\n\\tbackground-size: 3em 3em;\\n\\tbackground-origin: content-box;\\n\\toverflow: visible;\\n\\tpadding: 0;\\n}\\n\\ncode[class*=\\\"language\\\"] {\\n\\tmax-height: inherit;\\n\\theight: 100%;\\n\\tpadding: 0 1em;\\n\\tdisplay: block;\\n\\toverflow: auto;\\n}\\n\\n/* Margin bottom to accomodate shadow */\\n:not(pre) > code[class*=\\\"language-\\\"],\\npre[class*=\\\"language-\\\"] {\\n\\tbackground-color: #fdfdfd;\\n\\t-webkit-box-sizing: border-box;\\n\\t-moz-box-sizing: border-box;\\n\\tbox-sizing: border-box;\\n\\tmargin-bottom: 1em;\\n}\\n\\n/* Inline code */\\n:not(pre) > code[class*=\\\"language-\\\"] {\\n\\tposition: relative;\\n\\tpadding: .2em;\\n\\tborder-radius: 0.3em;\\n\\tcolor: #c92c2c;\\n\\tborder: 1px solid rgba(0, 0, 0, 0.1);\\n\\tdisplay: inline;\\n\\twhite-space: normal;\\n}\\n\\npre[class*=\\\"language-\\\"]:before,\\npre[class*=\\\"language-\\\"]:after {\\n\\tcontent: '';\\n\\tz-index: -2;\\n\\tdisplay: block;\\n\\tposition: absolute;\\n\\tbottom: 0.75em;\\n\\tleft: 0.18em;\\n\\twidth: 40%;\\n\\theight: 20%;\\n\\tmax-height: 13em;\\n\\tbox-shadow: 0px 13px 8px #979797;\\n\\t-webkit-transform: rotate(-2deg);\\n\\t-moz-transform: rotate(-2deg);\\n\\t-ms-transform: rotate(-2deg);\\n\\t-o-transform: rotate(-2deg);\\n\\ttransform: rotate(-2deg);\\n}\\n\\n:not(pre) > code[class*=\\\"language-\\\"]:after,\\npre[class*=\\\"language-\\\"]:after {\\n\\tright: 0.75em;\\n\\tleft: auto;\\n\\t-webkit-transform: rotate(2deg);\\n\\t-moz-transform: rotate(2deg);\\n\\t-ms-transform: rotate(2deg);\\n\\t-o-transform: rotate(2deg);\\n\\ttransform: rotate(2deg);\\n}\\n\\n.token.comment,\\n.token.block-comment,\\n.token.prolog,\\n.token.doctype,\\n.token.cdata {\\n\\tcolor: #7D8B99;\\n}\\n\\n.token.punctuation {\\n\\tcolor: #5F6364;\\n}\\n\\n.token.property,\\n.token.tag,\\n.token.boolean,\\n.token.number,\\n.token.function-name,\\n.token.constant,\\n.token.symbol,\\n.token.deleted {\\n\\tcolor: #c92c2c;\\n}\\n\\n.token.selector,\\n.token.attr-name,\\n.token.string,\\n.token.char,\\n.token.function,\\n.token.builtin,\\n.token.inserted {\\n\\tcolor: #2f9c0a;\\n}\\n\\n.token.operator,\\n.token.entity,\\n.token.url,\\n.token.variable {\\n\\tcolor: #a67f59;\\n\\tbackground: rgba(255, 255, 255, 0.5);\\n}\\n\\n.token.atrule,\\n.token.attr-value,\\n.token.keyword,\\n.token.class-name {\\n\\tcolor: #1990b8;\\n}\\n\\n.token.regex,\\n.token.important {\\n\\tcolor: #e90;\\n}\\n\\n.language-css .token.string,\\n.style .token.string {\\n\\tcolor: #a67f59;\\n\\tbackground: rgba(255, 255, 255, 0.5);\\n}\\n\\n.token.important {\\n\\tfont-weight: normal;\\n}\\n\\n.token.bold {\\n\\tfont-weight: bold;\\n}\\n.token.italic {\\n\\tfont-style: italic;\\n}\\n\\n.token.entity {\\n\\tcursor: help;\\n}\\n\\n.namespace {\\n\\topacity: .7;\\n}\\n\\n@media screen and (max-width: 767px) {\\n\\tpre[class*=\\\"language-\\\"]:before,\\n\\tpre[class*=\\\"language-\\\"]:after {\\n\\t\\tbottom: 14px;\\n\\t\\tbox-shadow: none;\\n\\t}\\n\\n}\\n\\n/* Plugin styles */\\n.token.tab:not(:empty):before,\\n.token.cr:before,\\n.token.lf:before {\\n\\tcolor: #e0d7d1;\\n}\\n\\n/* Plugin styles: Line Numbers */\\npre[class*=\\\"language-\\\"].line-numbers {\\n\\tpadding-left: 0;\\n}\\n\\npre[class*=\\\"language-\\\"].line-numbers code {\\n\\tpadding-left: 3.8em;\\n}\\n\\npre[class*=\\\"language-\\\"].line-numbers .line-numbers-rows {\\n\\tleft: 0;\\n}\\n\\n/* Plugin styles: Line Highlight */\\npre[class*=\\\"language-\\\"][data-line] {\\n\\tpadding-top: 0;\\n\\tpadding-bottom: 0;\\n\\tpadding-left: 0;\\n}\\npre[data-line] code {\\n\\tposition: relative;\\n\\tpadding-left: 4em;\\n}\\npre .line-highlight {\\n\\tmargin-top: 0;\\n}\\n\"],\"sourceRoot\":\"\"}]);\n\n// exports\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/css-loader?{\"minimize\":true,\"sourceMap\":true}!./~/prismjs/themes/prism-coy.css\n// module id = 20\n// module chunks = 0 1","// style-loader: Adds some css to the DOM by adding a <style> tag\n\n// load the styles\nvar content = require(\"!!../../css-loader/index.js??ref--5-2!./prism-coy.css\");\nif(typeof content === 'string') content = [[module.id, content, '']];\nif(content.locals) module.exports = content.locals;\n// add the styles to the DOM\nvar update = require(\"!../../vue-style-loader/lib/addStylesClient.js\")(\"1fcd7648\", content, true);\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/prismjs/themes/prism-coy.css\n// module id = 21\n// module chunks = 0 1","/**\n * marked - a markdown parser\n * Copyright (c) 2011-2014, Christopher Jeffrey. (MIT Licensed)\n * https://github.com/chjj/marked\n */\n\n;(function() {\n\n/**\n * Block-Level Grammar\n */\n\nvar block = {\n  newline: /^\\n+/,\n  code: /^( {4}[^\\n]+\\n*)+/,\n  fences: noop,\n  hr: /^( *[-*_]){3,} *(?:\\n+|$)/,\n  heading: /^ *(#{1,6}) *([^\\n]+?) *#* *(?:\\n+|$)/,\n  nptable: noop,\n  lheading: /^([^\\n]+)\\n *(=|-){2,} *(?:\\n+|$)/,\n  blockquote: /^( *>[^\\n]+(\\n(?!def)[^\\n]+)*\\n*)+/,\n  list: /^( *)(bull) [\\s\\S]+?(?:hr|def|\\n{2,}(?! )(?!\\1bull )\\n*|\\s*$)/,\n  html: /^ *(?:comment *(?:\\n|\\s*$)|closed *(?:\\n{2,}|\\s*$)|closing *(?:\\n{2,}|\\s*$))/,\n  def: /^ *\\[([^\\]]+)\\]: *<?([^\\s>]+)>?(?: +[\"(]([^\\n]+)[\")])? *(?:\\n+|$)/,\n  table: noop,\n  paragraph: /^((?:[^\\n]+\\n?(?!hr|heading|lheading|blockquote|tag|def))+)\\n*/,\n  text: /^[^\\n]+/\n};\n\nblock.bullet = /(?:[*+-]|\\d+\\.)/;\nblock.item = /^( *)(bull) [^\\n]*(?:\\n(?!\\1bull )[^\\n]*)*/;\nblock.item = replace(block.item, 'gm')\n  (/bull/g, block.bullet)\n  ();\n\nblock.list = replace(block.list)\n  (/bull/g, block.bullet)\n  ('hr', '\\\\n+(?=\\\\1?(?:[-*_] *){3,}(?:\\\\n+|$))')\n  ('def', '\\\\n+(?=' + block.def.source + ')')\n  ();\n\nblock.blockquote = replace(block.blockquote)\n  ('def', block.def)\n  ();\n\nblock._tag = '(?!(?:'\n  + 'a|em|strong|small|s|cite|q|dfn|abbr|data|time|code'\n  + '|var|samp|kbd|sub|sup|i|b|u|mark|ruby|rt|rp|bdi|bdo'\n  + '|span|br|wbr|ins|del|img)\\\\b)\\\\w+(?!:/|[^\\\\w\\\\s@]*@)\\\\b';\n\nblock.html = replace(block.html)\n  ('comment', /<!--[\\s\\S]*?-->/)\n  ('closed', /<(tag)[\\s\\S]+?<\\/\\1>/)\n  ('closing', /<tag(?:\"[^\"]*\"|'[^']*'|[^'\">])*?>/)\n  (/tag/g, block._tag)\n  ();\n\nblock.paragraph = replace(block.paragraph)\n  ('hr', block.hr)\n  ('heading', block.heading)\n  ('lheading', block.lheading)\n  ('blockquote', block.blockquote)\n  ('tag', '<' + block._tag)\n  ('def', block.def)\n  ();\n\n/**\n * Normal Block Grammar\n */\n\nblock.normal = merge({}, block);\n\n/**\n * GFM Block Grammar\n */\n\nblock.gfm = merge({}, block.normal, {\n  fences: /^ *(`{3,}|~{3,})[ \\.]*(\\S+)? *\\n([\\s\\S]*?)\\s*\\1 *(?:\\n+|$)/,\n  paragraph: /^/,\n  heading: /^ *(#{1,6}) +([^\\n]+?) *#* *(?:\\n+|$)/\n});\n\nblock.gfm.paragraph = replace(block.paragraph)\n  ('(?!', '(?!'\n    + block.gfm.fences.source.replace('\\\\1', '\\\\2') + '|'\n    + block.list.source.replace('\\\\1', '\\\\3') + '|')\n  ();\n\n/**\n * GFM + Tables Block Grammar\n */\n\nblock.tables = merge({}, block.gfm, {\n  nptable: /^ *(\\S.*\\|.*)\\n *([-:]+ *\\|[-| :]*)\\n((?:.*\\|.*(?:\\n|$))*)\\n*/,\n  table: /^ *\\|(.+)\\n *\\|( *[-:]+[-| :]*)\\n((?: *\\|.*(?:\\n|$))*)\\n*/\n});\n\n/**\n * Block Lexer\n */\n\nfunction Lexer(options) {\n  this.tokens = [];\n  this.tokens.links = {};\n  this.options = options || marked.defaults;\n  this.rules = block.normal;\n\n  if (this.options.gfm) {\n    if (this.options.tables) {\n      this.rules = block.tables;\n    } else {\n      this.rules = block.gfm;\n    }\n  }\n}\n\n/**\n * Expose Block Rules\n */\n\nLexer.rules = block;\n\n/**\n * Static Lex Method\n */\n\nLexer.lex = function(src, options) {\n  var lexer = new Lexer(options);\n  return lexer.lex(src);\n};\n\n/**\n * Preprocessing\n */\n\nLexer.prototype.lex = function(src) {\n  src = src\n    .replace(/\\r\\n|\\r/g, '\\n')\n    .replace(/\\t/g, '    ')\n    .replace(/\\u00a0/g, ' ')\n    .replace(/\\u2424/g, '\\n');\n\n  return this.token(src, true);\n};\n\n/**\n * Lexing\n */\n\nLexer.prototype.token = function(src, top, bq) {\n  var src = src.replace(/^ +$/gm, '')\n    , next\n    , loose\n    , cap\n    , bull\n    , b\n    , item\n    , space\n    , i\n    , l;\n\n  while (src) {\n    // newline\n    if (cap = this.rules.newline.exec(src)) {\n      src = src.substring(cap[0].length);\n      if (cap[0].length > 1) {\n        this.tokens.push({\n          type: 'space'\n        });\n      }\n    }\n\n    // code\n    if (cap = this.rules.code.exec(src)) {\n      src = src.substring(cap[0].length);\n      cap = cap[0].replace(/^ {4}/gm, '');\n      this.tokens.push({\n        type: 'code',\n        text: !this.options.pedantic\n          ? cap.replace(/\\n+$/, '')\n          : cap\n      });\n      continue;\n    }\n\n    // fences (gfm)\n    if (cap = this.rules.fences.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'code',\n        lang: cap[2],\n        text: cap[3] || ''\n      });\n      continue;\n    }\n\n    // heading\n    if (cap = this.rules.heading.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'heading',\n        depth: cap[1].length,\n        text: cap[2]\n      });\n      continue;\n    }\n\n    // table no leading pipe (gfm)\n    if (top && (cap = this.rules.nptable.exec(src))) {\n      src = src.substring(cap[0].length);\n\n      item = {\n        type: 'table',\n        header: cap[1].replace(/^ *| *\\| *$/g, '').split(/ *\\| */),\n        align: cap[2].replace(/^ *|\\| *$/g, '').split(/ *\\| */),\n        cells: cap[3].replace(/\\n$/, '').split('\\n')\n      };\n\n      for (i = 0; i < item.align.length; i++) {\n        if (/^ *-+: *$/.test(item.align[i])) {\n          item.align[i] = 'right';\n        } else if (/^ *:-+: *$/.test(item.align[i])) {\n          item.align[i] = 'center';\n        } else if (/^ *:-+ *$/.test(item.align[i])) {\n          item.align[i] = 'left';\n        } else {\n          item.align[i] = null;\n        }\n      }\n\n      for (i = 0; i < item.cells.length; i++) {\n        item.cells[i] = item.cells[i].split(/ *\\| */);\n      }\n\n      this.tokens.push(item);\n\n      continue;\n    }\n\n    // lheading\n    if (cap = this.rules.lheading.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'heading',\n        depth: cap[2] === '=' ? 1 : 2,\n        text: cap[1]\n      });\n      continue;\n    }\n\n    // hr\n    if (cap = this.rules.hr.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'hr'\n      });\n      continue;\n    }\n\n    // blockquote\n    if (cap = this.rules.blockquote.exec(src)) {\n      src = src.substring(cap[0].length);\n\n      this.tokens.push({\n        type: 'blockquote_start'\n      });\n\n      cap = cap[0].replace(/^ *> ?/gm, '');\n\n      // Pass `top` to keep the current\n      // \"toplevel\" state. This is exactly\n      // how markdown.pl works.\n      this.token(cap, top, true);\n\n      this.tokens.push({\n        type: 'blockquote_end'\n      });\n\n      continue;\n    }\n\n    // list\n    if (cap = this.rules.list.exec(src)) {\n      src = src.substring(cap[0].length);\n      bull = cap[2];\n\n      this.tokens.push({\n        type: 'list_start',\n        ordered: bull.length > 1\n      });\n\n      // Get each top-level item.\n      cap = cap[0].match(this.rules.item);\n\n      next = false;\n      l = cap.length;\n      i = 0;\n\n      for (; i < l; i++) {\n        item = cap[i];\n\n        // Remove the list item's bullet\n        // so it is seen as the next token.\n        space = item.length;\n        item = item.replace(/^ *([*+-]|\\d+\\.) +/, '');\n\n        // Outdent whatever the\n        // list item contains. Hacky.\n        if (~item.indexOf('\\n ')) {\n          space -= item.length;\n          item = !this.options.pedantic\n            ? item.replace(new RegExp('^ {1,' + space + '}', 'gm'), '')\n            : item.replace(/^ {1,4}/gm, '');\n        }\n\n        // Determine whether the next list item belongs here.\n        // Backpedal if it does not belong in this list.\n        if (this.options.smartLists && i !== l - 1) {\n          b = block.bullet.exec(cap[i + 1])[0];\n          if (bull !== b && !(bull.length > 1 && b.length > 1)) {\n            src = cap.slice(i + 1).join('\\n') + src;\n            i = l - 1;\n          }\n        }\n\n        // Determine whether item is loose or not.\n        // Use: /(^|\\n)(?! )[^\\n]+\\n\\n(?!\\s*$)/\n        // for discount behavior.\n        loose = next || /\\n\\n(?!\\s*$)/.test(item);\n        if (i !== l - 1) {\n          next = item.charAt(item.length - 1) === '\\n';\n          if (!loose) loose = next;\n        }\n\n        this.tokens.push({\n          type: loose\n            ? 'loose_item_start'\n            : 'list_item_start'\n        });\n\n        // Recurse.\n        this.token(item, false, bq);\n\n        this.tokens.push({\n          type: 'list_item_end'\n        });\n      }\n\n      this.tokens.push({\n        type: 'list_end'\n      });\n\n      continue;\n    }\n\n    // html\n    if (cap = this.rules.html.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: this.options.sanitize\n          ? 'paragraph'\n          : 'html',\n        pre: !this.options.sanitizer\n          && (cap[1] === 'pre' || cap[1] === 'script' || cap[1] === 'style'),\n        text: cap[0]\n      });\n      continue;\n    }\n\n    // def\n    if ((!bq && top) && (cap = this.rules.def.exec(src))) {\n      src = src.substring(cap[0].length);\n      this.tokens.links[cap[1].toLowerCase()] = {\n        href: cap[2],\n        title: cap[3]\n      };\n      continue;\n    }\n\n    // table (gfm)\n    if (top && (cap = this.rules.table.exec(src))) {\n      src = src.substring(cap[0].length);\n\n      item = {\n        type: 'table',\n        header: cap[1].replace(/^ *| *\\| *$/g, '').split(/ *\\| */),\n        align: cap[2].replace(/^ *|\\| *$/g, '').split(/ *\\| */),\n        cells: cap[3].replace(/(?: *\\| *)?\\n$/, '').split('\\n')\n      };\n\n      for (i = 0; i < item.align.length; i++) {\n        if (/^ *-+: *$/.test(item.align[i])) {\n          item.align[i] = 'right';\n        } else if (/^ *:-+: *$/.test(item.align[i])) {\n          item.align[i] = 'center';\n        } else if (/^ *:-+ *$/.test(item.align[i])) {\n          item.align[i] = 'left';\n        } else {\n          item.align[i] = null;\n        }\n      }\n\n      for (i = 0; i < item.cells.length; i++) {\n        item.cells[i] = item.cells[i]\n          .replace(/^ *\\| *| *\\| *$/g, '')\n          .split(/ *\\| */);\n      }\n\n      this.tokens.push(item);\n\n      continue;\n    }\n\n    // top-level paragraph\n    if (top && (cap = this.rules.paragraph.exec(src))) {\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'paragraph',\n        text: cap[1].charAt(cap[1].length - 1) === '\\n'\n          ? cap[1].slice(0, -1)\n          : cap[1]\n      });\n      continue;\n    }\n\n    // text\n    if (cap = this.rules.text.exec(src)) {\n      // Top-level should never reach here.\n      src = src.substring(cap[0].length);\n      this.tokens.push({\n        type: 'text',\n        text: cap[0]\n      });\n      continue;\n    }\n\n    if (src) {\n      throw new\n        Error('Infinite loop on byte: ' + src.charCodeAt(0));\n    }\n  }\n\n  return this.tokens;\n};\n\n/**\n * Inline-Level Grammar\n */\n\nvar inline = {\n  escape: /^\\\\([\\\\`*{}\\[\\]()#+\\-.!_>])/,\n  autolink: /^<([^ >]+(@|:\\/)[^ >]+)>/,\n  url: noop,\n  tag: /^<!--[\\s\\S]*?-->|^<\\/?\\w+(?:\"[^\"]*\"|'[^']*'|[^'\">])*?>/,\n  link: /^!?\\[(inside)\\]\\(href\\)/,\n  reflink: /^!?\\[(inside)\\]\\s*\\[([^\\]]*)\\]/,\n  nolink: /^!?\\[((?:\\[[^\\]]*\\]|[^\\[\\]])*)\\]/,\n  strong: /^__([\\s\\S]+?)__(?!_)|^\\*\\*([\\s\\S]+?)\\*\\*(?!\\*)/,\n  em: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n  code: /^(`+)\\s*([\\s\\S]*?[^`])\\s*\\1(?!`)/,\n  br: /^ {2,}\\n(?!\\s*$)/,\n  del: noop,\n  text: /^[\\s\\S]+?(?=[\\\\<!\\[_*`]| {2,}\\n|$)/\n};\n\ninline._inside = /(?:\\[[^\\]]*\\]|[^\\[\\]]|\\](?=[^\\[]*\\]))*/;\ninline._href = /\\s*<?([\\s\\S]*?)>?(?:\\s+['\"]([\\s\\S]*?)['\"])?\\s*/;\n\ninline.link = replace(inline.link)\n  ('inside', inline._inside)\n  ('href', inline._href)\n  ();\n\ninline.reflink = replace(inline.reflink)\n  ('inside', inline._inside)\n  ();\n\n/**\n * Normal Inline Grammar\n */\n\ninline.normal = merge({}, inline);\n\n/**\n * Pedantic Inline Grammar\n */\n\ninline.pedantic = merge({}, inline.normal, {\n  strong: /^__(?=\\S)([\\s\\S]*?\\S)__(?!_)|^\\*\\*(?=\\S)([\\s\\S]*?\\S)\\*\\*(?!\\*)/,\n  em: /^_(?=\\S)([\\s\\S]*?\\S)_(?!_)|^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/\n});\n\n/**\n * GFM Inline Grammar\n */\n\ninline.gfm = merge({}, inline.normal, {\n  escape: replace(inline.escape)('])', '~|])')(),\n  url: /^(https?:\\/\\/[^\\s<]+[^<.,:;\"')\\]\\s])/,\n  del: /^~~(?=\\S)([\\s\\S]*?\\S)~~/,\n  text: replace(inline.text)\n    (']|', '~]|')\n    ('|', '|https?://|')\n    ()\n});\n\n/**\n * GFM + Line Breaks Inline Grammar\n */\n\ninline.breaks = merge({}, inline.gfm, {\n  br: replace(inline.br)('{2,}', '*')(),\n  text: replace(inline.gfm.text)('{2,}', '*')()\n});\n\n/**\n * Inline Lexer & Compiler\n */\n\nfunction InlineLexer(links, options) {\n  this.options = options || marked.defaults;\n  this.links = links;\n  this.rules = inline.normal;\n  this.renderer = this.options.renderer || new Renderer;\n  this.renderer.options = this.options;\n\n  if (!this.links) {\n    throw new\n      Error('Tokens array requires a `links` property.');\n  }\n\n  if (this.options.gfm) {\n    if (this.options.breaks) {\n      this.rules = inline.breaks;\n    } else {\n      this.rules = inline.gfm;\n    }\n  } else if (this.options.pedantic) {\n    this.rules = inline.pedantic;\n  }\n}\n\n/**\n * Expose Inline Rules\n */\n\nInlineLexer.rules = inline;\n\n/**\n * Static Lexing/Compiling Method\n */\n\nInlineLexer.output = function(src, links, options) {\n  var inline = new InlineLexer(links, options);\n  return inline.output(src);\n};\n\n/**\n * Lexing/Compiling\n */\n\nInlineLexer.prototype.output = function(src) {\n  var out = ''\n    , link\n    , text\n    , href\n    , cap;\n\n  while (src) {\n    // escape\n    if (cap = this.rules.escape.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += cap[1];\n      continue;\n    }\n\n    // autolink\n    if (cap = this.rules.autolink.exec(src)) {\n      src = src.substring(cap[0].length);\n      if (cap[2] === '@') {\n        text = cap[1].charAt(6) === ':'\n          ? this.mangle(cap[1].substring(7))\n          : this.mangle(cap[1]);\n        href = this.mangle('mailto:') + text;\n      } else {\n        text = escape(cap[1]);\n        href = text;\n      }\n      out += this.renderer.link(href, null, text);\n      continue;\n    }\n\n    // url (gfm)\n    if (!this.inLink && (cap = this.rules.url.exec(src))) {\n      src = src.substring(cap[0].length);\n      text = escape(cap[1]);\n      href = text;\n      out += this.renderer.link(href, null, text);\n      continue;\n    }\n\n    // tag\n    if (cap = this.rules.tag.exec(src)) {\n      if (!this.inLink && /^<a /i.test(cap[0])) {\n        this.inLink = true;\n      } else if (this.inLink && /^<\\/a>/i.test(cap[0])) {\n        this.inLink = false;\n      }\n      src = src.substring(cap[0].length);\n      out += this.options.sanitize\n        ? this.options.sanitizer\n          ? this.options.sanitizer(cap[0])\n          : escape(cap[0])\n        : cap[0]\n      continue;\n    }\n\n    // link\n    if (cap = this.rules.link.exec(src)) {\n      src = src.substring(cap[0].length);\n      this.inLink = true;\n      out += this.outputLink(cap, {\n        href: cap[2],\n        title: cap[3]\n      });\n      this.inLink = false;\n      continue;\n    }\n\n    // reflink, nolink\n    if ((cap = this.rules.reflink.exec(src))\n        || (cap = this.rules.nolink.exec(src))) {\n      src = src.substring(cap[0].length);\n      link = (cap[2] || cap[1]).replace(/\\s+/g, ' ');\n      link = this.links[link.toLowerCase()];\n      if (!link || !link.href) {\n        out += cap[0].charAt(0);\n        src = cap[0].substring(1) + src;\n        continue;\n      }\n      this.inLink = true;\n      out += this.outputLink(cap, link);\n      this.inLink = false;\n      continue;\n    }\n\n    // strong\n    if (cap = this.rules.strong.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.strong(this.output(cap[2] || cap[1]));\n      continue;\n    }\n\n    // em\n    if (cap = this.rules.em.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.em(this.output(cap[2] || cap[1]));\n      continue;\n    }\n\n    // code\n    if (cap = this.rules.code.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.codespan(escape(cap[2], true));\n      continue;\n    }\n\n    // br\n    if (cap = this.rules.br.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.br();\n      continue;\n    }\n\n    // del (gfm)\n    if (cap = this.rules.del.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.del(this.output(cap[1]));\n      continue;\n    }\n\n    // text\n    if (cap = this.rules.text.exec(src)) {\n      src = src.substring(cap[0].length);\n      out += this.renderer.text(escape(this.smartypants(cap[0])));\n      continue;\n    }\n\n    if (src) {\n      throw new\n        Error('Infinite loop on byte: ' + src.charCodeAt(0));\n    }\n  }\n\n  return out;\n};\n\n/**\n * Compile Link\n */\n\nInlineLexer.prototype.outputLink = function(cap, link) {\n  var href = escape(link.href)\n    , title = link.title ? escape(link.title) : null;\n\n  return cap[0].charAt(0) !== '!'\n    ? this.renderer.link(href, title, this.output(cap[1]))\n    : this.renderer.image(href, title, escape(cap[1]));\n};\n\n/**\n * Smartypants Transformations\n */\n\nInlineLexer.prototype.smartypants = function(text) {\n  if (!this.options.smartypants) return text;\n  return text\n    // em-dashes\n    .replace(/---/g, '\\u2014')\n    // en-dashes\n    .replace(/--/g, '\\u2013')\n    // opening singles\n    .replace(/(^|[-\\u2014/(\\[{\"\\s])'/g, '$1\\u2018')\n    // closing singles & apostrophes\n    .replace(/'/g, '\\u2019')\n    // opening doubles\n    .replace(/(^|[-\\u2014/(\\[{\\u2018\\s])\"/g, '$1\\u201c')\n    // closing doubles\n    .replace(/\"/g, '\\u201d')\n    // ellipses\n    .replace(/\\.{3}/g, '\\u2026');\n};\n\n/**\n * Mangle Links\n */\n\nInlineLexer.prototype.mangle = function(text) {\n  if (!this.options.mangle) return text;\n  var out = ''\n    , l = text.length\n    , i = 0\n    , ch;\n\n  for (; i < l; i++) {\n    ch = text.charCodeAt(i);\n    if (Math.random() > 0.5) {\n      ch = 'x' + ch.toString(16);\n    }\n    out += '&#' + ch + ';';\n  }\n\n  return out;\n};\n\n/**\n * Renderer\n */\n\nfunction Renderer(options) {\n  this.options = options || {};\n}\n\nRenderer.prototype.code = function(code, lang, escaped) {\n  if (this.options.highlight) {\n    var out = this.options.highlight(code, lang);\n    if (out != null && out !== code) {\n      escaped = true;\n      code = out;\n    }\n  }\n\n  if (!lang) {\n    return '<pre><code>'\n      + (escaped ? code : escape(code, true))\n      + '\\n</code></pre>';\n  }\n\n  return '<pre><code class=\"'\n    + this.options.langPrefix\n    + escape(lang, true)\n    + '\">'\n    + (escaped ? code : escape(code, true))\n    + '\\n</code></pre>\\n';\n};\n\nRenderer.prototype.blockquote = function(quote) {\n  return '<blockquote>\\n' + quote + '</blockquote>\\n';\n};\n\nRenderer.prototype.html = function(html) {\n  return html;\n};\n\nRenderer.prototype.heading = function(text, level, raw) {\n  return '<h'\n    + level\n    + ' id=\"'\n    + this.options.headerPrefix\n    + raw.toLowerCase().replace(/[^\\w]+/g, '-')\n    + '\">'\n    + text\n    + '</h'\n    + level\n    + '>\\n';\n};\n\nRenderer.prototype.hr = function() {\n  return this.options.xhtml ? '<hr/>\\n' : '<hr>\\n';\n};\n\nRenderer.prototype.list = function(body, ordered) {\n  var type = ordered ? 'ol' : 'ul';\n  return '<' + type + '>\\n' + body + '</' + type + '>\\n';\n};\n\nRenderer.prototype.listitem = function(text) {\n  return '<li>' + text + '</li>\\n';\n};\n\nRenderer.prototype.paragraph = function(text) {\n  return '<p>' + text + '</p>\\n';\n};\n\nRenderer.prototype.table = function(header, body) {\n  return '<table>\\n'\n    + '<thead>\\n'\n    + header\n    + '</thead>\\n'\n    + '<tbody>\\n'\n    + body\n    + '</tbody>\\n'\n    + '</table>\\n';\n};\n\nRenderer.prototype.tablerow = function(content) {\n  return '<tr>\\n' + content + '</tr>\\n';\n};\n\nRenderer.prototype.tablecell = function(content, flags) {\n  var type = flags.header ? 'th' : 'td';\n  var tag = flags.align\n    ? '<' + type + ' style=\"text-align:' + flags.align + '\">'\n    : '<' + type + '>';\n  return tag + content + '</' + type + '>\\n';\n};\n\n// span level renderer\nRenderer.prototype.strong = function(text) {\n  return '<strong>' + text + '</strong>';\n};\n\nRenderer.prototype.em = function(text) {\n  return '<em>' + text + '</em>';\n};\n\nRenderer.prototype.codespan = function(text) {\n  return '<code>' + text + '</code>';\n};\n\nRenderer.prototype.br = function() {\n  return this.options.xhtml ? '<br/>' : '<br>';\n};\n\nRenderer.prototype.del = function(text) {\n  return '<del>' + text + '</del>';\n};\n\nRenderer.prototype.link = function(href, title, text) {\n  if (this.options.sanitize) {\n    try {\n      var prot = decodeURIComponent(unescape(href))\n        .replace(/[^\\w:]/g, '')\n        .toLowerCase();\n    } catch (e) {\n      return '';\n    }\n    if (prot.indexOf('javascript:') === 0 || prot.indexOf('vbscript:') === 0) {\n      return '';\n    }\n  }\n  var out = '<a href=\"' + href + '\"';\n  if (title) {\n    out += ' title=\"' + title + '\"';\n  }\n  out += '>' + text + '</a>';\n  return out;\n};\n\nRenderer.prototype.image = function(href, title, text) {\n  var out = '<img src=\"' + href + '\" alt=\"' + text + '\"';\n  if (title) {\n    out += ' title=\"' + title + '\"';\n  }\n  out += this.options.xhtml ? '/>' : '>';\n  return out;\n};\n\nRenderer.prototype.text = function(text) {\n  return text;\n};\n\n/**\n * Parsing & Compiling\n */\n\nfunction Parser(options) {\n  this.tokens = [];\n  this.token = null;\n  this.options = options || marked.defaults;\n  this.options.renderer = this.options.renderer || new Renderer;\n  this.renderer = this.options.renderer;\n  this.renderer.options = this.options;\n}\n\n/**\n * Static Parse Method\n */\n\nParser.parse = function(src, options, renderer) {\n  var parser = new Parser(options, renderer);\n  return parser.parse(src);\n};\n\n/**\n * Parse Loop\n */\n\nParser.prototype.parse = function(src) {\n  this.inline = new InlineLexer(src.links, this.options, this.renderer);\n  this.tokens = src.reverse();\n\n  var out = '';\n  while (this.next()) {\n    out += this.tok();\n  }\n\n  return out;\n};\n\n/**\n * Next Token\n */\n\nParser.prototype.next = function() {\n  return this.token = this.tokens.pop();\n};\n\n/**\n * Preview Next Token\n */\n\nParser.prototype.peek = function() {\n  return this.tokens[this.tokens.length - 1] || 0;\n};\n\n/**\n * Parse Text Tokens\n */\n\nParser.prototype.parseText = function() {\n  var body = this.token.text;\n\n  while (this.peek().type === 'text') {\n    body += '\\n' + this.next().text;\n  }\n\n  return this.inline.output(body);\n};\n\n/**\n * Parse Current Token\n */\n\nParser.prototype.tok = function() {\n  switch (this.token.type) {\n    case 'space': {\n      return '';\n    }\n    case 'hr': {\n      return this.renderer.hr();\n    }\n    case 'heading': {\n      return this.renderer.heading(\n        this.inline.output(this.token.text),\n        this.token.depth,\n        this.token.text);\n    }\n    case 'code': {\n      return this.renderer.code(this.token.text,\n        this.token.lang,\n        this.token.escaped);\n    }\n    case 'table': {\n      var header = ''\n        , body = ''\n        , i\n        , row\n        , cell\n        , flags\n        , j;\n\n      // header\n      cell = '';\n      for (i = 0; i < this.token.header.length; i++) {\n        flags = { header: true, align: this.token.align[i] };\n        cell += this.renderer.tablecell(\n          this.inline.output(this.token.header[i]),\n          { header: true, align: this.token.align[i] }\n        );\n      }\n      header += this.renderer.tablerow(cell);\n\n      for (i = 0; i < this.token.cells.length; i++) {\n        row = this.token.cells[i];\n\n        cell = '';\n        for (j = 0; j < row.length; j++) {\n          cell += this.renderer.tablecell(\n            this.inline.output(row[j]),\n            { header: false, align: this.token.align[j] }\n          );\n        }\n\n        body += this.renderer.tablerow(cell);\n      }\n      return this.renderer.table(header, body);\n    }\n    case 'blockquote_start': {\n      var body = '';\n\n      while (this.next().type !== 'blockquote_end') {\n        body += this.tok();\n      }\n\n      return this.renderer.blockquote(body);\n    }\n    case 'list_start': {\n      var body = ''\n        , ordered = this.token.ordered;\n\n      while (this.next().type !== 'list_end') {\n        body += this.tok();\n      }\n\n      return this.renderer.list(body, ordered);\n    }\n    case 'list_item_start': {\n      var body = '';\n\n      while (this.next().type !== 'list_item_end') {\n        body += this.token.type === 'text'\n          ? this.parseText()\n          : this.tok();\n      }\n\n      return this.renderer.listitem(body);\n    }\n    case 'loose_item_start': {\n      var body = '';\n\n      while (this.next().type !== 'list_item_end') {\n        body += this.tok();\n      }\n\n      return this.renderer.listitem(body);\n    }\n    case 'html': {\n      var html = !this.token.pre && !this.options.pedantic\n        ? this.inline.output(this.token.text)\n        : this.token.text;\n      return this.renderer.html(html);\n    }\n    case 'paragraph': {\n      return this.renderer.paragraph(this.inline.output(this.token.text));\n    }\n    case 'text': {\n      return this.renderer.paragraph(this.parseText());\n    }\n  }\n};\n\n/**\n * Helpers\n */\n\nfunction escape(html, encode) {\n  return html\n    .replace(!encode ? /&(?!#?\\w+;)/g : /&/g, '&amp;')\n    .replace(/</g, '&lt;')\n    .replace(/>/g, '&gt;')\n    .replace(/\"/g, '&quot;')\n    .replace(/'/g, '&#39;');\n}\n\nfunction unescape(html) {\n\t// explicitly match decimal, hex, and named HTML entities \n  return html.replace(/&(#(?:\\d+)|(?:#x[0-9A-Fa-f]+)|(?:\\w+));?/g, function(_, n) {\n    n = n.toLowerCase();\n    if (n === 'colon') return ':';\n    if (n.charAt(0) === '#') {\n      return n.charAt(1) === 'x'\n        ? String.fromCharCode(parseInt(n.substring(2), 16))\n        : String.fromCharCode(+n.substring(1));\n    }\n    return '';\n  });\n}\n\nfunction replace(regex, opt) {\n  regex = regex.source;\n  opt = opt || '';\n  return function self(name, val) {\n    if (!name) return new RegExp(regex, opt);\n    val = val.source || val;\n    val = val.replace(/(^|[^\\[])\\^/g, '$1');\n    regex = regex.replace(name, val);\n    return self;\n  };\n}\n\nfunction noop() {}\nnoop.exec = noop;\n\nfunction merge(obj) {\n  var i = 1\n    , target\n    , key;\n\n  for (; i < arguments.length; i++) {\n    target = arguments[i];\n    for (key in target) {\n      if (Object.prototype.hasOwnProperty.call(target, key)) {\n        obj[key] = target[key];\n      }\n    }\n  }\n\n  return obj;\n}\n\n\n/**\n * Marked\n */\n\nfunction marked(src, opt, callback) {\n  if (callback || typeof opt === 'function') {\n    if (!callback) {\n      callback = opt;\n      opt = null;\n    }\n\n    opt = merge({}, marked.defaults, opt || {});\n\n    var highlight = opt.highlight\n      , tokens\n      , pending\n      , i = 0;\n\n    try {\n      tokens = Lexer.lex(src, opt)\n    } catch (e) {\n      return callback(e);\n    }\n\n    pending = tokens.length;\n\n    var done = function(err) {\n      if (err) {\n        opt.highlight = highlight;\n        return callback(err);\n      }\n\n      var out;\n\n      try {\n        out = Parser.parse(tokens, opt);\n      } catch (e) {\n        err = e;\n      }\n\n      opt.highlight = highlight;\n\n      return err\n        ? callback(err)\n        : callback(null, out);\n    };\n\n    if (!highlight || highlight.length < 3) {\n      return done();\n    }\n\n    delete opt.highlight;\n\n    if (!pending) return done();\n\n    for (; i < tokens.length; i++) {\n      (function(token) {\n        if (token.type !== 'code') {\n          return --pending || done();\n        }\n        return highlight(token.text, token.lang, function(err, code) {\n          if (err) return done(err);\n          if (code == null || code === token.text) {\n            return --pending || done();\n          }\n          token.text = code;\n          token.escaped = true;\n          --pending || done();\n        });\n      })(tokens[i]);\n    }\n\n    return;\n  }\n  try {\n    if (opt) opt = merge({}, marked.defaults, opt);\n    return Parser.parse(Lexer.lex(src, opt), opt);\n  } catch (e) {\n    e.message += '\\nPlease report this to https://github.com/chjj/marked.';\n    if ((opt || marked.defaults).silent) {\n      return '<p>An error occured:</p><pre>'\n        + escape(e.message + '', true)\n        + '</pre>';\n    }\n    throw e;\n  }\n}\n\n/**\n * Options\n */\n\nmarked.options =\nmarked.setOptions = function(opt) {\n  merge(marked.defaults, opt);\n  return marked;\n};\n\nmarked.defaults = {\n  gfm: true,\n  tables: true,\n  breaks: false,\n  pedantic: false,\n  sanitize: false,\n  sanitizer: null,\n  mangle: true,\n  smartLists: false,\n  silent: false,\n  highlight: null,\n  langPrefix: 'lang-',\n  smartypants: false,\n  headerPrefix: '',\n  renderer: new Renderer,\n  xhtml: false\n};\n\n/**\n * Expose\n */\n\nmarked.Parser = Parser;\nmarked.parser = Parser.parse;\n\nmarked.Renderer = Renderer;\n\nmarked.Lexer = Lexer;\nmarked.lexer = Lexer.lex;\n\nmarked.InlineLexer = InlineLexer;\nmarked.inlineLexer = InlineLexer.output;\n\nmarked.parse = marked;\n\nif (typeof module !== 'undefined' && typeof exports === 'object') {\n  module.exports = marked;\n} else if (typeof define === 'function' && define.amd) {\n  define(function() { return marked; });\n} else {\n  this.marked = marked;\n}\n\n}).call(function() {\n  return this || (typeof window !== 'undefined' ? window : global);\n}());\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/marked/lib/marked.js\n// module id = 22\n// module chunks = 0 1","Prism.languages.python= {\n\t'triple-quoted-string': {\n\t\tpattern: /\"\"\"[\\s\\S]+?\"\"\"|'''[\\s\\S]+?'''/,\n\t\talias: 'string'\n\t},\n\t'comment': {\n\t\tpattern: /(^|[^\\\\])#.*/,\n\t\tlookbehind: true\n\t},\n\t'string': {\n\t\tpattern: /(\"|')(?:\\\\\\\\|\\\\?[^\\\\\\r\\n])*?\\1/,\n\t\tgreedy: true\n\t},\n\t'function' : {\n\t\tpattern: /((?:^|\\s)def[ \\t]+)[a-zA-Z_][a-zA-Z0-9_]*(?=\\()/g,\n\t\tlookbehind: true\n\t},\n\t'class-name': {\n\t\tpattern: /(\\bclass\\s+)[a-z0-9_]+/i,\n\t\tlookbehind: true\n\t},\n\t'keyword' : /\\b(?:as|assert|async|await|break|class|continue|def|del|elif|else|except|exec|finally|for|from|global|if|import|in|is|lambda|pass|print|raise|return|try|while|with|yield)\\b/,\n\t'boolean' : /\\b(?:True|False)\\b/,\n\t'number' : /\\b-?(?:0[bo])?(?:(?:\\d|0x[\\da-f])[\\da-f]*\\.?\\d*|\\.\\d+)(?:e[+-]?\\d+)?j?\\b/i,\n\t'operator' : /[-+%=]=?|!=|\\*\\*?=?|\\/\\/?=?|<[<=>]?|>[=>]?|[&|^~]|\\b(?:or|and|not)\\b/,\n\t'punctuation' : /[{}[\\];(),.:]/\n};\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/prismjs/components/prism-python.js\n// module id = 23\n// module chunks = 0 1","<template>\r\n  <div class=\"research-article\">\r\n    <div v-html=\"rawHtml\">\r\n    </div>\r\n  </div>\r\n</template>\r\n\r\n<script>\r\nimport marked from 'marked'\r\nimport Prism from 'prismjs'\r\nimport 'prismjs/themes/prism-coy.css'\r\nimport 'prismjs/prism.js'\r\nimport 'prismjs/components/prism-python.js'\r\n\r\nlet article = `\r\n# LSTM-RNN For Sentiment Analysis\r\nAuthor: Trilby Hren and Costa Huang\r\n\r\nSource code vailable at https://github.com/costahuang/Sentiment-Analysis-LSTM\r\n\r\n**Our project has three main parts:**\r\n1. Reproduce and understand Keras(a python machine learning library) official demo code on LSTM-RNN for sentiment analysis \r\n * [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py) Main program that rains a LSTM on the IMDB sentiment classification task [1]\r\n * [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py) Preprocessing script of IMDB movie review dataset [2]\r\n* Improve the program's accuracy by exploring different techniques:\r\n * Preprocessing techniques\r\n * Activation functions\r\n * Optimizer choices\r\n* Futher application\r\n * Visualization of sentiment analysis\r\n * Applied our result to Amazon Review Datas\r\n\r\n\r\n\r\n## Baseline algorithm (Demo code from Keras)\r\nFirstly we execute [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py)[1] and yields following results where a test accuracy of 82.35% is acquired.\r\n\r\n\r\n\\`\\`\\`python\r\n'''Trains a LSTM on the IMDB sentiment classification task.\r\nThe dataset is actually too small for LSTM to be of any advantage\r\ncompared to simpler, much faster methods such as TF-IDF + LogReg.\r\nNotes:\r\n- RNNs are tricky. Choice of batch size is important,\r\nchoice of loss and optimizer is critical, etc.\r\nSome configurations won't converge.\r\n- LSTM loss decrease patterns during training can be quite different\r\nfrom what you see with CNNs/MLPs/etc.\r\n'''\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nnp.random.seed(1337)  # for reproducibility\r\n\r\nfrom keras.preprocessing import sequence\r\nfrom keras.utils import np_utils\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation, Embedding\r\nfrom keras.layers import LSTM, SimpleRNN, GRU\r\nfrom keras.datasets import imdb\r\n\r\nmax_features = 20000\r\nmaxlen = 80  # cut texts after this number of words (among top max_features most common words)\r\nbatch_size = 32\r\n\r\nprint('Loading data...')\r\n(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\r\nprint(len(X_train), 'train sequences')\r\nprint(len(X_test), 'test sequences')\r\n\r\nprint('Pad sequences (samples x time)')\r\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\r\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\r\nprint('X_train shape:', X_train.shape)\r\nprint('X_test shape:', X_test.shape)\r\n\r\nprint('Build model...')\r\nmodel = Sequential()\r\nmodel.add(Embedding(max_features, 128, dropout=0.2))\r\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\n# try using different optimizers and different optimizer configs\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\nprint('Train...')\r\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\r\n          validation_data=(X_test, y_test))\r\nscore, acc = model.evaluate(X_test, y_test,\r\n                            batch_size=batch_size)\r\nprint('Test score:', score)\r\nprint('Test accuracy:', acc)\r\n\\`\\`\\`\r\n\r\n    Using Theano backend.\r\n    \r\n\r\n    Loading data...\r\n    25000 train sequences\r\n    25000 test sequences\r\n    Pad sequences (samples x time)\r\n    X_train shape: (25000, 80)\r\n    X_test shape: (25000, 80)\r\n    Build model...\r\n    Train...\r\n    Train on 25000 samples, validate on 25000 samples\r\n    Epoch 1/10\r\n    25000/25000 [==============================] - 138s - loss: 0.5333 - acc: 0.7316 - val_loss: 0.4631 - val_acc: 0.7898\r\n    Epoch 2/10\r\n    25000/25000 [==============================] - 144s - loss: 0.3812 - acc: 0.8358 - val_loss: 0.3776 - val_acc: 0.8370\r\n    Epoch 3/10\r\n    25000/25000 [==============================] - 147s - loss: 0.3077 - acc: 0.8737 - val_loss: 0.3687 - val_acc: 0.8360\r\n    Epoch 4/10\r\n    25000/25000 [==============================] - 146s - loss: 0.2497 - acc: 0.8979 - val_loss: 0.4161 - val_acc: 0.8322\r\n    Epoch 5/10\r\n    25000/25000 [==============================] - 149s - loss: 0.2100 - acc: 0.9180 - val_loss: 0.4277 - val_acc: 0.8330\r\n    Epoch 6/10\r\n    25000/25000 [==============================] - 151s - loss: 0.1801 - acc: 0.9298 - val_loss: 0.4318 - val_acc: 0.8362\r\n    Epoch 7/10\r\n    25000/25000 [==============================] - 148s - loss: 0.1503 - acc: 0.9423 - val_loss: 0.4964 - val_acc: 0.8280\r\n    Epoch 8/10\r\n    25000/25000 [==============================] - 150s - loss: 0.1328 - acc: 0.9499 - val_loss: 0.5169 - val_acc: 0.8266\r\n    Epoch 9/10\r\n    25000/25000 [==============================] - 146s - loss: 0.1163 - acc: 0.9548 - val_loss: 0.5354 - val_acc: 0.8227\r\n    Epoch 10/10\r\n    25000/25000 [==============================] - 153s - loss: 0.1047 - acc: 0.9618 - val_loss: 0.5375 - val_acc: 0.8236\r\n    25000/25000 [==============================] - 34s    \r\n    Test score: 0.537465095153\r\n    Test accuracy: 0.82356\r\n    \r\n\r\n## Understanding the source code\r\n** 1. Preprocessing**\r\n * Original Dataset is provided by a [Stanford research group](http://ai.stanford.edu/~amaas//data/sentiment/) [3], which provided a set of 25,000 preclassified IMDB movie reviews for training, and 25,000 for testing.\r\n * [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py)[2] performs the following tasks:\r\n     * Download tokenized the IMDB movie review pre-supplied by Keras, where each review is encoded as word indices. An example training review would look like [3, 213, 43, 324, 12, 4, ...] where 3 represents the third most popular word in the dataset.\r\n     * Shuffle the training and testing dataset.\r\n     * Include only *nb_words* most frequent word for ouput\r\n     * Add a [1] to each review to mark the start of the sequence\r\n     * Add a [2] to replace the words that are cut off because of *nb_words*\r\n     * Index actual words with *\"index_from = 3*\" and higher. (We think this part is **very problematic**, which is mentioned in the next code cell)\r\n\r\n\r\n\\`\\`\\`python\r\n# From imdb.py, it has a preprocessing code like this, where they use index_from = 3 as the default parameter\r\n# if start_char is not None:\r\n#     X = [[start_char] + [w + index_from for w in x] for x in X]\r\nindex_from = 3\r\nexample_review = [6, 499, 6, 99, 221, 9, 22, 501, 2, 3]\r\nexample_review = [[1] + [w + index_from for w in example_review]]\r\nprint(example_review)\r\n\\`\\`\\`\r\n\r\n    [[1, 9, 502, 9, 102, 224, 12, 25, 504, 5, 6]]\r\n    \r\n\r\n> As we demoed, this particular parameter \"shifted\" each word index of the review up by *\"index_from = 3\"*, which is totally nonsense in our opinion because it almost changes everything about that review. We also couldn't find any documentation or papers to support such parameter. As a result, in our experiment, we just simply set *\"index_from = 0\"*.\r\n\r\n**2. Model Building**\r\n * [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py) builds the model:\r\n     * Limit top most frequent words: Each review only has *\"max_features = nb_words = 20000\"* most frequent word index\r\n     * Pad Sequence: Each review is padded with a maximum length of *\"maxlen = 80\"*\r\n     * Embedding layer: Map each word index into *\"p = 128\"* dimentional vector space\r\n       * The original idea came from this landmark paper [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)[6] where they show a good vector representation for text sequences is achievable.\r\n     * LSTM layer\r\n        * Hochreiter & Schmidhuber [7] first introduced this special kind of RNN to solve the long-term dependency problem. Unlike traditional RNN, LSTM has a memory cell that it uses to determine what information is important by using \"forget gate layer\" and \"input gate layer\". [8]\r\n     * Dense(1): fully connect LSTM to create one demensional output\r\n     * Sigmoid\r\n     * Adam optimizer for training.\r\n\r\n## Improving the algorithm\r\n**_Activation functions_** by [Keras.layers.core.Activation](https://keras.io/activations/)\r\n1. Sigmoid  \r\n   1. σ(x) = 1/(1+e−x)  \r\n   2. The sigmoid function is a squashing function. We discovered that this activation function jumps up in test accuracy and converges to about 82.6% after 8 generations.     \r\n   \r\n2. ReLU  \r\n  1. f(x) = max(0,x)  \r\n  1. For the ReLU activation function our results show dramatic improvement in later generations. It jumps from very low results to much higher accuracy in both testing and training. The final accuracy is not as good as the results from the sigmoid function, it converges to 73.8% in the last generation.      \r\n  \r\n3. Tanh  \r\n  1. tanh(x) = 2σ(2x)−1tanh(x)=2σ(2x)−1  \r\n  1. The tanh function was slightly better than ReLU with a final accuracy of 77.6%. During our 8 generations, this function seemed to decrease in overall test accuracy perhaps due to overfitting.      \r\n  \r\n4. Hard sigmoid    \r\n  1. The hard sigmoid function performed only slightly better than the sigmoid with a final test accuracy of 83%. From our understanding, it should have converged faster due to increased speed since the precision isn't as important. Perhaps the lower test accuracy is because of the loss in precision rather than the speed.      \r\n\r\n[3]\r\nCheck out our [Activation functions data](https://docs.google.com/document/d/1uQ49jhYYOmwzSPxYYamTXYCq-07Dhhu75eleew3LBPU/edit?usp=sharing)\r\n\r\n\r\n**_Optimizer choices_** by [keras.optimizers](https://keras.io/optimizers/)\r\n1. SGD (Stochastic Gradient Decent)\r\n    1. This type of optimizer uses only a few training examples rather than the entire training set. It also can lead to fast convergence since it doesn't have to run back propagation over the full training set. \r\n    1. Sometimes a meaningful order leads to a bias for the gradient and causes poor convergence. Since \\`our data was randomly shuffled before each training\\`, this shouldn't have been an issue.\r\n        2. _Adam_\r\n            1. Adaptive Moment Estimation\r\n            1. This optimizer uses momentum and past gradients to help with learning by combining the two advantages of RMSprop and AdaGrad. \r\n            1. While we expected this optimizer to perfom the best since it combines the better parts of RMSprop and AdaGrad, it was actually one of our lowest perfoming tests. It showed very slow convergence to 82.6% during our 8 generations. Perhaps it would have converged to a higher accuracy than the others, given a higher number of generations. \r\n        3. _RMSprop_\r\n            1. Root Mean Square Propagation\r\n            1. This method adapts the learning rate for each parameter by using tha magnitude of recent gradients to normalize the gradients.\r\n            1. From out tests, RSMprop performed the best with a final testing accuracy converging to 84.8%. This optimizer had steady performance through the 8 generations with the lowest accuracy being 82.8%.\r\n        4. _AdaGrad_\r\n            1. Adaptive Gradient Algorithm\r\n            1. AdaGrad adapts to the data in order to chose a learning rate for each feature.\r\n            1. The performance of the AdaGrad optimizer was mediocre, with a final convergence at 83.4%.\r\n        5. _Nadam_\r\n            1. Nesterov Adaptive Moment Estimation\r\n            1. Nadam combines Adam with Nesterov's accelerated gradient.\r\n            1. Our results from this test were one of the lowest with Adam, converging to 82.3% and increasing by a whopping 0.3% over the course of 8 generations.\r\n\r\n[4]\r\nCheck out our [Optimizer choices data](https://docs.google.com/document/d/1YI5wluhh3rqHs8LBqqL2vOxqNwZdYiPHstWkS2_1s4s/edit?usp=sharing)\r\n\r\n** Preprocessing techniques**\r\n1. Change the padding(*\"maxlen = 80\"*) for each reviews\r\n    1. It is reasonable to assume when people write reviews, they would express their opinion at the start more often than at the end. So we can limit each review to some *\"maxlen\"* to eliminate noisy data. \r\n2. Change the top most frequent words(*\"max_features = nb_words = 20000\"*) for training and testing dataset to keep \r\n    1. Some words are essential than other words for expressing sentiment. For example, \"love\" would express more emotion than \"keyboard\". We therefore limit *\"max_features\"* to eliminate less sentimentally expresed words.\r\n3. Set *\"index_from = 0\"* as we explained previously\r\n    \r\nCheck out our [Preprocessing techniques data](https://docs.google.com/document/d/1pjZbvzbbHWRUvybd--uTJUhPKvMCjbjxacE3j_zQCM4/edit?usp=sharing)\r\n\r\n\r\n\r\n\\`\\`\\`python\r\nimport pandas as pd\r\nimport re\r\nimport matplotlib.pyplot as plt\r\n\r\n\"\"\" get data from model generated text; we further used this script to get datas below. \"\"\"\r\nmodel_output = \"Epoch 1/8 - acc: 0.7090 - val_acc: 0.8205 Epoch 2/8 - acc: 0.8402 - val_acc: 0.8370 , ....\"\r\ndata_list = re.findall(r'acc: (\\d+\\.\\d+)', model_output)\r\ndata_list = list(map(float, data_list))\r\nprint(data_list[::2])\r\nprint(data_list[1::2])\r\n\r\n\"\"\" activation function data \"\"\"\r\ntrain_hard_sigmoid = pd.DataFrame({'hard_sigmoid': [0.72,0.8137,0.8403,0.8659,0.8892,0.8974,0.9116,0.9184]})\r\ntrain_sigmoid = pd.DataFrame({'sigmoid': [0.7316,0.8358,0.8737,0.8979,0.9180,0.9298,0.9423,0.9499]})\r\ntrain_tanh = pd.DataFrame({'tanh': [0.6574,0.7830,0.8335,0.8710,0.8882,0.8954,0.9052,0.8746]})\r\ntrain_relu = pd.DataFrame({'relu': [0.6524,0.7396,0.7273,0.5278,0.5663,0.6519,0.8128,0.8407]})\r\nactivation_train = pd.concat([train_hard_sigmoid,train_sigmoid,train_tanh,train_relu], axis = 1)\r\n\r\ntest_hard_sigmoid = pd.DataFrame({'hard_sigmoid': [0.7417,0.8195,0.8336,0.8351,0.8280,0.8241,0.8282,0.8302]})\r\ntest_sigmoid = pd.DataFrame({'sigmoid': [0.7898,0.8370,0.8360,0.8322,0.8330,0.8362,0.8280,0.8266]})\r\ntest_tanh = pd.DataFrame({'tanh': [0.7883,0.7926,0.8153,0.8193,0.7427,0.7679,0.7967,0.7760]})\r\ntest_relu = pd.DataFrame({'relu': [0.7634,0.7986,0.5040,0.4519,0.5000,0.7477,0.7482,0.7388]})\r\nactivation_test = pd.concat([test_hard_sigmoid,test_sigmoid, test_tanh, test_relu], axis = 1)\r\n\r\n\r\n\"\"\" optimizer data \"\"\"\r\ntrain_adam = pd.DataFrame({'adam': [0.7316, 0.8358, 0.8737, 0.8979, 0.918, 0.9298, 0.9423, 0.9499]})\r\ntrain_RMSprop = pd.DataFrame({'RMSprop': [0.7357, 0.8212, 0.8461, 0.8664, 0.8764, 0.8844, 0.8927, 0.8998]})\r\ntrain_Adagrad = pd.DataFrame({'Adagrad': [0.7349, 0.8347, 0.8654, 0.8828, 0.8963, 0.902, 0.9118, 0.9164]})\r\ntrain_Nadam = pd.DataFrame({'Nadam': [0.709, 0.8402, 0.8871, 0.9174, 0.9358, 0.947, 0.957, 0.9625]})\r\noptimizer_train = pd.concat([train_adam,train_RMSprop,train_Adagrad,train_Nadam], axis = 1)\r\n\r\n\r\ntest_adam = pd.DataFrame({'adam': [0.7898, 0.837, 0.836, 0.8322, 0.833, 0.8362, 0.828, 0.8266]})\r\ntest_RMSprop = pd.DataFrame({'RMSprop': [0.8288, 0.831, 0.8512, 0.8317, 0.8529, 0.8401, 0.847, 0.849]})\r\ntest_Adagrad = pd.DataFrame({'Adagrad': [0.8264, 0.7822, 0.8432, 0.8435, 0.838, 0.8372, 0.8384, 0.835]})\r\ntest_Nadam = pd.DataFrame({'Nadam': [0.8205, 0.837, 0.833, 0.8366, 0.8325, 0.8289, 0.8276, 0.8238]})\r\noptimizer_test = pd.concat([test_adam,test_RMSprop,test_Adagrad,test_Nadam], axis = 1)\r\n\r\n\r\n\"\"\" preprocessing data \"\"\"\r\ntrain_max_len_120 = pd.DataFrame({'max_len_120': [0.7041, 0.8279, 0.8577, 0.8767, 0.889, 0.8983, 0.9042, 0.9142, 0.9198, 0.9247]})\r\ntrain_max_len_160 = pd.DataFrame({'max_len_160': [0.722, 0.8315, 0.863, 0.8832, 0.8937, 0.9032, 0.9124, 0.9202, 0.9259, 0.9305]})\r\ntrain_max_len_200 = pd.DataFrame({'max_len_200': [0.7176, 0.8294, 0.8623, 0.885, 0.8975, 0.9068, 0.9131, 0.9214, 0.9265, 0.9327]})\r\ntrain_max_len = pd.concat([train_max_len_120,train_max_len_160,train_max_len_200], axis = 1)\r\n\r\ntest_max_len_120 = pd.DataFrame({'max_len_120': [0.8318, 0.8538, 0.8657, 0.8227, 0.8681, 0.8693, 0.8731, 0.8708, 0.8718, 0.8584]})\r\ntest_max_len_160 = pd.DataFrame({'max_len_160': [0.8481, 0.694, 0.8731, 0.8752, 0.8768, 0.8794, 0.8779, 0.88, 0.8834, 0.8792]})\r\ntest_max_len_200 = pd.DataFrame({'max_len_200': [0.8254, 0.8498, 0.877, 0.8671, 0.8828, 0.8854, 0.8891, 0.8862, 0.8877, 0.8769]})\r\ntest_max_len = pd.concat([test_max_len_120,test_max_len_160,test_max_len_200], axis = 1)\r\n\r\ntrain_max_features_4000 = pd.DataFrame({'max_features_4000': [0.7287, 0.8087, 0.8272, 0.8416, 0.8522, 0.857, 0.8608, 0.8672, 0.8729, 0.8763]})\r\ntrain_max_features_3000 = pd.DataFrame({'max_features_3000': [0.7218, 0.8039, 0.823, 0.8349, 0.8422, 0.8492, 0.8562, 0.8607, 0.8638, 0.8696]})\r\ntrain_max_features_2000 = pd.DataFrame({'max_features_2000': [0.7016, 0.7971, 0.8116, 0.8214, 0.8314, 0.8364, 0.8417, 0.8476, 0.8466, 0.8499]})\r\ntrain_max_features = pd.concat([train_max_features_4000,train_max_features_3000,train_max_features_2000], axis = 1)\r\n\r\ntest_max_features_4000 = pd.DataFrame({'max_features_4000': [0.7834, 0.821, 0.8296, 0.8473, 0.8521, 0.8435, 0.8461, 0.85, 0.8545, 0.8486]})\r\ntest_max_features_3000 = pd.DataFrame({'max_features_3000': [0.7859, 0.824, 0.8231, 0.8448, 0.8465, 0.84, 0.8501, 0.8412, 0.8488, 0.8536]})\r\ntest_max_features_2000 = pd.DataFrame({'max_features_2000': [0.8152, 0.8274, 0.83, 0.8392, 0.8268, 0.8376, 0.8452, 0.8457, 0.8424, 0.8413]})\r\ntest_max_features = pd.concat([test_max_features_4000,test_max_features_3000,test_max_features_2000], axis = 1)\r\n\r\n\"\"\" plotting \"\"\"\r\n%matplotlib inline\r\nf1, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\r\nf1.set_figwidth(15)\r\nactivation_train.plot(ax = ax1, title = \"activation_train\")\r\nactivation_test.plot(ax = ax2, title = \"activation_test\")\r\n\r\nf2, (ax3, ax4) = plt.subplots(1, 2, sharey=True)\r\nf2.set_figwidth(15)\r\noptimizer_train.plot(ax = ax3,title = \"optimizer_train\")\r\noptimizer_test.plot(ax = ax4,title = \"optimizer_test\")\r\n\r\nf3, (ax5, ax6) = plt.subplots(1, 2, sharey=True)\r\nf3.set_figwidth(15)\r\ntrain_max_len.plot(ax = ax5,title = \"train_max_len\")\r\ntest_max_len.plot(ax = ax6,title = \"test_max_len\")\r\n\r\nf4, (ax7, ax8) = plt.subplots(1, 2, sharey=True)\r\nf4.set_figwidth(15)\r\ntrain_max_features.plot(ax = ax7,title = \"train_max_features\")\r\ntest_max_features.plot(ax = ax8,title = \"test_max_features\")\r\n\\`\\`\\`\r\n\r\n    [0.709, 0.8402]\r\n    [0.8205, 0.837]\r\n\r\n    <matplotlib.axes._subplots.AxesSubplot at 0x1d192d00f28>\r\n\r\n![png](${require('./output_11_2.png')})\r\n\r\n\r\n\r\n![png](${require('./output_11_3.png')})\r\n\r\n\r\n\r\n![png](${require('./output_11_4.png')})\r\n\r\n\r\n\r\n![png](${require('./output_11_5.png')})\r\n\r\n\r\n## Building New Model \r\nAfter such comparision, we decided to build model using *RMSprop* optimizer, *sigmoid* activation function, *max_features* = 3000, and *max_len*= 200.\r\n\r\n\r\n\\`\\`\\`python\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nnp.random.seed(1337)  # for reproducibility\r\n\r\nfrom keras.preprocessing import sequence\r\nfrom keras.utils import np_utils\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation, Embedding\r\nfrom keras.layers import LSTM, SimpleRNN, GRU\r\nfrom keras.datasets import imdb\r\nfrom keras.layers.wrappers import TimeDistributed\r\nimport pickle\r\nfrom six.moves import cPickle\r\nimport pandas as pd\r\nimport math\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\nmax_features = 3000\r\nmaxlen = 200  # cut texts after this number of words (among top max_features most common words)\r\nbatch_size = 32\r\n\r\nprint('Loading data...')\r\n(XX_train, y_train), (XX_test, y_test) = imdb.load_data(path='imdb_full.pkl', nb_words=max_features, skip_top=10,\r\n              maxlen=None, seed=113,\r\n              start_char=1, oov_char=2, index_from=0)\r\nprint(len(XX_train), 'train sequences')\r\nprint(len(XX_test), 'test sequences')\r\n\r\nprint('Pad sequences (samples x time)')\r\nX_train = sequence.pad_sequences(XX_train, maxlen=maxlen)\r\nX_test = sequence.pad_sequences(XX_test, maxlen=maxlen)\r\nprint('X_train shape:', X_train.shape)\r\nprint('X_test shape:', X_test.shape)\r\n\r\nprint('Build model...')\r\nmodel = Sequential()\r\nmodel.add(Embedding(max_features, 128, dropout=0.2))\r\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\n# try using different optimizers and different optimizer configs\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy'])\r\n\r\nprint('Train...')\r\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\r\n          validation_data=(X_test, y_test))\r\n\\`\\`\\`\r\n\r\n    Loading data...\r\n    25000 train sequences\r\n    25000 test sequences\r\n    Pad sequences (samples x time)\r\n    X_train shape: (25000, 200)\r\n    X_test shape: (25000, 200)\r\n    Build model...\r\n    Train...\r\n    Train on 25000 samples, validate on 25000 samples\r\n    Epoch 1/10\r\n    25000/25000 [==============================] - 288s - loss: 0.5716 - acc: 0.6874 - val_loss: 0.3979 - val_acc: 0.8376\r\n    Epoch 2/10\r\n    25000/25000 [==============================] - 289s - loss: 0.4373 - acc: 0.8071 - val_loss: 0.3447 - val_acc: 0.8603\r\n    Epoch 3/10\r\n    25000/25000 [==============================] - 297s - loss: 0.3914 - acc: 0.8336 - val_loss: 0.3410 - val_acc: 0.8577\r\n    Epoch 4/10\r\n    25000/25000 [==============================] - 301s - loss: 0.3526 - acc: 0.8508 - val_loss: 0.3228 - val_acc: 0.8604\r\n    Epoch 5/10\r\n    25000/25000 [==============================] - 305s - loss: 0.3224 - acc: 0.8631 - val_loss: 0.2915 - val_acc: 0.8782\r\n    Epoch 6/10\r\n    25000/25000 [==============================] - 302s - loss: 0.3031 - acc: 0.8756 - val_loss: 0.3119 - val_acc: 0.8671\r\n    Epoch 7/10\r\n    25000/25000 [==============================] - 305s - loss: 0.2868 - acc: 0.8830 - val_loss: 0.2771 - val_acc: 0.8833\r\n    Epoch 8/10\r\n    25000/25000 [==============================] - 290s - loss: 0.2745 - acc: 0.8867 - val_loss: 0.2739 - val_acc: 0.8842\r\n    Epoch 9/10\r\n    25000/25000 [==============================] - 275s - loss: 0.2649 - acc: 0.8904 - val_loss: 0.2715 - val_acc: 0.8844\r\n    Epoch 10/10\r\n    25000/25000 [==============================] - 284s - loss: 0.2587 - acc: 0.8937 - val_loss: 0.2718 - val_acc: 0.8875\r\n    \r\n\r\n\r\n\r\n\r\n    <keras.callbacks.History at 0x1d1950c3588>\r\n\r\n\r\n\r\nAs we can see, our new model has a better prediction of 88.75% accuracy versus the original model.\r\n\r\n## Further Applications\r\nVisualization of sentiment analysis ,inspired by [Taylor Arnold's jupyter notebook](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/notebook21.html) [9]\r\n\r\n\r\n\\`\\`\\`python\r\n%matplotlib inline\r\nfrom keras.layers.wrappers import TimeDistributed\r\nimport pickle\r\nfrom six.moves import cPickle\r\nimport pandas as pd\r\nimport math\r\nimport matplotlib.pyplot as plt\r\n\r\ndef reconstruct_text(index, index_to_word):\r\n    text = []\r\n    for ind in index:\r\n        if ind != 0:\r\n            text += [index_to_word[ind]]\r\n        else:\r\n            text += [\"\"]\r\n    return text\r\n\r\nword_to_index = imdb.get_word_index()\r\nindex_to_word = {k:v for v,k in word_to_index.items()}\r\nf = open('imdb_full.pkl', 'rb')\r\n(x_train, labels_train), (x_test, labels_test) = cPickle.load(f)\r\nf.close()\r\ndf = pd.DataFrame(x_train)\r\n\r\nmodel2 = Sequential()\r\nmodel2.add(Embedding(max_features, 128, dropout=0.2))\r\nmodel2.add(LSTM(128, dropout_W=0.2, dropout_U=0.2, return_sequences=True))  # try using a GRU instead, for fun\r\nmodel2.add(TimeDistributed(Dense(1)))\r\nmodel2.add(Activation('sigmoid'))\r\nmodel2.compile(loss='binary_crossentropy',\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy'])\r\n\r\nmodel2.set_weights(model.get_weights())\r\ny_hat2 = model2.predict(X_train)\r\n\r\nind = 100\r\ntokens = reconstruct_text(X_train[ind], index_to_word)\r\n\r\nplt.figure(figsize=(16, 10))\r\nplt.plot(y_hat2[ind],alpha=0.5)\r\nfor i in range(len(tokens)):\r\n    plt.text(i,0.5,tokens[i],rotation=90)\r\n\\`\\`\\`\r\n\r\n\r\n![png](${require('./output_16_0.png')})\r\n\r\n\r\n## Apply Current Model to Amazon Review Data\r\nNow that we have successfully improved the model, we further apply our new model to Amazon Review data provided by Dr. Allen.\r\nAmazon Review Dataset has preclassified sentiment value on 6 categories: books, camera, dvd, health, music, software. We need to complie data in a specific way so that our current model can directly use it.\r\n\r\n### 1. Change File Structure\r\nWe will split the reviews into training set and testing set by changing the file structure\r\n\r\n\r\n\\`\\`\\`python\r\nfrom IPython.display import Image\r\ni = Image(filename='file_structure.png')\r\ni\r\n\\`\\`\\`\r\n\r\n\r\n\r\n\r\n![png](${require('./output_18_0.png')})\r\n\r\n\r\n\r\n### 2. Preprocess Amazon Reviews\r\n**Load reviews:**\r\n\r\n\r\n\\`\\`\\`python\r\nfrom six.moves import cPickle\r\nimport numpy\r\nimport os\r\nimport pandas as pd\r\nimport string\r\nfrom collections import Counter\r\nfrom keras.preprocessing.text import Tokenizer\r\n\r\npath = [\"books/\", \"camera/\", \"dvd/\", \"health/\", \"music/\", \"software/\"]  # camera, has missing data\r\nff = []\r\ninput_label = []\r\nfor i in range(6):\r\n    ff += [path[i] + \"train/pos/\" + x for x in os.listdir(path[i] + \"train/pos\")] + \\\r\n         [path[i] + \"train/neg/\" + x for x in os.listdir(path[i] + \"train/neg\")] + \\\r\n         [path[i] + \"test/pos/\" + x for x in os.listdir(path[i] + \"test/pos\")] + \\\r\n         [path[i] + \"test/neg/\" + x for x in os.listdir(path[i] + \"test/neg\")]\r\n    \r\n    # Because of missing data, we need to measure how many reviews are there in each folder in order to label them correctly.\r\n    train_pos = len(os.listdir(path[i] + \"train/pos\"))\r\n    train_neg = len(os.listdir(path[i] + \"train/neg\"))\r\n    test_pos = len(os.listdir(path[i] + \"test/pos\"))\r\n    test_neg = len(os.listdir(path[i] + \"test/neg\"))\r\n    input_label += [1] * train_pos + [0] * train_neg + [1] * test_pos + [0] * test_neg\r\n    \r\n      \r\ninput_text  = []\r\nfor f in ff:\r\n    with open(f, 'rb') as fin:\r\n        temp = fin.read().splitlines()\r\n        x = \" \".join([x.decode(\"utf-8\", errors = 'ignore') for x in temp])\r\n        input_text += [x]\r\n        \r\nprint(input_text[0])\r\n\\`\\`\\`\r\n\r\n    I wish I could give this book four and a half stars instead of four; I can't quite justify five stars in my mind.  Two of the stories were definitely good: the first, \"masked Riders\" by Parhelion, and the third, \"Ricochet\" by BA Tortuga.  I enjoyed both, but was not particularly snowed by the intensity of the conflict/plot line or the main characters and their relationships.  Not so the second story: \"Hung Up\" by Cat Kane was a powerful, intense and moving story about two delightful yet flawed characters who had secrets that tore their relationship up until they were able to work their ways (separately) around the issues.  That story deserved a five stars plus rating and I wish Kane would turn it into a full length novel and develop the characters and their backgrounds more.  This book is an excellent read both for the plot line and the erotic substance.  Enjoy\r\n    \r\n\r\n**Tokenize reviews:**\r\n\r\n\r\n\\`\\`\\`python\r\ninput_text  = []\r\nfor f in ff:\r\n    with open(f, 'rb') as fin:\r\n        temp = fin.read().splitlines()\r\n        x = \" \".join([x.decode(\"utf-8\", errors = 'ignore') for x in temp])\r\n        input_text += [x]\r\n\r\ncut_index = int(len(input_text)/2)\r\ntok = Tokenizer()\r\ntok.fit_on_texts(input_text[:cut_index])\r\n\r\nX_train = tok.texts_to_sequences(input_text[:cut_index])\r\nX_test  = tok.texts_to_sequences(input_text[cut_index:])\r\ny_train = input_label[:cut_index]\r\ny_test  = input_label[cut_index:]\r\n\r\nprint(\"As you can see, the first review has been encoded as their word index:\")\r\nprint(X_train[0][:10])\r\n\\`\\`\\`\r\n\r\n    As you can see, the first review has been encoded as their word index:\r\n    [6, 499, 6, 99, 221, 9, 22, 501, 2, 3]\r\n    \r\n\r\n**Reconstruct reviews:**\r\n\r\n\r\n\\`\\`\\`python\r\nwords = {k:v for v,k in tok.word_index.items()}\r\ndef reconstruct_text(index, words):\r\n    text = []\r\n    for ind in index:\r\n        if ind != 0:\r\n            text += [words[ind]]\r\n        else:\r\n            text += [\"\"]\r\n    return text\r\n    \r\nprint(input_text[100])\r\nprint(reconstruct_text(X_train[100], words))\r\n\\`\\`\\`\r\n\r\n    Peck relates growing up in rural/small town Vermont with a best friend who gets him into lots of trouble. Humor and pranks abound in between lessons learned\r\n    ['peck', 'relates', 'growing', 'up', 'in', 'rural', 'small', 'town', 'vermont', 'with', 'a', 'best', 'friend', 'who', 'gets', 'him', 'into', 'lots', 'of', 'trouble', 'humor', 'and', 'pranks', 'abound', 'in', 'between', 'lessons', 'learned']\r\n    \r\n\r\n**Store reviews: **\r\n\r\n\r\n\\`\\`\\`python\r\nf = open('amzn_full.pkl', 'wb')\r\ntrain_tuple = (X_train, y_train)\r\ntest_tuple = (X_test, y_test)\r\ncombine = (train_tuple, test_tuple)\r\ncPickle.dump(combine, f)\r\nf.close()\r\n\\`\\`\\`\r\n\r\n### 3. Apply Current Model to Amazon Reviews:\r\nFirstly we need to make a copy of [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py)(the script that loads imdb_full.pkl data) and make it loads our amzn_full.pkl instead. Name the new file amzn.py. Then we apply our model:\r\n\r\n\r\n\\`\\`\\`python\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nnp.random.seed(1337)  # for reproducibility\r\n\r\nfrom keras.preprocessing import sequence\r\nfrom keras.utils import np_utils\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation, Embedding\r\nfrom keras.layers import LSTM, SimpleRNN, GRU\r\nimport amzn\r\nfrom keras.layers.wrappers import TimeDistributed\r\nimport pickle\r\nfrom six.moves import cPickle\r\nimport pandas as pd\r\nimport math\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\nmax_features = 3000\r\nmaxlen = 200  # cut texts after this number of words (among top max_features most common words)\r\nbatch_size = 32\r\n\r\nprint('Loading data...')\r\n(XX_train, y_train), (XX_test, y_test) = amzn.load_data(path='amzn_full.pkl', nb_words=max_features)\r\nprint(len(XX_train), 'train sequences')\r\nprint(len(XX_test), 'test sequences')\r\n\r\nprint('Pad sequences (samples x time)')\r\nX_train = sequence.pad_sequences(XX_train, maxlen=maxlen)\r\nX_test = sequence.pad_sequences(XX_test, maxlen=maxlen)\r\nprint('X_train shape:', X_train.shape)\r\nprint('X_test shape:', X_test.shape)\r\n\r\nprint('Build model...')\r\nmodel = Sequential()\r\nmodel.add(Embedding(max_features, 128, dropout=0.2))\r\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\n# try using different optimizers and different optimizer configs\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='RMSprop',\r\n              metrics=['accuracy'])\r\n\r\nprint('Train...')\r\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\r\n          validation_data=(X_test, y_test))\r\n\\`\\`\\`\r\n\r\n    Loading data...\r\n    5957 train sequences\r\n    5957 test sequences\r\n    Pad sequences (samples x time)\r\n    X_train shape: (5957, 200)\r\n    X_test shape: (5957, 200)\r\n    Build model...\r\n    Train...\r\n    Train on 5957 samples, validate on 5957 samples\r\n    Epoch 1/10\r\n    5957/5957 [==============================] - 63s - loss: 0.6823 - acc: 0.5587 - val_loss: 0.6493 - val_acc: 0.6325\r\n    Epoch 2/10\r\n    5957/5957 [==============================] - 63s - loss: 0.5696 - acc: 0.7082 - val_loss: 0.5350 - val_acc: 0.7386\r\n    Epoch 3/10\r\n    5957/5957 [==============================] - 68s - loss: 0.4870 - acc: 0.7692 - val_loss: 0.5081 - val_acc: 0.7480\r\n    Epoch 4/10\r\n    5957/5957 [==============================] - 70s - loss: 0.4290 - acc: 0.8054 - val_loss: 0.8713 - val_acc: 0.6157\r\n    Epoch 5/10\r\n    5957/5957 [==============================] - 60s - loss: 0.3910 - acc: 0.8348 - val_loss: 0.5099 - val_acc: 0.7646\r\n    Epoch 6/10\r\n    5957/5957 [==============================] - 60s - loss: 0.3738 - acc: 0.8385 - val_loss: 0.5034 - val_acc: 0.7702\r\n    Epoch 7/10\r\n    5957/5957 [==============================] - 59s - loss: 0.3465 - acc: 0.8531 - val_loss: 0.4990 - val_acc: 0.7875\r\n    Epoch 8/10\r\n    5957/5957 [==============================] - 62s - loss: 0.3143 - acc: 0.8724 - val_loss: 0.5216 - val_acc: 0.7747\r\n    Epoch 9/10\r\n    5957/5957 [==============================] - 64s - loss: 0.2992 - acc: 0.8795 - val_loss: 0.4989 - val_acc: 0.7900\r\n    Epoch 10/10\r\n    5957/5957 [==============================] - 64s - loss: 0.2846 - acc: 0.8815 - val_loss: 0.4971 - val_acc: 0.7890\r\n    \r\n\r\n\r\n\r\n\r\n    <keras.callbacks.History at 0x1d1f921a668>\r\n\r\n\r\n\r\n## Compare Result to Original Model\r\n\r\n\r\n\\`\\`\\`python\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nnp.random.seed(1337)  # for reproducibility\r\n\r\nfrom keras.preprocessing import sequence\r\nfrom keras.utils import np_utils\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation, Embedding\r\nfrom keras.layers import LSTM, SimpleRNN, GRU\r\nimport amzn\r\n\r\nmax_features = 20000\r\nmaxlen = 80  # cut texts after this number of words (among top max_features most common words)\r\nbatch_size = 32\r\n\r\nprint('Loading data...')\r\n(X_train, y_train), (X_test, y_test) = amzn.load_data(path='amzn_full.pkl', nb_words=max_features)\r\nprint(len(X_train), 'train sequences')\r\nprint(len(X_test), 'test sequences')\r\n\r\nprint('Pad sequences (samples x time)')\r\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\r\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\r\nprint('X_train shape:', X_train.shape)\r\nprint('X_test shape:', X_test.shape)\r\n\r\nprint('Build model...')\r\nmodel = Sequential()\r\nmodel.add(Embedding(max_features, 128, dropout=0.2))\r\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\n# try using different optimizers and different optimizer configs\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\nprint('Train...')\r\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\r\n          validation_data=(X_test, y_test))\r\n\\`\\`\\`\r\n\r\n    Loading data...\r\n    5957 train sequences\r\n    5957 test sequences\r\n    Pad sequences (samples x time)\r\n    X_train shape: (5957, 80)\r\n    X_test shape: (5957, 80)\r\n    Build model...\r\n    Train...\r\n    Train on 5957 samples, validate on 5957 samples\r\n    Epoch 1/10\r\n    5957/5957 [==============================] - 31s - loss: 0.6641 - acc: 0.5949 - val_loss: 0.5708 - val_acc: 0.7077\r\n    Epoch 2/10\r\n    5957/5957 [==============================] - 31s - loss: 0.4679 - acc: 0.7823 - val_loss: 0.5437 - val_acc: 0.7326\r\n    Epoch 3/10\r\n    5957/5957 [==============================] - 31s - loss: 0.3293 - acc: 0.8687 - val_loss: 0.5521 - val_acc: 0.7521\r\n    Epoch 4/10\r\n    5957/5957 [==============================] - 31s - loss: 0.2428 - acc: 0.9077 - val_loss: 0.5821 - val_acc: 0.7601\r\n    Epoch 5/10\r\n    5957/5957 [==============================] - 31s - loss: 0.1790 - acc: 0.9347 - val_loss: 0.7185 - val_acc: 0.7408\r\n    Epoch 6/10\r\n    5957/5957 [==============================] - 31s - loss: 0.1270 - acc: 0.9542 - val_loss: 0.7469 - val_acc: 0.7519\r\n    Epoch 7/10\r\n    5957/5957 [==============================] - 32s - loss: 0.1136 - acc: 0.9599 - val_loss: 0.8070 - val_acc: 0.7430\r\n    Epoch 8/10\r\n    5957/5957 [==============================] - 31s - loss: 0.0890 - acc: 0.9694 - val_loss: 0.9022 - val_acc: 0.7521\r\n    Epoch 9/10\r\n    5957/5957 [==============================] - 31s - loss: 0.0721 - acc: 0.9758 - val_loss: 0.9214 - val_acc: 0.7275\r\n    Epoch 10/10\r\n    5957/5957 [==============================] - 32s - loss: 0.0583 - acc: 0.9802 - val_loss: 0.9235 - val_acc: 0.7334\r\n    \r\n\r\n\r\n\r\n\r\n    <keras.callbacks.History at 0x1d20002c588>\r\n\r\n\r\n\r\n\r\n\\`\\`\\`python\r\n%%html\r\n<style>\r\ntable {float:left}\r\n</style>\r\n\\`\\`\\`\r\n\r\n\r\n<style>\r\ntable {float:left}\r\n</style>\r\n\r\n\r\n# Conclusion\r\nOur new model is based on two main hypothesis:\r\n 1. It is reasonable to assume when people write reviews, they would express their opinion at the start more often than at the end. So we can limit each review to some *\"maxlen\"* to eliminate noisy data. \r\n 2. Some words are essential than other words for expressing sentiment. For example, \"love\" would express more emotion than \"keyboard\". We therefore limit *\"max_features\"* to eliminate less sentimentally expresed words.\r\n\r\n\r\nTherefore by experimenting different *\"maxlen\"*, *\"max_features\"* , and optimizers, our new model improves the prediction accuracy on both IMDB and Amazon Review dataset, hence validating our hypothesis.\r\n\r\n\r\n\r\n|                        | Original Model Provided by Keras| Our New Model\r\n| :------                | :-----------                    | ----       |\r\n| **Amazon Review **     | 73.34%                          | 78.90%     |\r\n| **IMDB Movie Review ** | 82.35%                          | 88.75%     |\r\n\r\n\r\n\r\n<br/>\r\n<br/>\r\n<br/>\r\n\r\n# Reference\r\n\r\n\r\n[1] Fran¸cois Chollet. Keras. [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py), 2015 \r\n\r\n[2] Fran¸cois Chollet. Keras. [imdb.py](https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py), 2015 \r\n\r\n[3] \"Convolutional Neural Networks for Visual Recognition.\" Web. 06 Dec. 2016. (http://cs231n.github.io/neural-networks-1).\r\n\r\n[4] Ruder, Sebastian. \"An Overview of Gradient Descent Optimization Algorithms.\" 30 Sept. 2016. Web. (http://sebastianruder.com/optimizing-gradient-descent/).\r\n\r\n[5] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas//papers/wvSent_acl2011.pdf). *The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)*, 2011\r\n\r\n[6] T Mikolov, I Sutskever, K Chen, GS Corrado, J Dean - Advances in neural information processing systems, [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), 2013\r\n\r\n[7] Hochreiter, S., & Schmidhuber, J. [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf). Neural computation, 9(8), 1735-1780. 1997\r\n\r\n[8] Olah C [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), 2015\r\n\r\n[9] Arnold T, *STAT 365/665: Data Mining and Machine Learning*, [Recurrent neural networks](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/notebook21.html), 2016\r\n`\r\n\r\n// document.getElementById('results').innerHTML = (marked(article))\r\nexport default {\r\n  name: 'article',\r\n  data(){\r\n    return {\r\n      rawHtml: marked(article)\r\n    }\r\n  },\r\n  mounted(){\r\n    Prism.highlightAll()\r\n  }\r\n}\r\n</script>\r\n\r\n<!-- Add \"scoped\" attribute to limit CSS to this component only -->\r\n<style scoped>\r\n\r\n</style>\r\n\n\n\n// WEBPACK FOOTER //\n// article.vue?26b16100","exports = module.exports = require(\"../../../../node_modules/css-loader/lib/css-base.js\")(true);\n// imports\n\n\n// module\nexports.push([module.id, \"\", \"\", {\"version\":3,\"sources\":[],\"names\":[],\"mappings\":\"\",\"file\":\"article.vue\",\"sourceRoot\":\"\"}]);\n\n// exports\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/css-loader?{\"minimize\":true,\"sourceMap\":true}!./~/vue-loader/lib/style-compiler?{\"id\":\"data-v-fc76b68e\",\"scoped\":true,\"hasInlineConfig\":false}!./~/vue-loader/lib/selector.js?type=styles&index=0!./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/article.vue\n// module id = 48\n// module chunks = 1","// style-loader: Adds some css to the DOM by adding a <style> tag\n\n// load the styles\nvar content = require(\"!!../../../../node_modules/css-loader/index.js?{\\\"minimize\\\":true,\\\"sourceMap\\\":true}!../../../../node_modules/vue-loader/lib/style-compiler/index.js?{\\\"id\\\":\\\"data-v-fc76b68e\\\",\\\"scoped\\\":true,\\\"hasInlineConfig\\\":false}!../../../../node_modules/vue-loader/lib/selector.js?type=styles&index=0!./article.vue\");\nif(typeof content === 'string') content = [[module.id, content, '']];\nif(content.locals) module.exports = content.locals;\n// add the styles to the DOM\nvar update = require(\"!../../../../node_modules/vue-style-loader/lib/addStylesClient.js\")(\"4eb8fe15\", content, true);\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/extract-text-webpack-plugin/loader.js?{\"omit\":1,\"remove\":true}!./~/vue-style-loader!./~/css-loader?{\"minimize\":true,\"sourceMap\":true}!./~/vue-loader/lib/style-compiler?{\"id\":\"data-v-fc76b68e\",\"scoped\":true,\"hasInlineConfig\":false}!./~/vue-loader/lib/selector.js?type=styles&index=0!./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/article.vue\n// module id = 53\n// module chunks = 1","module.exports = __webpack_public_path__ + \"static/img/output_11_2.0408346.png\";\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_11_2.png\n// module id = 57\n// module chunks = 1","module.exports = __webpack_public_path__ + \"static/img/output_11_3.eda9622.png\";\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_11_3.png\n// module id = 58\n// module chunks = 1","module.exports = __webpack_public_path__ + \"static/img/output_11_4.f5d8933.png\";\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_11_4.png\n// module id = 59\n// module chunks = 1","module.exports = __webpack_public_path__ + \"static/img/output_11_5.827a616.png\";\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_11_5.png\n// module id = 60\n// module chunks = 1","module.exports = __webpack_public_path__ + \"static/img/output_16_0.a4a67e4.png\";\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_16_0.png\n// module id = 61\n// module chunks = 1","module.exports = \"data:image/png;base64,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\"\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/components/research_article/LSTM-RNN_For_Sentiment_Analysis/output_18_0.png\n// module id = 62\n// module chunks = 1"],"sourceRoot":""}